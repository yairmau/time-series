{
  "hash": "19f2720efb549a2a231e0317cd0bb4f3",
  "result": {
    "markdown": "---\ntitle: finite differences\nexecute:\n  freeze: auto\n---\n\n![](central_diff.png)\n\nDefinition of a **derivative**:\n\n$$\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n$$\n\nNumerically, we can approximate the derivative $f'(t)$ of a time series $f(t)$ as\n\n$$\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n$$ {#eq-tpfdf}\n\n::: {.column-margin}\nThe expression $\\mathcal{O}(\\Delta t)$ means that the error associated with the approximation is proportional to $\\Delta t$. This is called [\"Big O notation\"](https://en.wikipedia.org/wiki/Big_O_notation).\n:::\n\nThe expression above is called the *two-point forward difference formula*.\nLikewise, we can define the *two-point backward difference formula*:\n\n$$\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n$$ {#eq-tpbdf}\n\nIf we sum together @eq-tpfdf and @eq-tpbdf we get:\n\n<!-- $$\n\\cancel{abcde}\n$$ -->\n\n$$\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n &= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n$$ {#eq-sum}\n\nDividing both sides by 2 gives the *two-point central difference formula*:\n\n$$\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2). \n$$ {#eq-twcdf}\n\nTwo things are worth mentioning about the approximation above:\n\n1. it is balanced, that is, there is no preference of the future over the past.\n1. its error is proportional to $\\Delta t^2$, it is a lot more precise than the unbalanced approximations :)\n\n::: {.column-margin}\nTo understand why the error is proportional to $\\Delta t^2$, one can subtract the Taylor expansion of $f(t-\\Delta t)$ from the Taylor expansion of $f(t+\\Delta t)$.\n[See this, pages 3 and 4.](https://home.cc.umanitoba.ca/~farhadi/Math2120/Numerical%20Differentiation.pdf)\n:::\n\n![](central_diff.png)\n\nThe function [`np.gradient`](https://numpy.org/doc/stable/reference/generated/numpy.gradient.html) calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n::: {.column-margin}\nThe \"gradient\" usually refers to a first derivative with respect to space, and it is denoted as $\\nabla f(x)=\\frac{df(x)}{dx}$. However, it doesn't really matter if we call the independent variable $x$ or $t$, the derivative operator is exactly the same.\n:::\n\nCheck out this [nice example](https://gist.github.com/astrojuanlu/e4d47fec5d94d2224762a61680419eb2).\n\n",
    "supporting": [
      "finite-differences_files/figure-pdf"
    ],
    "filters": []
  }
}