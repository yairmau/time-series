[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "About\nWelcome to Time Series Analysis for Environmental Sciences (71606) at the Hebrew University of Jerusalem. This is Yair Mau, your host for today. I am a senior lecturer at the Institute of Environmental Sciences, at the Faculty of Agriculture, Food and Environment, in Rehovot, Israel.\nThis website contains (almost) all the material you‚Äôll need for the course. If you find any mistakes, or have any comments, please email me."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "Disclaimer",
    "text": "Disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#what-who-when-and-where",
    "href": "index.html#what-who-when-and-where",
    "title": "Time Series Analysis",
    "section": "What, who, when and where?",
    "text": "What, who, when and where?\n Course number 71606, 3 academic points\n Yair Mau (lecturer), Erez Feuer (TA)\n Tuesdays, from 14:15 to 17:00\n Computer classroom #16\n Office hours: upon request"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "Syllabus",
    "text": "Syllabus\n\nCourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\nCourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nLearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\nCourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nBooks and other sources\nClick here.\n\n\nCourse evaluation\nThere will be 2 projects during the semester (each worth 25% of the final grade), and one final project (50%)."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "Weekly program",
    "text": "Weekly program\n\nWeek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nWeek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nWeek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nWeek 4\n\nLecture: Time series plotting: best practices. Dos and don‚Äôts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nWeek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nWeek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nWeek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nWeek 8\nSame as lecture 7 above\n\n\nWeek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nWeek 10\n\nLecture: Fourier decomposition, filtering, Nyquist‚ÄìShannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nWeek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nWeek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nWeek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "introduction/first-steps.html#nan-missing-data-outliers",
    "href": "introduction/first-steps.html#nan-missing-data-outliers",
    "title": "3¬† First Steps ‚Äî basic time series analysis",
    "section": "3.1 NaN, Missing data, Outliers",
    "text": "3.1 NaN, Missing data, Outliers\n\n# %matplotlib widget\n\nstart = \"2022-05-03 12:00:00\"\nend = \"2022-05-06 00:00:00\"\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# plot using pandas' plot method\ndf.loc[start:end, 'T2'].plot(ax=ax,\n                             linestyle='-',\n                             marker='o',\n                             color=\"tab:blue\",\n                             label=\"data\")\n\n# annotate examples here:\n# https://jakevdp.github.io/PythonDataScienceHandbook/04.09-text-and-annotation.html\nax.annotate(\"NaN\",                             # text to write, if nothing, then \"\"\n            xy=('2022-05-03 20:30:00', 25),    # (x,y coordinates for the tip of the arrow)\n            xycoords='data',                   # xy as 'data' coordinates\n            xytext=(-20, 60),                  # xy coordinates for the text\n            textcoords='offset points',        # xytext relative to xy\n            arrowprops=dict(arrowstyle=\"->\")   # pretty arrow\n           )\nax.annotate(\"outlier\",\n            xy=('2022-05-03 22:30:00', 85),\n            xycoords='data',\n            xytext=(40, -20),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\nax.annotate(\"missing rows\",\n            xy=('2022-05-05 00:00:00', 25),\n            xycoords='data',\n            xytext=(0, 40),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\n\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d %b, %H:00'))\nplt.gcf().autofmt_xdate()\nax.set(xlabel=\"\",\n       ylabel=\"Temperature (deg C)\")\n\n[Text(0.5, 0, ''), Text(0, 0.5, 'Temperature (deg C)')]\n\n\n\n\n\nThe arrows (annotate) work because the plot was\ndf['column'].plot()\nIf you use the usual\nax.plot(df['column'])\nthen you matplotlib will not understand timestamps as x-positions. In this case follow the instructions below.\n\n# %matplotlib widget\n\nstart = \"2022-05-03 12:00:00\"\nend = \"2022-05-06 00:00:00\"\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\nax.plot(df.loc[start:end, 'T2'], linestyle='-', marker='o', color=\"tab:blue\", label=\"data\")\n\nt_nan = '2022-05-03 20:30:00'\nx_nan = mdates.date2num(dt.datetime.strptime(t_nan, \"%Y-%m-%d %H:%M:%S\"))\nax.annotate(\"NaN\",\n            xy=(x_nan, 25),\n            xycoords='data',\n            xytext=(-20, 60),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\nt_outlier = '2022-05-03 22:30:00'\nx_outlier = mdates.date2num(dt.datetime.strptime(t_outlier, \"%Y-%m-%d %H:%M:%S\"))\nax.annotate(\"outlier\",\n            xy=(x_outlier, 85),\n            xycoords='data',\n            xytext=(40, -20),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\nt_missing = '2022-05-05 00:00:00'\nx_missing = mdates.date2num(dt.datetime.strptime(t_missing, \"%Y-%m-%d %H:%M:%S\"))\nax.annotate(\"missing rows\",\n            xy=(x_missing, 25),\n            xycoords='data',\n            xytext=(0, 40),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\n# code for hours, days, etc\n# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d %b, %H:00'))\nplt.gcf().autofmt_xdate()\nax.set(xlabel=\"\",\n       ylabel=\"Temperature (deg C)\")\n\n[Text(0.5, 0, ''), Text(0, 0.5, 'Temperature (deg C)')]\n\n\n\n\n\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\ndelta_index = (df.index.to_series().diff() / pd.Timedelta('1 sec') ).values\nax.plot(delta_index)\nax.set(ylim=[0, 100],\n       xlabel=\"running index\",\n       ylabel=r\"$\\Delta t$ (s)\",\n       title=\"Time difference between consecutive rows\")\n\n[(0.0, 100.0),\n Text(0.5, 0, 'running index'),\n Text(0, 0.5, '$\\\\Delta t$ (s)'),\n Text(0.5, 1.0, 'Time difference between consecutive rows')]"
  },
  {
    "objectID": "introduction/first-steps.html#resample",
    "href": "introduction/first-steps.html#resample",
    "title": "3¬† First Steps ‚Äî basic time series analysis",
    "section": "3.2 Resample",
    "text": "3.2 Resample\n\n3.2.1 Downsampling\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# Downsample to spaced out data points. Change the number below, see what happens.\nwindow_size = '15min'\ndf_resampled = (df['T2'].resample(window_size)  # resample doesn't do anything yet, just divides data into buckets\n                        .mean()                 # this is where stuff happens. you can also choose \"sum\", \"max\", etc\n               )\n# optional, add half a window size to timestamp\ndf_resampled.index = df_resampled.index + to_offset(window_size) / 2\n\nax.plot(df['T2'], color=\"tab:blue\", label=\"original data\")\nax.plot(df_resampled, marker='x', color=\"tab:orange\", zorder=-1,\n        label=f\"resampled {window_size} data\")\nax.legend()\n\nax.set(xlabel=\"time\",\n       ylabel=\"temperature (deg C)\")\n\n[Text(0.5, 0, 'time'), Text(0, 0.5, 'temperature (deg C)')]\n\n\n\n\n\n\n\n3.2.2 Filling missing data\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# see options for interpolation methods here:\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\ndf_interpolated1 = df_resampled.interpolate(method='time')\ndf_interpolated2 = df_resampled.interpolate(method='nearest')\n\nax.plot(df_resampled, color=\"tab:orange\", label=\"resampled\")\nax.plot(df_interpolated1, '.', color=\"tab:purple\", zorder=-1,\n        label=f\"time-interpolated\")\nax.plot(df_interpolated2, '.', color=\"tab:cyan\", zorder=-2,\n        label=f\"nearest-interpolated\")\nax.legend()\n\nax.set(xlabel=\"time\",\n       ylabel=\"temperature (deg C)\")\n\n[Text(0.5, 0, 'time'), Text(0, 0.5, 'temperature (deg C)')]"
  },
  {
    "objectID": "introduction/first-steps.html#smoothing-noisy-data",
    "href": "introduction/first-steps.html#smoothing-noisy-data",
    "title": "3¬† First Steps ‚Äî basic time series analysis",
    "section": "3.3 Smoothing noisy data",
    "text": "3.3 Smoothing noisy data\nLet‚Äôs first download data from a different project.\n\nfilename2 = \"test_peleg.csv\"\n# if file is not there, go fetch it from thingspeak\nif not os.path.isfile(filename2):\n    # define what to download\n    channels = \"1708067\"\n    fields = \"1,2,3,4,5\"\n    minutes = \"30\"\n\n    # https://www.mathworks.com/help/thingspeak/readdata.html\n    # format YYYY-MM-DD%20HH:NN:SS\n    start = \"2022-05-15%2000:00:00\"\n    end = \"2022-05-25%2000:00:00\"\n\n    # download using Thingspeak's API\n    # url = f\"https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?minutes={minutes}\"\n    url = f\"https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?start={start}&end={end}\"\n    data = urllib.request.urlopen(url)\n    d = data.read()\n\n    # save data to csv\n    file = open(filename2, \"w\")\n    file.write(d.decode('UTF-8'))\n    file.close()\n\n\n# load data\ndf = pd.read_csv(filename2)\n# rename columns\ndf = df.rename(columns={\"created_at\": \"timestamp\",\n                        \"field1\": \"T\",\n                        \"field2\": \"Tw\",\n                        \"field3\": \"RH\",\n                        \"field4\": \"VPD\",\n                        \"field5\": \"dist\",\n                        })\n# set timestamp as index\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.set_index('timestamp')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      entry_id\n      T\n      Tw\n      RH\n      VPD\n      dist\n    \n    \n      timestamp\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-05-18 20:09:31+00:00\n      24716\n      23.85\n      23.3125\n      65.32\n      1.02532\n      7.208\n    \n    \n      2022-05-18 20:10:32+00:00\n      24717\n      23.88\n      23.2500\n      65.32\n      1.02717\n      7.208\n    \n    \n      2022-05-18 20:11:33+00:00\n      24718\n      23.90\n      23.2500\n      65.23\n      1.03107\n      7.276\n    \n    \n      2022-05-18 20:12:33+00:00\n      24719\n      23.90\n      23.2500\n      65.19\n      1.03226\n      7.208\n    \n    \n      2022-05-18 20:13:34+00:00\n      24720\n      23.89\n      23.2500\n      65.15\n      1.03282\n      7.633\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-05-24 12:18:35+00:00\n      32711\n      27.47\n      26.1250\n      47.49\n      1.92397\n      8.925\n    \n    \n      2022-05-24 12:19:36+00:00\n      32712\n      27.47\n      26.1250\n      47.62\n      1.91921\n      8.925\n    \n    \n      2022-05-24 12:20:39+00:00\n      32713\n      27.47\n      26.1250\n      47.96\n      1.90675\n      8.925\n    \n    \n      2022-05-24 12:21:40+00:00\n      32714\n      27.47\n      26.1875\n      47.75\n      1.91444\n      8.925\n    \n    \n      2022-05-24 12:22:41+00:00\n      32715\n      27.49\n      26.1875\n      47.94\n      1.90971\n      8.925\n    \n  \n\n8000 rows √ó 6 columns"
  },
  {
    "objectID": "introduction/first-steps.html#smoothing-noisy-data-1",
    "href": "introduction/first-steps.html#smoothing-noisy-data-1",
    "title": "3¬† First Steps ‚Äî basic time series analysis",
    "section": "3.4 Smoothing noisy data",
    "text": "3.4 Smoothing noisy data\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\nax.plot(df['RH'], '.')\n# add labels and title\nax.set(xlabel = \"time\",\n       ylabel = \"RH (%)\",\n       title = \"Relative Humidity\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()  \n\n\n\n\n\n3.4.1 Moving average and SavGol\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# apply a rolling average of size \"window_size\",\n# it can be either by number of points, or by window time\n# window_size = 30  # number of measurements\nwindow_size = '120min'  # minutes\nRH_smooth = df['RH'].rolling(window_size, center=True).mean().to_frame()\nRH_smooth.rename(columns={'RH': 'rolling_avg'}, inplace=True)\n\nRH_smooth['SG'] = savgol_filter(df['RH'], window_length=121, polyorder=2)\n\nax.plot(df['RH'], color=\"tab:blue\", label=\"data\")\nax.plot(RH_smooth['rolling_avg'], color=\"tab:orange\", label=\"moving average\")\nax.plot(RH_smooth['SG'], color=\"tab:red\", label=\"Savitzky-Golay filter\")\n# add labels and title\nax.set(xlabel = \"time\",\n       ylabel = \"RH (%)\",\n       title = \"Relative Humidity\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()\nax.legend()\n\n<matplotlib.legend.Legend at 0x7fe6a0525730>"
  },
  {
    "objectID": "outliers/zscore.html",
    "href": "outliers/zscore.html",
    "title": "4¬† Z-score",
    "section": "",
    "text": "\\[\nz  = \\frac{x-\\mu}{\\sigma},\n\\]\n\n\nwhere\n\n\\(x=\\) data point,\n\n\\(\\mu=\\) time series mean\n\n\\(\\sigma=\\) time series standard deviation.\n\nLet‚Äôs write a function that identifies outliers according to the Z-score.\ndef zscore(df, degree=3):\n    data = df.copy()\n    data['zscore'] = (data - data.mean())/data.std()\n    outliers = data[(data['zscore'] <= -degree) | (data['zscore'] >= degree)]\n    return outliers['value'], data\nNow we can simply use this function:\nthreshold = 2.5\noutliers, transformed = zscore(tx, threshold)\nSource: Atwan (2022)\n\n\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook: Practical Recipes for Exploratory Data Analysis, Data Preparation, Forecasting, and Model Evaluation. Packt."
  },
  {
    "objectID": "smoothing/convolution.html#convolution",
    "href": "smoothing/convolution.html#convolution",
    "title": "8¬† Convolution",
    "section": "8.1 Convolution",
    "text": "8.1 Convolution\nConvolution is a fancy word for averaging a time series using a running window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/convolution.html#kernels",
    "href": "smoothing/convolution.html#kernels",
    "title": "8¬† Convolution",
    "section": "8.2 Kernels",
    "text": "8.2 Kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation (the window in the animation is 4 standard deviations wide)."
  },
  {
    "objectID": "smoothing/convolution.html#math",
    "href": "smoothing/convolution.html#math",
    "title": "8¬† Convolution",
    "section": "8.3 Math",
    "text": "8.3 Math\nThe definition of a convolution between signal \\(f(t)\\) and kernel \\(k(t)\\) is\n\\[\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\\]\nThe expression \\(f*k\\) denotes the convolution of these two functions. The argument of \\(k\\) is \\(t-\\tau\\), meaning that the kernel runs from left to right (as \\(t\\) does), and at every point the two signals (\\(f\\) and \\(k\\)) are multiplied together. It is the product of the signal with the weight function \\(k\\) that gives us an average. Because of \\(-\\tau\\), the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\\[\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\\]\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/convolution.html#numerics",
    "href": "smoothing/convolution.html#numerics",
    "title": "8¬† Convolution",
    "section": "8.4 Numerics",
    "text": "8.4 Numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes\ndf['temperature'].rolling(window='20', center=True).mean()\n\nwindow=20 means that the width of the window is 20 points. Pandas lets us define a window width in time units, for example, window='120min'.\ncenter=True is needed in order to assign the result of averaging to the center of the window. Make it False and see what happens.\nmean() is the actual calculation, the average of temperature over the window. The rolling part does not compute anything, it just creates a moving window, and we are free to calculate whatever we want. Try to calculate the standard deviation or the maximum, for example.\n\nIt is implicit in the command above a ‚Äúrectangular‚Äù kernel. What if we want other shapes?\n\n8.4.1 Gaussian\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere * window_width is an integer, number of points in your window * std_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package. The graph above was achieved by running:\ng = scipy.signal.gaussian(window_width, std)\nplt.plot(g)\n\n\n8.4.2 Triangular\nSame idea as gaussian, but simpler, because we don‚Äôt need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "title": "8¬† Convolution",
    "section": "8.5 ü§∑‚Äç‚ôÇÔ∏è Which window shape and width to choose?",
    "text": "8.5 ü§∑‚Äç‚ôÇÔ∏è Which window shape and width to choose?\nSorry, there is not definite answer here‚Ä¶ It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above."
  },
  {
    "objectID": "smoothing/perfect-smoother.html",
    "href": "smoothing/perfect-smoother.html",
    "title": "10¬† A perfect smoother",
    "section": "",
    "text": "Source: Eilers (2003)\nGitHub repository\nNoisy series \\(y\\) of length \\(m\\).\nThe smoothed series is called \\(z\\).\nWe have conflicting interests:\n\nwe want a \\(z\\) series ‚Äúas smooth as possible‚Äù.\nhowever, the smoother \\(z\\) is, the farthest from \\(y\\) it will be (low fidelity).\n\nRoughness:\n\\[\nR = \\displaystyle\\sum_i (z_i - z_{i-1})^2\n\\]\nFit to data:\n\\[\nS = \\displaystyle\\sum_i (y_i - z_i)^2\n\\]\nCost functional to be minimized:\n\\[\nQ = S + \\lambda R\n\\]\n\n\n\n\nEilers, Paul HC. 2003. ‚ÄúA Perfect Smoother.‚Äù Analytical Chemistry 75 (14): 3631‚Äì36. https://doi.org/10.1021/ac034173t."
  },
  {
    "objectID": "stationarity/autocorrelation.html#question",
    "href": "stationarity/autocorrelation.html#question",
    "title": "12¬† Autocorrelation",
    "section": "12.1 Question",
    "text": "12.1 Question\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nTo answer this, we need to talk about autocorrelation. Let‚Äôs start by introducing the necessary concepts."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "12¬† Autocorrelation",
    "section": "12.2 Mean and standard deviation",
    "text": "12.2 Mean and standard deviation\nLet‚Äôs call our time series from above \\(X\\), and its length \\(N\\). Then:\n\\[\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\\]\nThe mean and standard deviation can be visualized thus:\n\nOne last basic concept we need is the expected value: \\[\nE[X] = \\sum_{i=1}^N X_i p_i\n\\]\nFor our time series, the probability \\(p_i\\) that a given point \\(X_i\\) is in the dataset is simply \\(1/N\\), therefore the expectation becomes\n\\[\nE[X] = \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N}\n\\]"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "12¬† Autocorrelation",
    "section": "12.3 Autocorrelation",
    "text": "12.3 Autocorrelation\nThe autocorrelation of a time series \\(X\\) is the answer to the following question:\n\nif we shift \\(X\\) by \\(\\tau\\) units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are \\(X(t)\\) and \\(X(t+\\tau)\\)?\n\nUsing the Pearson correlation coefficient\n\n\nPearson correlation coefficient between \\(X\\) and \\(Y\\): \\[\n\\rho_{X,Y} = \\frac{E\\left[ (X - \\mu_X)(X_Y - \\mu_Y) \\right]}{\\sigma_X\\sigma_Y}\n\\]\nwe get\n\\[\n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\\]\nA video is worth a billion words, so let‚Äôs see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\(\\tau=0\\) (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "13¬† Cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "15¬† LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "frequency/fourier.html#basic-wave-concepts",
    "href": "frequency/fourier.html#basic-wave-concepts",
    "title": "16¬† Fourier transform",
    "section": "16.1 Basic wave concepts",
    "text": "16.1 Basic wave concepts\nThe function\n\\[\nf(t) = B\\sin(2\\pi f t)\n\\tag{16.1}\\]\nhas two basic characteristics, its amplitude \\(B\\) and frequency \\(f\\).\n\nIn the figure above, the amplitude \\(B=0.6\\) and we see that the distance between two peaks is called period, \\(T=2\\) s. The frequency is defined as the inverse of the period:\n\\[\nf = \\frac{1}{T}.\n\\tag{16.2}\\]\nWhen time is in seconds, then the frequency is measured in Hertz (Hz). For the graph above, therefore, we see a wave whose frequency is \\(f = 1/(2 \\text{ s}) = 0.5\\) Hz.\nIn the figure below, we see what happens when we vary the values of the frequency and amplitude.\n\nThe graph above introduces two new characteristics of a wave, its phase \\(\\phi\\), and its offset \\(B\\). A more general description of a sine wave is\n\\[\nf(t) = B\\sin(2\\pi f t + \\phi) + B_0.\n\\tag{16.3}\\]\nThe offset \\(B_0\\) moves the wave up and down, while changing the value of \\(\\phi\\) makes the sine wave move left and right. When the phase \\(\\phi=2\\pi\\), the sine wave will have shifted a full period, and the resulting wave is identical to the original:\n\\[\nB\\sin(2\\pi f t) = B\\sin(2\\pi f t + 2\\pi).\n\\tag{16.4}\\]\nAll the above can also be said about a cosine, whose general for can be given as\n\\[\nA\\cos(2\\pi f t + \\phi) + A_0\n\\tag{16.5}\\]\nOne final point before we jump into the deep waters is that the sine and cosine functions are related through a simple phase shift:\n\\[\n\\cos\\left(2\\pi f t + \\frac{\\pi}{2}\\right) = \\sin\\left(2\\pi f t\\right)\n\\]"
  },
  {
    "objectID": "frequency/fourier.html#fouriers-theorem",
    "href": "frequency/fourier.html#fouriers-theorem",
    "title": "16¬† Fourier transform",
    "section": "16.2 Fourier‚Äôs theorem",
    "text": "16.2 Fourier‚Äôs theorem\nFourier‚Äôs theorem states that\n\nAny periodic signal is composed of a superposition of pure sine waves, with suitably chosen amplitudes and phases, whose frequencies are harmonics of the fundamental frequency of the signal.\n\nSee the following animations to visualize the theorem in action.\n\nSource: https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Fourier_synthesis_square_wave_animated.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Sawtooth_Fourier_Animation.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Continuous_Fourier_transform_of_rect_and_sinc_functions.gif"
  },
  {
    "objectID": "frequency/fourier.html#fourier-series",
    "href": "frequency/fourier.html#fourier-series",
    "title": "16¬† Fourier transform",
    "section": "16.3 Fourier series",
    "text": "16.3 Fourier series\n\na periodic function can be described as a sum of sines and cosines.\n\n\n\nNot any function, but certainly most functions we will deal with in this course. The function has to fullful the Dirichlet conditions\nThe classic examples are usually the square function and the sawtooth function:\n\n\n[Source: https://www.geogebra.org/m/tkajbzmg]\nhttps://www.geogebra.org/m/k4eq4fkr\n\n\n\\[\nF[x(t)] = F(f) = \\int_{-\\infty}^{\\infty}x(t)e^{-2\\pi i f t}dt\n\\]\n\\[\nf(t) = \\int_{-\\infty}^{\\infty}F(f)e^{2\\pi i f t}df\n\\]\nhttps://dibsmethodsmeetings.github.io/fourier-transforms/"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html",
    "href": "seasonality/seasonal-decomposition.html",
    "title": "19¬† Seasonal Decomposition",
    "section": "",
    "text": "20 Trends in Atmospheric Carbon Dioxide\nMauna Loa CO2 concentration.\ndata from NOAA\nfill missing data. interpolate method: ‚Äòtime‚Äô\ninterpolation methods visualized"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#decompose-data",
    "href": "seasonality/seasonal-decomposition.html#decompose-data",
    "title": "19¬† Seasonal Decomposition",
    "section": "20.1 decompose data",
    "text": "20.1 decompose data\nseasonal_decompose returns an object with four components:\n\nobserved: \\(Y(t)\\)\ntrend: \\(T(t)\\)\nseasonal: \\(S(t)\\)\nresid: \\(e(t)\\)\n\nAdditive model: \\[\nY(t) = T(t) + S(t) + e(t)\n\\]\nMultiplicative model: \\[\nY(t) = T(t) \\times S(t) \\times e(t)\n\\]\n\n20.1.0.1 Interlude\nlearn how to use zip in a loop\n\nletters = ['a', 'b', 'c', 'd', 'e']\nnumbers = [1, 2, 3, 4, 5]\n# zip let's us iterate over to lists at the same time\nfor l, n in zip(letters, numbers):\n    print(f\"{l} = {n}\")\n\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n\nPlot each component separately.\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\ndecomposed_m = seasonal_decompose(df['co2'], model='multiplicative')\ndecomposed_a = seasonal_decompose(df['co2'], model='additive')\ndecomposed = decomposed_m\npos = (0.5, 0.9)\ncomponents =[\"observed\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(decomposed, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\n\n\n\n\n\n# %matplotlib widget\n\ndecomposed = decomposed_m\n\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df['co2'], color=\"tab:blue\", label=\"observed\")\nax[0].plot(decomposed.trend * decomposed.resid, color=\"tab:orange\", label=\"trend*resid\")\nax[0].plot(decomposed.trend * decomposed.seasonal, color=\"tab:red\", label=\"trend*seasonal\")\nax[0].plot(decomposed.trend, color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"CO$_2$ concentration (ppm)\",\n          title=\"Mauna Loa CO$_2$ concentration\")\nax[0].legend(frameon=False)\n\nstart = \"2000-01-01\"\nend = \"2003-01-01\"\nzoom = slice(start, end)\nax[1].plot(df.loc[zoom, 'co2'], color=\"tab:blue\", label=\"observed\")\nax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=\"tab:orange\", label=\"trend*resid\")\nax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=\"tab:red\", label=\"trend*seasonal\")\nax[1].plot(decomposed.trend[zoom], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"Components, 2000--2003\");"
  },
  {
    "objectID": "rates-of-change/finite-differences.html",
    "href": "rates-of-change/finite-differences.html",
    "title": "22¬† Finite differences",
    "section": "",
    "text": "Definition of a derivative:\n\\[\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n\\]\nNumerically, we can approximate the derivative \\(f'(t)\\) of a time series \\(f(t)\\) as\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{22.1}\\]\n\n\nThe expression \\(\\mathcal{O}(\\Delta t)\\) means that the error associated with the approximation is proportional to \\(\\Delta t\\). This is called ‚ÄúBig O notation‚Äù.\nThe expression above is called the two-point forward difference formula. Likewise, we can define the two-point backward difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{22.2}\\]\nIf we sum together Equation¬†22.1 and Equation¬†22.2 we get:\n\n\\[\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n&= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n\\tag{22.3}\\]\nDividing both sides by 2 gives the two-point central difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n\\tag{22.4}\\]\nTwo things are worth mentioning about the approximation above:\n\nit is balanced, that is, there is no preference of the future over the past.\nits error is proportional to \\(\\Delta t^2\\), it is a lot more precise than the unbalanced approximations :)\n\n\n\nTo understand why the error is proportional to \\(\\Delta t^2\\), one can subtract the Taylor expansion of \\(f(t-\\Delta t)\\) from the Taylor expansion of \\(f(t+\\Delta t)\\). See this, pages 3 and 4.\n\nThe function np.gradient calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n\nThe ‚Äúgradient‚Äù usually refers to a first derivative with respect to space, and it is denoted as \\(\\nabla f(x)=\\frac{df(x)}{dx}\\). However, it doesn‚Äôt really matter if we call the independent variable \\(x\\) or \\(t\\), the derivative operator is exactly the same.\nCheck out this nice example."
  },
  {
    "objectID": "rates-of-change/fourier-based-derivatives.html",
    "href": "rates-of-change/fourier-based-derivatives.html",
    "title": "23¬† Fourier-based derivatives",
    "section": "",
    "text": "This tutorial is based on Pelliccia (2019).\nnice trick: https://math.stackexchange.com/questions/430858/fourier-transform-of-derivative\n\n\n\n\nPelliccia, Daniel. 2019. ‚ÄúFourier Spectral Smoothing Method.‚Äù 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "Technical Stuff",
    "section": "Operating systems",
    "text": "Operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "Technical Stuff",
    "section": "Software",
    "text": "Software\nAnaconda‚Äôs Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "Technical Stuff",
    "section": "Python packages",
    "text": "Python packages\nKats ‚Äî a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "references.html#books",
    "href": "references.html#books",
    "title": "Sources",
    "section": "Books",
    "text": "Books\nfrom Data to Viz\nFundamentals of Data Visualization, by Claus O. Wilke\nPyNotes in Agriscience\nForecasting: Principles and Practice (3rd ed), by Rob J Hyndman and George Athanasopoulos\nPython for Finance Cookbook 2nd Edition - Code Repository\nPractical time series analysis,: prediction with statistics and machine learning, by Aileen Nielsen\nThe online edition of this book is available for Hebrew University staff and students.\nTime series analysis with Python cookbook : practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation, by Tarek A. Atwan\nThe online edition of this book is available for Hebrew University staff and students.\nHands-on Time Series Analysis with Python: From Basics to Bleeding Edge Techniques, by B V Vishwas, Ashish Patel\nThe online edition of this book is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#videos",
    "href": "references.html#videos",
    "title": "Sources",
    "section": "Videos",
    "text": "Videos\nTimes Series Analysis for Everyone, by Bruno Goncalves\nThis series is available for Hebrew University staff and students.\nTime Series Analysis with Pandas, by Joshua Malina This video is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "Sources",
    "section": "References",
    "text": "References\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook:\nPractical Recipes for Exploratory Data Analysis, Data Preparation,\nForecasting, and Model Evaluation. Packt.\n\n\nEilers, Paul HC. 2003. ‚ÄúA Perfect Smoother.‚Äù Analytical\nChemistry 75 (14): 3631‚Äì36. https://doi.org/10.1021/ac034173t.\n\n\nPelliccia, Daniel. 2019. ‚ÄúFourier Spectral Smoothing\nMethod.‚Äù 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  }
]