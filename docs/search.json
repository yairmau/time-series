[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "about\nWelcome to Time Series Analysis for Environmental Sciences (71606) at the Hebrew University of Jerusalem. This is Yair Mau, your host for today. I am a senior lecturer at the Institute of Environmental Sciences, at the Faculty of Agriculture, Food and Environment, in Rehovot, Israel.\nThis website contains (almost) all the material you’ll need for the course. If you find any mistakes, or have any comments, please email me."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "disclaimer",
    "text": "disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#what-who-when-and-where",
    "href": "index.html#what-who-when-and-where",
    "title": "Time Series Analysis",
    "section": "what, who, when and where?",
    "text": "what, who, when and where?\n Course number 71606, 3 academic points\n Yair Mau (lecturer), Erez Feuer (TA)\n Tuesdays, from 14:15 to 17:00\n Computer classroom #16\n Office hours: upon request"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "syllabus",
    "text": "syllabus\n\ncourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\ncourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nlearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\ncourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nbooks and other sources\nClick here.\n\n\ncourse evaluation\nThere will be 2 projects during the semester (each worth 25% of the final grade), and one final project (50%)."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "weekly program",
    "text": "weekly program\n\nweek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nweek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nweek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nweek 4\n\nLecture: Time series plotting: best practices. Dos and don’ts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nweek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nweek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nweek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nweek 8\nSame as lecture 7 above\n\n\nweek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nweek 10\n\nLecture: Fourier decomposition, filtering, Nyquist–Shannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nweek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "who-cares.html#why-time-series-analysis",
    "href": "who-cares.html#why-time-series-analysis",
    "title": "who cares?",
    "section": "why “Time Series Analysis?”",
    "text": "why “Time Series Analysis?”\n\nTime has two aspects. There is the arrow, the running river, without which there is no change, no progress, or direction, or creation. And there is the circle or the cycle, without which there is chaos, meaningless succession of instants, a world without clocks or seasons or promises.\nURSULA K. LE GUIN\n\nYou are here because you are interested in how things change, evolve. In this course I want to discuss with you how to make sense of data whose temporal nature is in its very essence. We will talk about randomness, cycles, frequencies, correlations, and more."
  },
  {
    "objectID": "who-cares.html#why-environmental-sciences",
    "href": "who-cares.html#why-environmental-sciences",
    "title": "who cares?",
    "section": "why “Environmental Sciences”",
    "text": "why “Environmental Sciences”\nThis same time series analysis (TSA) course could be called instead “TSA for finance”, “TSA for Biology”, or any other application. The emphasis in this course is not Environmental Sciences, but the concepts and tools of TSA. Because my research is in Environmental Science, and many of the graduate students at HUJI-Rehovot research this, I chose to use examples “close to home”. The same toolset should be useful for students of other disciplines."
  },
  {
    "objectID": "who-cares.html#what-is-it-good-for",
    "href": "who-cares.html#what-is-it-good-for",
    "title": "who cares?",
    "section": "what is it good for?",
    "text": "what is it good for?\nIn many fields of science we are flooded by data, and it’s hard to see the forest for the trees. I hope that the topics we’ll discuss in this course can help you find meaningful patterns in your data, formulate interesting hypotheses, and design better experiments."
  },
  {
    "objectID": "who-cares.html#do-i-need-it",
    "href": "who-cares.html#do-i-need-it",
    "title": "who cares?",
    "section": "do I need it?",
    "text": "do I need it?\nMaybe. If you are a grad student and you have temporal data to analyze, then probably yes. However, I have very fond memories of courses that I took as a grad student that were completely unrelated to my research. Sometimes “because it’s fun” is a perfectly good answer."
  },
  {
    "objectID": "who-cares.html#what-will-i-actually-gain-from-it",
    "href": "who-cares.html#what-will-i-actually-gain-from-it",
    "title": "who cares?",
    "section": "what will I actually gain from it?",
    "text": "what will I actually gain from it?\nBy the end of this course you will have gained:\n\na hands-on experience of fundamental time-series analysis tools\nan intuition regarding the basic concepts\ntechnical abilities\na springboard for learning more about the subject by yourself"
  },
  {
    "objectID": "basics/boring.html#anaconda",
    "href": "basics/boring.html#anaconda",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.1 Anaconda",
    "text": "1.1 Anaconda\nInstall Anaconda’s Python distribution. The Anaconda installation brings with it all the main python packages we will need to use. In order to install extra packages, refer to these two tutorials: tutorial 1, tutorial 2"
  },
  {
    "objectID": "basics/boring.html#vscode",
    "href": "basics/boring.html#vscode",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.2 VSCode",
    "text": "1.2 VSCode\nInstall VSCode. Visual Studio Code is a very nice IDE (Integrated Development Environment) made by Microsoft, available to all operating systems. Contrary to the title of this page, it is not absolutely necessary to use it, but I like VSCode, and as my student, so do you."
  },
  {
    "objectID": "basics/boring.html#jupyter-notebooks",
    "href": "basics/boring.html#jupyter-notebooks",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.3 jupyter notebooks",
    "text": "1.3 jupyter notebooks\nWe will code exclusively in Jupyter Notebooks. Get acquainted with them. Make sure you can point VSCode to the Anaconda environment of your choice (“base” by default). Don’t worry, this is easier than it sounds."
  },
  {
    "objectID": "basics/boring.html#hujis-computers",
    "href": "basics/boring.html#hujis-computers",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.4 HUJI’s computers",
    "text": "1.4 HUJI’s computers\nIf you are using the computers in the Hebrew University’s computer lab, then:\n\nopen Anaconda Navigator\nin “Environments”, choose “asgard”\nopen VSCode from inside Anaconda Navigator"
  },
  {
    "objectID": "basics/boring.html#folder-structure",
    "href": "basics/boring.html#folder-structure",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.5 folder structure",
    "text": "1.5 folder structure\nYou NEED to be confortable with you computer’s folder (or directory) structure. Where are files located? How to navigate through different folders? How is my stuff organized? If you don’t feel absolutely comfortable with this, then read this, Windows, MacOS. If you use Linux then you surely know this stuff. Make yourself a “time-series” folder wherever you want, and have it backed up regularly (use Google Drive, Dropbox, do it manually, etc). “My dog deleted my files” is not an excuse."
  },
  {
    "objectID": "basics/pandas.html",
    "href": "basics/pandas.html",
    "title": "2  pandas",
    "section": "",
    "text": "We will primarily use the Pandas package to deal with data. Pandas has become the standard Python tool to manipulate time series, and you should get acquainted with its basic usage. This course will provide you the opportunity to learn by example, but I’m sure we will only scratch the surface, and you’ll be left with lots of questions.\nI provide below a (non-comprehensive) list of useful tutorials, they are a good reference for the beginner and for the experienced user.\n\nPython Data Science Handbook, by Jake VanderPlas\nData Wrangling with pandas Cheat Sheet\nWorking with Dates and Times in Python\nCheat Sheet: The pandas DataFrame Object\nYouTube tutorials by Corey Schafer\n\nPandas is based on another famous package: Numpy. We will be using both a lot, and you will start every code with\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "basics/pyplot.html",
    "href": "basics/pyplot.html",
    "title": "3  pyplot",
    "section": "",
    "text": "Matplotlib is probably the most common Python package used for plotting. It is both great and horrible:\n\nGreat: you’ll have absolutely full control of everything you want to plot. The sky is the limit.\nHorrible: you’ll cry as you do it, because there is so much to know, and it is not the most friendly plotting package.\n\nFor our needs, matplotlib will be just fine. Actually, we will be using a submodule called Pyplot, and your fingers will learn to type the following line without thinking at the top of every project you start:\nimport matplotlib.pyplot as plt\nAs for Pandas, you will learn by example, and in my opinion the commands are usually self explanatory. Pyplot is object oriented, so you will usually manipulate the axes object like this.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 2, 0, 3]\n\n# Figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 6))\n# plot on the left\nax1.plot(x, y, color=\"tab:blue\")\nax1.plot(x, y[::-1], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n        ylabel=\"something\",\n        title=\"left panel\")\n# plot on the right\nax2.plot(x, y[::-1])\nax2.set(xlabel=\"date\",\n        ylabel=\"something else\",\n        title=\"right panel\")\n\n[Text(0.5, 0, 'date'),\n Text(0, 0.5, 'something else'),\n Text(0.5, 1.0, 'right panel')]\n\n\n\n\n\nFor the very beginners, you need to know that “figure” refers to the whole white canvas, and “axes” means the rectangle inside which something will be plotted:\n\nIf you are new to all this, I recommend that you go to Earth Lab’s Introduction to Plotting in Python Using Matplotlib, and to Jake VanderPlas’s Python Data Science Handbook"
  },
  {
    "objectID": "basics/example.html#tweaks",
    "href": "basics/example.html#tweaks",
    "title": "4  Learn by example",
    "section": "4.1 Tweaks",
    "text": "4.1 Tweaks\nLet’s change a lot of plotting options to see how things could be different.\n\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,4)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"tab:blue\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax2.set(xlabel=\"date\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=5, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2010-12-25', 395),  xycoords='data',\n    xytext=(-100, 40), textcoords='offset points',\n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"white\"),\n    arrowprops=dict(arrowstyle=\"-&gt;\",\n                    color=\"black\",\n                    connectionstyle=\"angle,angleA=0,angleB=-90,rad=40\"))\n\nText(-100, 40, '2010/11')\n\n\n\n\n\nThe main changes were:\n\nUsing the Seaborn package, we changed the fontsize and the overall plot style. Read more.\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\nWe changed the colors of the lineplots. To know what colors exist, click here.\nThe arrow annotation has a different style. Read more."
  },
  {
    "objectID": "outliers/motivation.html",
    "href": "outliers/motivation.html",
    "title": "5  motivation",
    "section": "",
    "text": "Outliers are observations significantly different from all other observations. Consider, for example, this temperature graph:\n\nWhile most measured points are between 20 and 30 °C, there is obviously something very wrong with the one data point above 80 °C.\nHow could such a thing come about? This could be the result of non-natural causes, such as measurement errors, wrong data collection, or wrong data entry. On the other hand, this point could have natural sources, such as a very hot spark flying next to the temperature sensor.\nIdentifying outliers is important, because they might greatly impact measures like mean and standard deviation. When left untouched, outliers might make us reach wrong conclusions about our data. See what happens to the slope of this linear regression with and without the outliers.\n\n\n\nSource: Zhang (2020)\n\n\n\n\n\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.” ouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "outliers/zscore.html",
    "href": "outliers/zscore.html",
    "title": "6  Z-score",
    "section": "",
    "text": "\\[\nz  = \\frac{x-\\mu}{\\sigma},\n\\]\n\n\nwhere\n\n\\(x=\\) data point,\n\n\\(\\mu=\\) time series mean\n\n\\(\\sigma=\\) time series standard deviation.\n\nLet’s write a function that identifies outliers according to the Z-score.\ndef zscore(df, degree=3):\n    data = df.copy()\n    data['zscore'] = (data - data.mean())/data.std()\n    outliers = data[(data['zscore'] &lt;= -degree) | (data['zscore'] &gt;= degree)]\n    return outliers['value'], data\nNow we can simply use this function:\nthreshold = 2.5\noutliers, transformed = zscore(tx, threshold)\nSource: Atwan (2022)\n\n\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook: Practical Recipes for Exploratory Data Analysis, Data Preparation, Forecasting, and Model Evaluation. Packt."
  },
  {
    "objectID": "outliers/IQR.html",
    "href": "outliers/IQR.html",
    "title": "7  IQR (Inter Quartile Range)",
    "section": "",
    "text": "The IQR (Inter Quartile Range) is the distance between the 25th percentile (Q1) and the 75th percentile (Q3). In a box plot, the whiskers usually extend \\(1.5\\times\\)IQR beyond Q1 and Q3, see below.\n\n\n\nSource: McDonald (2022)\n\n\nA common outlier detection method is to consider whatever points outside the whisker range as outliers.\n\n\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python Library.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57."
  },
  {
    "objectID": "smoothing/convolution.html#convolution",
    "href": "smoothing/convolution.html#convolution",
    "title": "14  convolution",
    "section": "14.1 convolution",
    "text": "14.1 convolution\nConvolution is a fancy word for averaging a time series using a running window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/convolution.html#kernels",
    "href": "smoothing/convolution.html#kernels",
    "title": "14  convolution",
    "section": "14.2 kernels",
    "text": "14.2 kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation (the window in the animation is 4 standard deviations wide)."
  },
  {
    "objectID": "smoothing/convolution.html#math",
    "href": "smoothing/convolution.html#math",
    "title": "14  convolution",
    "section": "14.3 math",
    "text": "14.3 math\nThe definition of a convolution between signal \\(f(t)\\) and kernel \\(k(t)\\) is\n\\[\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\\]\nThe expression \\(f*k\\) denotes the convolution of these two functions. The argument of \\(k\\) is \\(t-\\tau\\), meaning that the kernel runs from left to right (as \\(t\\) does), and at every point the two signals (\\(f\\) and \\(k\\)) are multiplied together. It is the product of the signal with the weight function \\(k\\) that gives us an average. Because of \\(-\\tau\\), the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\\[\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\\]\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/convolution.html#numerics",
    "href": "smoothing/convolution.html#numerics",
    "title": "14  convolution",
    "section": "14.4 numerics",
    "text": "14.4 numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes\ndf['temperature'].rolling(window='20', center=True).mean()\n\nwindow=20 means that the width of the window is 20 points. Pandas lets us define a window width in time units, for example, window='120min'.\ncenter=True is needed in order to assign the result of averaging to the center of the window. Make it False and see what happens.\nmean() is the actual calculation, the average of temperature over the window. The rolling part does not compute anything, it just creates a moving window, and we are free to calculate whatever we want. Try to calculate the standard deviation or the maximum, for example.\n\nIt is implicit in the command above a “rectangular” kernel. What if we want other shapes?\n\n14.4.1 gaussian\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere\n\nwindow_width is an integer, number of points in your window\nstd_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\n\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package. The graph above was achieved by running:\ng = scipy.signal.gaussian(window_width, std)\nplt.plot(g)\n\n\n14.4.2 triangular\nSame idea as gaussian, but simpler, because we don’t need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "title": "14  convolution",
    "section": "14.5 which window shape and width to choose?",
    "text": "14.5 which window shape and width to choose?\n🤷‍♂️\nSorry, there is not definite answer here… It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above.\n\nOne important question you need to ask is: what are the time scales associated with the processes I’m interested in? For example, if I’m interested in the daily temperature pattern, getting rid of 1-minute-long fluctuations would probably be ok. On the other hand, if we were to smooth the signal so much that all that can be seen are the temperature changes between summer and winter, then my smoothing got out of hand, and I threw away the very process I wanted to study.\nAll this is to say that you need to know in advance a few things about the system you are studying, otherwise you can’t know what is “noise” that can be smoothed away."
  },
  {
    "objectID": "smoothing/perfect-smoother.html",
    "href": "smoothing/perfect-smoother.html",
    "title": "16  a perfect smoother",
    "section": "",
    "text": "Source: Eilers (2003)\nGitHub repository\nNoisy series \\(y\\) of length \\(m\\).\nThe smoothed series is called \\(z\\).\nWe have conflicting interests:\n\nwe want a \\(z\\) series “as smooth as possible”.\nhowever, the smoother \\(z\\) is, the farthest from \\(y\\) it will be (low fidelity).\n\nRoughness:\n\\[\nR = \\displaystyle\\sum_i (z_i - z_{i-1})^2\n\\]\nFit to data:\n\\[\nS = \\displaystyle\\sum_i (y_i - z_i)^2\n\\]\nCost functional to be minimized:\n\\[\nQ = S + \\lambda R\n\\]\n\n\n\n\nEilers, Paul HC. 2003. “A Perfect Smoother.” Analytical Chemistry 75 (14): 3631–36. https://doi.org/10.1021/ac034173t."
  },
  {
    "objectID": "stationarity/autocorrelation.html#question",
    "href": "stationarity/autocorrelation.html#question",
    "title": "19  autocorrelation",
    "section": "19.1 question",
    "text": "19.1 question\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nTo answer this, we need to talk about autocorrelation. Let’s start by introducing the necessary concepts."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "19  autocorrelation",
    "section": "19.2 mean and standard deviation",
    "text": "19.2 mean and standard deviation\nLet’s call our time series from above \\(X\\), and its length \\(N\\). Then:\n\\[\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\\]\nThe mean and standard deviation can be visualized thus:\n\nOne last basic concept we need is the expected value: \\[\nE[X] = \\sum_{i=1}^N X_i p_i\n\\]\nFor our time series, the probability \\(p_i\\) that a given point \\(X_i\\) is in the dataset is simply \\(1/N\\), therefore the expectation becomes\n\\[\nE[X] = \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N}\n\\]"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "19  autocorrelation",
    "section": "19.3 autocorrelation",
    "text": "19.3 autocorrelation\nThe autocorrelation of a time series \\(X\\) is the answer to the following question:\n\nif we shift \\(X\\) by \\(\\tau\\) units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are \\(X(t)\\) and \\(X(t+\\tau)\\)?\n\nUsing the Pearson correlation coefficient\n\n\nPearson correlation coefficient between \\(X\\) and \\(Y\\): \\[\n\\rho_{X,Y} = \\frac{E\\left[ (X - \\mu_X)(X_Y - \\mu_Y) \\right]}{\\sigma_X\\sigma_Y}\n\\]\nwe get\n\\[\n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\\]\nA video is worth a billion words, so let’s see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\(\\tau=0\\) (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "21  cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "23  LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "frequency/fourier.html#basic-wave-concepts",
    "href": "frequency/fourier.html#basic-wave-concepts",
    "title": "25  Fourier transform",
    "section": "25.1 basic wave concepts",
    "text": "25.1 basic wave concepts\nThe function\n\\[\nf(t) = B\\sin(2\\pi f t)\n\\tag{25.1}\\]\nhas two basic characteristics, its amplitude \\(B\\) and frequency \\(f\\).\n\nIn the figure above, the amplitude \\(B=0.6\\) and we see that the distance between two peaks is called period, \\(T=2\\) s. The frequency is defined as the inverse of the period:\n\\[\nf = \\frac{1}{T}.\n\\tag{25.2}\\]\nWhen time is in seconds, then the frequency is measured in Hertz (Hz). For the graph above, therefore, we see a wave whose frequency is \\(f = 1/(2 \\text{ s}) = 0.5\\) Hz.\nIn the figure below, we see what happens when we vary the values of the frequency and amplitude.\n\nThe graph above introduces two new characteristics of a wave, its phase \\(\\phi\\), and its offset \\(B\\). A more general description of a sine wave is\n\\[\nf(t) = B\\sin(2\\pi f t + \\phi) + B_0.\n\\tag{25.3}\\]\nThe offset \\(B_0\\) moves the wave up and down, while changing the value of \\(\\phi\\) makes the sine wave move left and right. When the phase \\(\\phi=2\\pi\\), the sine wave will have shifted a full period, and the resulting wave is identical to the original:\n\\[\nB\\sin(2\\pi f t) = B\\sin(2\\pi f t + 2\\pi).\n\\tag{25.4}\\]\nAll the above can also be said about a cosine, whose general for can be given as\n\\[\nA\\cos(2\\pi f t + \\phi) + A_0\n\\tag{25.5}\\]\nOne final point before we jump into the deep waters is that the sine and cosine functions are related through a simple phase shift:\n\\[\n\\cos\\left(2\\pi f t + \\frac{\\pi}{2}\\right) = \\sin\\left(2\\pi f t\\right)\n\\]"
  },
  {
    "objectID": "frequency/fourier.html#fouriers-theorem",
    "href": "frequency/fourier.html#fouriers-theorem",
    "title": "25  Fourier transform",
    "section": "25.2 Fourier’s theorem",
    "text": "25.2 Fourier’s theorem\nFourier’s theorem states that\n\nAny periodic signal is composed of a superposition of pure sine waves, with suitably chosen amplitudes and phases, whose frequencies are harmonics of the fundamental frequency of the signal.\n\nSee the following animations to visualize the theorem in action.\n\nSource: https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Fourier_synthesis_square_wave_animated.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Sawtooth_Fourier_Animation.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Continuous_Fourier_transform_of_rect_and_sinc_functions.gif"
  },
  {
    "objectID": "frequency/fourier.html#fourier-series",
    "href": "frequency/fourier.html#fourier-series",
    "title": "25  Fourier transform",
    "section": "25.3 Fourier series",
    "text": "25.3 Fourier series\n\na periodic function can be described as a sum of sines and cosines.\n\n\n\nNot any function, but certainly most functions we will deal with in this course. The function has to fullful the Dirichlet conditions\nThe classic examples are usually the square function and the sawtooth function:\n\n\n[Source: https://www.geogebra.org/m/tkajbzmg]\nhttps://www.geogebra.org/m/k4eq4fkr\n\n\n\\[\nF[x(t)] = F(f) = \\int_{-\\infty}^{\\infty}x(t)e^{-2\\pi i f t}dt\n\\]\n\\[\nf(t) = \\int_{-\\infty}^{\\infty}F(f)e^{2\\pi i f t}df\n\\]\nhttps://dibsmethodsmeetings.github.io/fourier-transforms/\nhttps://www.jezzamon.com/fourier/index.html"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "href": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "title": "29  seasonal decomposition",
    "section": "29.1 trends in atmospheric carbon dioxide",
    "text": "29.1 trends in atmospheric carbon dioxide\nMauna Loa CO2 concentration.\ndata from NOAA\n\nurl = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url, header=47, na_values=[-999.99])\n\n# you can first download, and then read the csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename, header=35, na_values=[-999.99])\n\ndf\n\n\n\n\n\n\n\n\n1974\n5\n19\n1974.3795\n333.37\n5.1\n-999.99\n-999.99.1\n50.39\n\n\n\n\n0\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n1\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n2\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n3\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n4\n1974\n6\n23\n1974.4753\n331.73\n5\nNaN\nNaN\n49.72\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2565\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2566\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2567\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2568\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2569\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2570 rows × 9 columns\n\n\n\n\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.40\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.06\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.60\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.65\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-26\n2022\n6\n26\n2022.4836\n420.31\n7\n418.14\n395.36\n138.71\n\n\n2022-07-03\n2022\n7\n3\n2022.5027\n419.73\n6\n417.49\n395.15\n138.64\n\n\n2022-07-10\n2022\n7\n10\n2022.5219\n419.08\n6\n417.25\n394.59\n138.52\n\n\n2022-07-17\n2022\n7\n17\n2022.5411\n418.43\n6\n417.14\n394.64\n138.41\n\n\n2022-07-24\n2022\n7\n24\n2022.5603\n417.84\n6\n415.68\n394.11\n138.36\n\n\n\n\n2515 rows × 9 columns\n\n\n\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,6))\nax.plot(df['average'])\nax.set(xlabel=\"date\",\n       ylabel=\"CO2 concentration (ppm)\",\n       # ylim=[0, 430],\n       title=\"Mauna Loa CO2 concentration\");\n\nKeyError: 'average'\n\n\n\n\n\nfill missing data. interpolate method: ‘time’\ninterpolation methods visualized\n\ndf['co2'] = (df['average'].resample(\"D\") #resample daily\n                          .interpolate(method='time') #interpolate by time\n            )\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\nco2\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.40\n333.37\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.06\n332.95\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.60\n332.35\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.65\n332.20\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n332.37\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-26\n2022\n6\n26\n2022.4836\n420.31\n7\n418.14\n395.36\n138.71\n420.31\n\n\n2022-07-03\n2022\n7\n3\n2022.5027\n419.73\n6\n417.49\n395.15\n138.64\n419.73\n\n\n2022-07-10\n2022\n7\n10\n2022.5219\n419.08\n6\n417.25\n394.59\n138.52\n419.08\n\n\n2022-07-17\n2022\n7\n17\n2022.5411\n418.43\n6\n417.14\n394.64\n138.41\n418.43\n\n\n2022-07-24\n2022\n7\n24\n2022.5603\n417.84\n6\n415.68\n394.11\n138.36\n417.84\n\n\n\n\n2515 rows × 10 columns"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#decompose-data",
    "href": "seasonality/seasonal-decomposition.html#decompose-data",
    "title": "29  seasonal decomposition",
    "section": "29.2 decompose data",
    "text": "29.2 decompose data\nseasonal_decompose returns an object with four components:\n\nobserved: \\(Y(t)\\)\ntrend: \\(T(t)\\)\nseasonal: \\(S(t)\\)\nresid: \\(e(t)\\)\n\nAdditive model: \\[\nY(t) = T(t) + S(t) + e(t)\n\\]\nMultiplicative model: \\[\nY(t) = T(t) \\times S(t) \\times e(t)\n\\]\n\n29.2.0.1 Interlude\nlearn how to use zip in a loop\n\nletters = ['a', 'b', 'c', 'd', 'e']\nnumbers = [1, 2, 3, 4, 5]\n# zip let's us iterate over to lists at the same time\nfor l, n in zip(letters, numbers):\n    print(f\"{l} = {n}\")\n\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n\nPlot each component separately.\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\ndecomposed_m = seasonal_decompose(df['co2'], model='multiplicative')\ndecomposed_a = seasonal_decompose(df['co2'], model='additive')\ndecomposed = decomposed_m\npos = (0.5, 0.9)\ncomponents =[\"observed\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(decomposed, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\n\n\n\n\n\n# %matplotlib widget\n\ndecomposed = decomposed_m\n\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df['co2'], color=\"tab:blue\", label=\"observed\")\nax[0].plot(decomposed.trend * decomposed.resid, color=\"tab:orange\", label=\"trend*resid\")\nax[0].plot(decomposed.trend * decomposed.seasonal, color=\"tab:red\", label=\"trend*seasonal\")\nax[0].plot(decomposed.trend, color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"CO$_2$ concentration (ppm)\",\n          title=\"Mauna Loa CO$_2$ concentration\")\nax[0].legend(frameon=False)\n\nstart = \"2000-01-01\"\nend = \"2003-01-01\"\nzoom = slice(start, end)\nax[1].plot(df.loc[zoom, 'co2'], color=\"tab:blue\", label=\"observed\")\nax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=\"tab:orange\", label=\"trend*resid\")\nax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=\"tab:red\", label=\"trend*seasonal\")\nax[1].plot(decomposed.trend[zoom], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"Components, 2000--2003\");"
  },
  {
    "objectID": "rates-of-change/finite-differences.html",
    "href": "rates-of-change/finite-differences.html",
    "title": "33  finite differences",
    "section": "",
    "text": "Definition of a derivative:\n\\[\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n\\]\nNumerically, we can approximate the derivative \\(f'(t)\\) of a time series \\(f(t)\\) as\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{33.1}\\]\n\n\nThe expression \\(\\mathcal{O}(\\Delta t)\\) means that the error associated with the approximation is proportional to \\(\\Delta t\\). This is called “Big O notation”.\nThe expression above is called the two-point forward difference formula. Likewise, we can define the two-point backward difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{33.2}\\]\nIf we sum together Equation 33.1 and Equation 33.2 we get:\n\n\\[\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n&= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n\\tag{33.3}\\]\nDividing both sides by 2 gives the two-point central difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n\\tag{33.4}\\]\nTwo things are worth mentioning about the approximation above:\n\nit is balanced, that is, there is no preference of the future over the past.\nits error is proportional to \\(\\Delta t^2\\), it is a lot more precise than the unbalanced approximations :)\n\n\n\nTo understand why the error is proportional to \\(\\Delta t^2\\), one can subtract the Taylor expansion of \\(f(t-\\Delta t)\\) from the Taylor expansion of \\(f(t+\\Delta t)\\). See this, pages 3 and 4.\n\nThe function np.gradient calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n\nThe “gradient” usually refers to a first derivative with respect to space, and it is denoted as \\(\\nabla f(x)=\\frac{df(x)}{dx}\\). However, it doesn’t really matter if we call the independent variable \\(x\\) or \\(t\\), the derivative operator is exactly the same.\nCheck out this nice example."
  },
  {
    "objectID": "rates-of-change/fourier-based-derivatives.html",
    "href": "rates-of-change/fourier-based-derivatives.html",
    "title": "34  Fourier-based derivatives",
    "section": "",
    "text": "This tutorial is based on Pelliccia (2019).\nnice trick: https://math.stackexchange.com/questions/430858/fourier-transform-of-derivative\n\n\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing Method.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "technical stuff",
    "section": "operating systems",
    "text": "operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "technical stuff",
    "section": "software",
    "text": "software\nAnaconda’s Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "technical stuff",
    "section": "python packages",
    "text": "python packages\nKats — a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "references.html#books",
    "href": "references.html#books",
    "title": "sources",
    "section": "books",
    "text": "books\nfrom Data to Viz\nFundamentals of Data Visualization, by Claus O. Wilke\nPyNotes in Agriscience\nForecasting: Principles and Practice (3rd ed), by Rob J Hyndman and George Athanasopoulos\nPython for Finance Cookbook 2nd Edition - Code Repository\nPractical time series analysis,: prediction with statistics and machine learning, by Aileen Nielsen\nThe online edition of this book is available for Hebrew University staff and students.\nTime series analysis with Python cookbook : practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation, by Tarek A. Atwan\nThe online edition of this book is available for Hebrew University staff and students.\nHands-on Time Series Analysis with Python: From Basics to Bleeding Edge Techniques, by B V Vishwas, Ashish Patel\nThe online edition of this book is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#videos",
    "href": "references.html#videos",
    "title": "sources",
    "section": "videos",
    "text": "videos\nTimes Series Analysis for Everyone, by Bruno Goncalves\nThis series is available for Hebrew University staff and students.\nTime Series Analysis with Pandas, by Joshua Malina This video is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "sources",
    "section": "references",
    "text": "references\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook:\nPractical Recipes for Exploratory Data Analysis, Data Preparation,\nForecasting, and Model Evaluation. Packt.\n\n\nEilers, Paul HC. 2003. “A Perfect Smoother.” Analytical\nChemistry 75 (14): 3631–36. https://doi.org/10.1021/ac034173t.\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python\nLibrary.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57.\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing\nMethod.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/.\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.”\nouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  }
]