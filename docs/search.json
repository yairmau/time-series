[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "about\nWelcome to Time Series Analysis for Environmental Sciences (71606) at the Hebrew University of Jerusalem. This is Yair Mau, your host for today. I am a senior lecturer at the Institute of Environmental Sciences, at the Faculty of Agriculture, Food and Environment, in Rehovot, Israel.\nThis website contains (almost) all the material you’ll need for the course. If you find any mistakes, or have any comments, please email me."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "disclaimer",
    "text": "disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#what-who-when-and-where",
    "href": "index.html#what-who-when-and-where",
    "title": "Time Series Analysis",
    "section": "what, who, when and where?",
    "text": "what, who, when and where?\n Course number 71606, 3 academic points\n Yair Mau (lecturer), Erez Feuer (TA)\n Tuesdays, from 14:15 to 17:00\n Computer classroom #18\n Office hours: upon request"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "syllabus",
    "text": "syllabus\n\ncourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\ncourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nlearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\ncourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nbooks and other sources\nClick here.\n\n\ncourse evaluation\nThere will be 2 projects during the semester (each worth 25% of the final grade), and one final project (50%)."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "weekly program",
    "text": "weekly program\n\nweek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nweek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nweek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nweek 4\n\nLecture: Time series plotting: best practices. Dos and don’ts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nweek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nweek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nweek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nweek 8\nSame as lecture 7 above\n\n\nweek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nweek 10\n\nLecture: Fourier decomposition, filtering, Nyquist–Shannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nweek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "who-cares.html#why-time-series-analysis",
    "href": "who-cares.html#why-time-series-analysis",
    "title": "who cares?",
    "section": "why “Time Series Analysis?”",
    "text": "why “Time Series Analysis?”\n\nTime has two aspects. There is the arrow, the running river, without which there is no change, no progress, or direction, or creation. And there is the circle or the cycle, without which there is chaos, meaningless succession of instants, a world without clocks or seasons or promises.\nURSULA K. LE GUIN\n\nYou are here because you are interested in how things change, evolve. In this course I want to discuss with you how to make sense of data whose temporal nature is in its very essence. We will talk about randomness, cycles, frequencies, correlations, and more."
  },
  {
    "objectID": "who-cares.html#why-environmental-sciences",
    "href": "who-cares.html#why-environmental-sciences",
    "title": "who cares?",
    "section": "why “Environmental Sciences”",
    "text": "why “Environmental Sciences”\nThis same time series analysis (TSA) course could be called instead “TSA for finance”, “TSA for Biology”, or any other application. The emphasis in this course is not Environmental Sciences, but the concepts and tools of TSA. Because my research is in Environmental Science, and many of the graduate students at HUJI-Rehovot research this, I chose to use examples “close to home”. The same toolset should be useful for students of other disciplines."
  },
  {
    "objectID": "who-cares.html#what-is-it-good-for",
    "href": "who-cares.html#what-is-it-good-for",
    "title": "who cares?",
    "section": "what is it good for?",
    "text": "what is it good for?\nIn many fields of science we are flooded by data, and it’s hard to see the forest for the trees. I hope that the topics we’ll discuss in this course can help you find meaningful patterns in your data, formulate interesting hypotheses, and design better experiments."
  },
  {
    "objectID": "who-cares.html#do-i-need-it",
    "href": "who-cares.html#do-i-need-it",
    "title": "who cares?",
    "section": "do I need it?",
    "text": "do I need it?\nMaybe. If you are a grad student and you have temporal data to analyze, then probably yes. However, I have very fond memories of courses that I took as a grad student that were completely unrelated to my research. Sometimes “because it’s fun” is a perfectly good answer."
  },
  {
    "objectID": "who-cares.html#what-will-i-actually-gain-from-it",
    "href": "who-cares.html#what-will-i-actually-gain-from-it",
    "title": "who cares?",
    "section": "what will I actually gain from it?",
    "text": "what will I actually gain from it?\nBy the end of this course you will have gained:\n\na hands-on experience of fundamental time-series analysis tools\nan intuition regarding the basic concepts\ntechnical abilities\na springboard for learning more about the subject by yourself"
  },
  {
    "objectID": "basics/boring.html#anaconda",
    "href": "basics/boring.html#anaconda",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.1 Anaconda",
    "text": "1.1 Anaconda\nInstall Anaconda’s Python distribution. The Anaconda installation brings with it all the main python packages we will need to use. In order to install extra packages, refer to these two tutorials: tutorial 1, tutorial 2"
  },
  {
    "objectID": "basics/boring.html#vscode",
    "href": "basics/boring.html#vscode",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.2 VSCode",
    "text": "1.2 VSCode\nInstall VSCode. Visual Studio Code is a very nice IDE (Integrated Development Environment) made by Microsoft, available to all operating systems. Contrary to the title of this page, it is not absolutely necessary to use it, but I like VSCode, and as my student, so do you."
  },
  {
    "objectID": "basics/boring.html#jupyter-notebooks",
    "href": "basics/boring.html#jupyter-notebooks",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.3 jupyter notebooks",
    "text": "1.3 jupyter notebooks\nWe will code exclusively in Jupyter Notebooks. Get acquainted with them. Make sure you can point VSCode to the Anaconda environment of your choice (“base” by default). Don’t worry, this is easier than it sounds."
  },
  {
    "objectID": "basics/boring.html#hujis-computers",
    "href": "basics/boring.html#hujis-computers",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.4 HUJI’s computers",
    "text": "1.4 HUJI’s computers\nIf you are using the computers in the Hebrew University’s computer lab, then:\n\nopen Anaconda Navigator\nin “Environments”, choose “asgard”\nopen VSCode from inside Anaconda Navigator"
  },
  {
    "objectID": "basics/boring.html#folder-structure",
    "href": "basics/boring.html#folder-structure",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.5 folder structure",
    "text": "1.5 folder structure\nYou NEED to be confortable with you computer’s folder (or directory) structure. Where are files located? How to navigate through different folders? How is my stuff organized? If you don’t feel absolutely comfortable with this, then read this, Windows, MacOS. If you use Linux then you surely know this stuff. Make yourself a “time-series” folder wherever you want, and have it backed up regularly (use Google Drive, Dropbox, do it manually, etc). “My dog deleted my files” is not an excuse."
  },
  {
    "objectID": "basics/pandas.html",
    "href": "basics/pandas.html",
    "title": "2  pandas",
    "section": "",
    "text": "We will primarily use the Pandas package to deal with data. Pandas has become the standard Python tool to manipulate time series, and you should get acquainted with its basic usage. This course will provide you the opportunity to learn by example, but I’m sure we will only scratch the surface, and you’ll be left with lots of questions.\nI provide below a (non-comprehensive) list of useful tutorials, they are a good reference for the beginner and for the experienced user.\n\nPython Data Science Handbook, by Jake VanderPlas\nData Wrangling with pandas Cheat Sheet\nWorking with Dates and Times in Python\nCheat Sheet: The pandas DataFrame Object\nYouTube tutorials by Corey Schafer\n\nPandas is based on another famous package: Numpy. We will be using both a lot, and you will start every code with\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "basics/pyplot.html",
    "href": "basics/pyplot.html",
    "title": "3  pyplot",
    "section": "",
    "text": "Matplotlib is probably the most common Python package used for plotting. It is both great and horrible:\n\nGreat: you’ll have absolutely full control of everything you want to plot. The sky is the limit.\nHorrible: you’ll cry as you do it, because there is so much to know, and it is not the most friendly plotting package.\n\nFor our needs, matplotlib will be just fine. Actually, we will be using a submodule called Pyplot, and your fingers will learn to type the following line without thinking at the top of every project you start:\nimport matplotlib.pyplot as plt\nAs for Pandas, you will learn by example, and in my opinion the commands are usually self explanatory. Pyplot is object oriented, so you will usually manipulate the axes object like this.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 2, 0, 3]\n\n# Figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 6))\n# plot on the left\nax1.plot(x, y, color=\"tab:blue\")\nax1.plot(x, y[::-1], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n        ylabel=\"something\",\n        title=\"left panel\")\n# plot on the right\nax2.plot(x, y[::-1])\nax2.set(xlabel=\"date\",\n        ylabel=\"something else\",\n        title=\"right panel\")\n\n[Text(0.5, 0, 'date'),\n Text(0, 0.5, 'something else'),\n Text(0.5, 1.0, 'right panel')]\n\n\n\n\n\nFor the very beginners, you need to know that “figure” refers to the whole white canvas, and “axes” means the rectangle inside which something will be plotted:\n\nIf you are new to all this, I recommend that you go to Earth Lab’s Introduction to Plotting in Python Using Matplotlib, and to Jake VanderPlas’s Python Data Science Handbook"
  },
  {
    "objectID": "basics/example.html#open-a-new-jupyter-notebook",
    "href": "basics/example.html#open-a-new-jupyter-notebook",
    "title": "4  learn by example",
    "section": "4.1 open a new Jupyter Notebook",
    "text": "4.1 open a new Jupyter Notebook\n\nOn your computer, open the program Anaconda Navigator (it may take a while to load).\nFind the white box called VS Code and click Launch.\nNow go to File &gt; Open Folder, and open the folder you created for this course. VS Code may ask you if you trust the authors, and the answer is “yes” (it’s your computer).\nFile &gt; New File, and call it example.ipynb\nYou can start copying and pasting code from this website to your Jupyter Notebook. To run a cell, press Shift+Enter.\nYou may be asked to choose to Select Kernel. This is VS Code wanting to know which python installation to use. Click on “Python Environments”, and then choose the option with the word anaconda in it.\nThat’s all! Congratulations!"
  },
  {
    "objectID": "basics/example.html#import-packages",
    "href": "basics/example.html#import-packages",
    "title": "4  learn by example",
    "section": "4.2 import packages",
    "text": "4.2 import packages\nFirst, import packages to be used. They should all be already included in the Anaconda distribution you installed.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters"
  },
  {
    "objectID": "basics/example.html#load-data",
    "href": "basics/example.html#load-data",
    "title": "4  learn by example",
    "section": "4.3 load data",
    "text": "4.3 load data\nLoad CO2 data into a Pandas dataframe. You can load it directly from the URL (option 1), or first download the CSV to your computer and then load it (option 2). The link to download the data directly form NOAA is this. If for some reason this doesn’t work, download from this website’s GitHub repository.\n\n# option 1: load data directly from URL\n# url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url, header=34, na_values=[-999.99])\n\n# option 2: download first (use the URL above and save it to your computer), then load csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename,\n                 header=34,  # use row 34 as column names. Whatever above it is ignored\n                 na_values=[-999.99]  # substitute -999.99 for NaN (Not a Number), data not available\n                 )\n# check how the dataframe (table) looks like\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\n\n\n0\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.39\n\n\n1\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n2\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n3\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n4\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2566\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2567\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2568\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2569\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2570\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2571 rows × 9 columns"
  },
  {
    "objectID": "basics/example.html#dealing-with-dates",
    "href": "basics/example.html#dealing-with-dates",
    "title": "4  learn by example",
    "section": "4.4 dealing with dates",
    "text": "4.4 dealing with dates\nCreate a new column called date, that combines the information from three separate columns: year, month, day.\n\n# function to_datetime translates the full date into a pandas datetime object,\n# that is, pandas knows this is a date, it's not just a string\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n# make 'date' column the dataframe index\ndf = df.set_index('date')\n# now see if everything is ok\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.39\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-07-23\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2023-07-30\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2023-08-06\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2023-08-13\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2023-08-20\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2571 rows × 9 columns"
  },
  {
    "objectID": "basics/example.html#first-plot",
    "href": "basics/example.html#first-plot",
    "title": "4  learn by example",
    "section": "4.5 first plot",
    "text": "4.5 first plot\nWe are now ready for our first plot! Let’s see the weekly CO2 average.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()"
  },
  {
    "objectID": "basics/example.html#first-plot-v2.0",
    "href": "basics/example.html#first-plot-v2.0",
    "title": "4  learn by example",
    "section": "4.6 first plot, v2.0",
    "text": "4.6 first plot, v2.0\nThe dates in the x-label are not great. Let’s try to make them prettier.\nWe need to import a few more packages first.\n\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()  # datetime converter for a matplotlib\n\nNow let’s replot.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2011-12-25', 389),  xycoords='data',\n    xytext=(-10, -80), textcoords='offset points',\n    arrowprops=dict(arrowstyle=\"-&gt;\",\n                    color=\"black\",\n                    connectionstyle=\"arc3,rad=0.2\"))\nfig.savefig(\"CO2-graph.png\", dpi=300)\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_31828/850389963.py:42: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.savefig(\"CO2-graph.png\", dpi=300)\n/opt/anaconda3/lib/python3.9/site-packages/IPython/core/events.py:89: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  func(*args, **kwargs)\n/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nThe dates on the horizontal axis are determined thus:\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nThis deremines the location of the ticks (between 3 and 5 ticks, whatever “works best”)\nax1.xaxis.set_major_locator(locator)\nThis actually puts the ticks in the positions determined above\nformatter = mdates.ConciseDateFormatter(locator)\nThis says that the labels will be placed at the locations determined in 1.\nax1.xaxis.set_major_formatter(formatter)\nFinally, labels are written down\n\nThe arrow is placed in the graph using annotate. It has a tricky syntax and a million options. Read Jake VanderPlas’s excellent examples to learn more."
  },
  {
    "objectID": "basics/example.html#modifications",
    "href": "basics/example.html#modifications",
    "title": "4  learn by example",
    "section": "4.7 modifications",
    "text": "4.7 modifications\nLet’s change a lot of plotting options to see how things could be different.\n\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,4)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"tab:blue\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax2.set(xlabel=\"date\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=5, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2010-12-25', 395),  xycoords='data',\n    xytext=(-100, 40), textcoords='offset points',\n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"white\"),\n    arrowprops=dict(arrowstyle=\"-&gt;\",\n                    color=\"black\",\n                    connectionstyle=\"angle,angleA=0,angleB=-90,rad=40\"))\n\nText(-100, 40, '2010/11')\n\n\n\n\n\nThe main changes were:\n\nUsing the Seaborn package, we changed the fontsize and the overall plot style. Read more.\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\nWe changed the colors of the lineplots. To know what colors exist, click here.\nThe arrow annotation has a different style. Read more."
  },
  {
    "objectID": "basics/example.html#playing-with-the-code",
    "href": "basics/example.html#playing-with-the-code",
    "title": "4  learn by example",
    "section": "4.8 playing with the code",
    "text": "4.8 playing with the code\nI encourage you to play with the code you just ran. An easy way of learning what each line does is to comment something out and see what changes in the output you see. If you feel brave, try to modify the code a little bit."
  },
  {
    "objectID": "resampling/motivation.html#jerusalem-2019",
    "href": "resampling/motivation.html#jerusalem-2019",
    "title": "5  motivation",
    "section": "5.1 Jerusalem, 2019",
    "text": "5.1 Jerusalem, 2019\nData from the Israel Meteorological Service, IMS.\n\n\n\n\n\n\n\n\ndiscussion\nThe temperature fluctuates on various time scales, from daily to yearly. Let’s think together a few questions we’d like to ask about the data above.\n\nNow let’s see precipitation data:\n\n\n\n\n\n\n\n\ndiscussion\nWhat would be interesting to know about precipitation?\n\nWe have not talked about what kind of data we have in our hands here. The csv file provided by the IMS looks like this:\n\n\n\n\n\n\n\n\n\nStation\nDate & Time (Winter)\nDiffused radiation (W/m^2)\nGlobal radiation (W/m^2)\nDirect radiation (W/m^2)\nRelative humidity (%)\nTemperature (°C)\nMaximum temperature (°C)\nMinimum temperature (°C)\nWind direction (°)\nGust wind direction (°)\nWind speed (m/s)\nMaximum 1 minute wind speed (m/s)\nMaximum 10 minutes wind speed (m/s)\nTime ending maximum 10 minutes wind speed (hhmm)\nGust wind speed (m/s)\nStandard deviation wind direction (°)\nRainfall (mm)\n\n\n\n\n0\nJerusalem Givat Ram\n01/01/2019 00:00\n0.0\n0.0\n0.0\n80.0\n8.7\n8.8\n8.6\n75.0\n84.0\n3.3\n4.3\n3.5\n23:58\n6.0\n15.6\n0.0\n\n\n1\nJerusalem Givat Ram\n01/01/2019 00:10\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n74.0\n82.0\n3.3\n4.1\n3.3\n00:01\n4.9\n14.3\n0.0\n\n\n2\nJerusalem Givat Ram\n01/01/2019 00:20\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n76.0\n82.0\n3.2\n4.1\n3.3\n00:19\n4.9\n9.9\n0.0\n\n\n3\nJerusalem Givat Ram\n01/01/2019 00:30\n0.0\n0.0\n0.0\n79.0\n8.7\n8.7\n8.6\n78.0\n73.0\n3.6\n4.2\n3.6\n00:30\n5.2\n11.7\n0.0\n\n\n4\nJerusalem Givat Ram\n01/01/2019 00:40\n0.0\n0.0\n0.0\n79.0\n8.6\n8.7\n8.5\n80.0\n74.0\n3.6\n4.4\n3.8\n00:35\n5.4\n10.5\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n52549\nJerusalem Givat Ram\n31/12/2019 22:20\n0.0\n0.0\n1.0\n81.0\n7.4\n7.6\n7.3\n222.0\n255.0\n0.5\n0.9\n1.0\n22:11\n1.0\n47.9\n0.0\n\n\n52550\nJerusalem Givat Ram\n31/12/2019 22:30\n0.0\n0.0\n1.0\n83.0\n7.3\n7.4\n7.3\n266.0\n259.0\n0.6\n0.8\n0.6\n22:28\n1.1\n22.8\n0.0\n\n\n52551\nJerusalem Givat Ram\n31/12/2019 22:40\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.3\n331.0\n317.0\n0.5\n0.8\n0.6\n22:35\n1.0\n31.6\n0.0\n\n\n52552\nJerusalem Givat Ram\n31/12/2019 22:50\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.4\n312.0\n285.0\n0.6\n1.0\n0.6\n22:50\n1.4\n31.3\n0.0\n\n\n52553\nJerusalem Givat Ram\n31/12/2019 23:00\n0.0\n0.0\n1.0\n83.0\n7.6\n7.7\n7.4\n315.0\n321.0\n0.7\n1.0\n0.8\n22:54\n1.3\n23.5\n0.0\n\n\n\n\n52554 rows × 18 columns\n\n\n\nWe see that we have data points spaced out evenly every 10 minutes."
  },
  {
    "objectID": "resampling/motivation.html#monthly-summaries",
    "href": "resampling/motivation.html#monthly-summaries",
    "title": "5  motivation",
    "section": "5.2 Monthly summaries",
    "text": "5.2 Monthly summaries\nLet’s try to answer the following questions:\n\n\n\n\n\n\n What is the mean temperature for each month?\n\n\n\n\n\nFirst we have to divide temperature data by month, and then take the average for each month.\n\n\n\n\n\n\n\n\n\n What is the mean daily max/min temperature for each month?\n\n\n\n\n\nThis is a bit trickier.\n\nWe need to find the maximum/minimum temperature for each day.\nOnly then we split the daily data by month and take the average.\n\n\n\n\n\n\n\n\n\n\n How much rain was there every month?\n\n\n\n\n\nWe have to divide rain data by month, and then sum the totals of each month.\n\n\n\n\n\n\n\n\n\n How many rainy days were there each month?\n\n\n\n\n\n\nWe need to sum rain by day.\nWe need to count how many days are there each month where rain &gt; 0."
  },
  {
    "objectID": "resampling/resampling.html",
    "href": "resampling/resampling.html",
    "title": "6  resampling",
    "section": "",
    "text": "We can only really understand how to calculate monthly means if we do it ourselves.\nFirst, let’s import a bunch of packages we need to use.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport altair as alt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\nNow we load the csv file for Jerusalem (2019), provided by the IMS.\n\nWe substitute every occurence of - for NaN (not a number, that is, the data is missing).\nWe call the columns Temperature (°C) and Rainfall (mm) with more convenient names, since we will be using them a lot.\nWe interpret the column Date & Time (Winter) as a date, saying to python that day comes first.\nWe make date the index of the dataframe.\n\n\nfilename = \"../archive/data/jerusalem2019.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Temperature (°C)': 'temperature',\n                   'Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nStation\nDate & Time (Winter)\nDiffused radiation (W/m^2)\nGlobal radiation (W/m^2)\nDirect radiation (W/m^2)\nRelative humidity (%)\ntemperature\nMaximum temperature (°C)\nMinimum temperature (°C)\nWind direction (°)\nGust wind direction (°)\nWind speed (m/s)\nMaximum 1 minute wind speed (m/s)\nMaximum 10 minutes wind speed (m/s)\nTime ending maximum 10 minutes wind speed (hhmm)\nGust wind speed (m/s)\nStandard deviation wind direction (°)\nrain\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-01-01 00:00:00\nJerusalem Givat Ram\n01/01/2019 00:00\n0.0\n0.0\n0.0\n80.0\n8.7\n8.8\n8.6\n75.0\n84.0\n3.3\n4.3\n3.5\n23:58\n6.0\n15.6\n0.0\n\n\n2019-01-01 00:10:00\nJerusalem Givat Ram\n01/01/2019 00:10\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n74.0\n82.0\n3.3\n4.1\n3.3\n00:01\n4.9\n14.3\n0.0\n\n\n2019-01-01 00:20:00\nJerusalem Givat Ram\n01/01/2019 00:20\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n76.0\n82.0\n3.2\n4.1\n3.3\n00:19\n4.9\n9.9\n0.0\n\n\n2019-01-01 00:30:00\nJerusalem Givat Ram\n01/01/2019 00:30\n0.0\n0.0\n0.0\n79.0\n8.7\n8.7\n8.6\n78.0\n73.0\n3.6\n4.2\n3.6\n00:30\n5.2\n11.7\n0.0\n\n\n2019-01-01 00:40:00\nJerusalem Givat Ram\n01/01/2019 00:40\n0.0\n0.0\n0.0\n79.0\n8.6\n8.7\n8.5\n80.0\n74.0\n3.6\n4.4\n3.8\n00:35\n5.4\n10.5\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019-12-31 22:20:00\nJerusalem Givat Ram\n31/12/2019 22:20\n0.0\n0.0\n1.0\n81.0\n7.4\n7.6\n7.3\n222.0\n255.0\n0.5\n0.9\n1.0\n22:11\n1.0\n47.9\n0.0\n\n\n2019-12-31 22:30:00\nJerusalem Givat Ram\n31/12/2019 22:30\n0.0\n0.0\n1.0\n83.0\n7.3\n7.4\n7.3\n266.0\n259.0\n0.6\n0.8\n0.6\n22:28\n1.1\n22.8\n0.0\n\n\n2019-12-31 22:40:00\nJerusalem Givat Ram\n31/12/2019 22:40\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.3\n331.0\n317.0\n0.5\n0.8\n0.6\n22:35\n1.0\n31.6\n0.0\n\n\n2019-12-31 22:50:00\nJerusalem Givat Ram\n31/12/2019 22:50\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.4\n312.0\n285.0\n0.6\n1.0\n0.6\n22:50\n1.4\n31.3\n0.0\n\n\n2019-12-31 23:00:00\nJerusalem Givat Ram\n31/12/2019 23:00\n0.0\n0.0\n1.0\n83.0\n7.6\n7.7\n7.4\n315.0\n321.0\n0.7\n1.0\n0.8\n22:54\n1.3\n23.5\n0.0\n\n\n\n\n52554 rows × 18 columns\n\n\n\nWith resample it’s easy to compute monthly averages. Resample by itself only divides the data into buckets (in this case monthly buckets), and waits for a further instruction. Here, the next instruction is mean.\n\ndf_month = df['temperature'].resample('M').mean()\ndf_month\n\ndate\n2019-01-31     9.119937\n2019-02-28     9.629812\n2019-03-31    10.731571\n2019-04-30    14.514329\n2019-05-31    22.916894\n2019-06-30    23.587361\n2019-07-31    24.019403\n2019-08-31    24.050822\n2019-09-30    22.313287\n2019-10-31    20.641868\n2019-11-30    17.257153\n2019-12-31    11.224131\nFreq: M, Name: temperature, dtype: float64\n\n\nInstead of M for month, which other options do I have? The full list can be found here, but the most commonly used are:\nM         month end frequency\nMS        month start frequency\nA         year end frequency\nAS, YS    year start frequency\nD         calendar day frequency\nH         hourly frequency\nT, min    minutely frequency\nS         secondly frequency\nThe results we got for the monthly means were given as a pandas series, not dataframe. Let’s correct this:\n\ndf_month = df['temperature'].resample('M').mean().to_frame('mean temp')\ndf_month\n\n\n\n\n\n\n\n\nmean temp\n\n\ndate\n\n\n\n\n\n2019-01-31\n9.119937\n\n\n2019-02-28\n9.629812\n\n\n2019-03-31\n10.731571\n\n\n2019-04-30\n14.514329\n\n\n2019-05-31\n22.916894\n\n\n2019-06-30\n23.587361\n\n\n2019-07-31\n24.019403\n\n\n2019-08-31\n24.050822\n\n\n2019-09-30\n22.313287\n\n\n2019-10-31\n20.641868\n\n\n2019-11-30\n17.257153\n\n\n2019-12-31\n11.224131\n\n\n\n\n\n\n\nNow it’s time to plot!\n\nfig, ax = plt.subplots()\nax.plot(df_month['mean temp'], color='black')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\")\n\n[Text(0, 0.5, 'Temperature (°C)'),\n [&lt;matplotlib.axis.YTick at 0x7fa878083ac0&gt;,\n  &lt;matplotlib.axis.YTick at 0x7fa878083fd0&gt;,\n  &lt;matplotlib.axis.YTick at 0x7fa8780830a0&gt;,\n  &lt;matplotlib.axis.YTick at 0x7fa877fa0280&gt;,\n  &lt;matplotlib.axis.YTick at 0x7fa877fa0a60&gt;,\n  &lt;matplotlib.axis.YTick at 0x7fa877fa6550&gt;],\n Text(0.5, 1.0, 'Jerusalem, 2019')]\n\n\n\n\n\nAlthough all the calculations are correct, the graph is not great.\n\nEach monthly average was assigned to the last day of the month.\nThe ticks on the x-axis are on the first of the month, so there is a mismatch between data and labels.\n\nWe will resample now using MS, which assigns the result to the first of the month (Month Start), and we will then add 14 more days to it (left offset), so that every point is assigned to the middle (15th) of the month.\n\ndf_month = df['temperature'].resample('MS',loffset=pd.Timedelta(14, 'd')).mean().to_frame('mean temp')\ndf_month\n\n\n\n\n\n\n\n\nmean temp\n\n\ndate\n\n\n\n\n\n2019-01-15\n9.119937\n\n\n2019-02-15\n9.629812\n\n\n2019-03-15\n10.731571\n\n\n2019-04-15\n14.514329\n\n\n2019-05-15\n22.916894\n\n\n2019-06-15\n23.587361\n\n\n2019-07-15\n24.019403\n\n\n2019-08-15\n24.050822\n\n\n2019-09-15\n22.313287\n\n\n2019-10-15\n20.641868\n\n\n2019-11-15\n17.257153\n\n\n2019-12-15\n11.224131\n\n\n\n\n\n\n\nNow let’s plot again, and we’ll choose a cleaner x-axis labeling.\n\nfig, ax = plt.subplots()\nax.plot(df_month['mean temp'], color='black')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\")\nax.xaxis.set_major_locator(mdates.MonthLocator(range(1, 13, 2), bymonthday=15))\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\n\n\n\n\nWe’re done! Congratulations :)\nNow we need to calculate the average minimum/maximum daily temperatures. We start by creating an empty dataframe.\n\ndf_day = pd.DataFrame()\n\nNow resample data by day (D), and take the min/max of each day.\n\ndf_day['min temp'] = df['temperature'].resample('D').min()\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_day\n\n\n\n\n\n\n\n\nmin temp\nmax temp\n\n\ndate\n\n\n\n\n\n\n2019-01-01\n7.5\n14.1\n\n\n2019-01-02\n6.6\n11.5\n\n\n2019-01-03\n6.3\n10.7\n\n\n2019-01-04\n6.6\n14.6\n\n\n2019-01-05\n7.0\n11.4\n\n\n...\n...\n...\n\n\n2019-12-27\n4.4\n7.4\n\n\n2019-12-28\n6.6\n10.3\n\n\n2019-12-29\n8.1\n12.5\n\n\n2019-12-30\n6.9\n13.0\n\n\n2019-12-31\n5.2\n13.3\n\n\n\n\n365 rows × 2 columns\n\n\n\nThe next step is to calculate the average minimum/maximum for each month. This is similar to what we did above.\n\ndf_month['min temp'] = df_day['min temp'].resample('MS',loffset=pd.Timedelta(14, 'd')).mean()\ndf_month['max temp'] = df_day['max temp'].resample('MS',loffset=pd.Timedelta(14, 'd')).mean()\ndf_month\n\n\n\n\n\n\n\n\nmean temp\nmin temp\nmax temp\n\n\ndate\n\n\n\n\n\n\n\n2019-01-15\n9.119937\n5.922581\n12.470968\n\n\n2019-02-15\n9.629812\n6.825000\n13.089286\n\n\n2019-03-15\n10.731571\n7.532258\n14.661290\n\n\n2019-04-15\n14.514329\n10.866667\n19.113333\n\n\n2019-05-15\n22.916894\n17.296774\n29.038710\n\n\n2019-06-15\n23.587361\n19.163333\n28.860000\n\n\n2019-07-15\n24.019403\n19.367742\n29.564516\n\n\n2019-08-15\n24.050822\n19.903226\n29.767742\n\n\n2019-09-15\n22.313287\n18.430000\n28.456667\n\n\n2019-10-15\n20.641868\n16.945161\n26.190323\n\n\n2019-11-15\n17.257153\n14.066667\n21.436667\n\n\n2019-12-15\n11.224131\n8.806452\n14.448387\n\n\n\n\n\n\n\nLet’s plot…\n\nfig, ax = plt.subplots()\nax.plot(df_month['max temp'], color='tab:red', label='max')\nax.plot(df_month['mean temp'], color='black', label='mean')\nax.plot(df_month['min temp'], color='tab:blue', label='min')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(10,35,5),\n       title=\"Jerusalem, 2019\")\nax.xaxis.set_major_locator(mdates.MonthLocator(range(1, 13, 2), bymonthday=15))\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nax.legend(fontsize=12, frameon=False);\n\n\n\n\nVoilà! You made a beautiful graph!"
  },
  {
    "objectID": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "href": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "title": "7  upsampling, interpolation",
    "section": "7.1 Potential Evapotranspiration using Penman’s equation",
    "text": "7.1 Potential Evapotranspiration using Penman’s equation\nWe want to calculate the daily potential evapotranspiration using Penman’s equation. Part of the calculation involves characterizing the energy budget on soil surface. When direct solar radiation measurements are not available, we can estimate the energy balance by knowing the “cloudless skies mean solar radiation”, \\(R_{so}\\). This is the amount of energy (MJ/m\\(^2\\)/d) that hits the surface, assuming no clouds. This radiation depends on the season and on the latitude you are. For Israel, located at latitude 32° N, we can use the following data for 30°:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\ndates = pd.date_range(start='2021-01-01', periods=13, freq='MS')\nvalues = [17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04, 17.46]\ndf = pd.DataFrame({'date': dates, 'radiation': values})\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nradiation\n\n\ndate\n\n\n\n\n\n2021-01-01\n17.46\n\n\n2021-02-01\n21.65\n\n\n2021-03-01\n25.96\n\n\n2021-04-01\n29.85\n\n\n2021-05-01\n32.11\n\n\n2021-06-01\n33.20\n\n\n2021-07-01\n32.66\n\n\n2021-08-01\n30.44\n\n\n2021-09-01\n26.67\n\n\n2021-10-01\n22.48\n\n\n2021-11-01\n18.30\n\n\n2021-12-01\n16.04\n\n\n2022-01-01\n17.46\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None')\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nWe only have 12 values for the whole year, and we can’t use this dataframe to compute daily ET. We need to upsample!\nIn the example below, we resample the monthly data into daily data, and do nothing else. Pandas doesn’t know what to do with the new points, so it fills them with NaN.\n\ndf_nan = df['radiation'].resample('D').asfreq().to_frame()\ndf_nan.head(33)\n\n\n\n\n\n\n\n\nradiation\n\n\ndate\n\n\n\n\n\n2021-01-01\n17.46\n\n\n2021-01-02\nNaN\n\n\n2021-01-03\nNaN\n\n\n2021-01-04\nNaN\n\n\n2021-01-05\nNaN\n\n\n2021-01-06\nNaN\n\n\n2021-01-07\nNaN\n\n\n2021-01-08\nNaN\n\n\n2021-01-09\nNaN\n\n\n2021-01-10\nNaN\n\n\n2021-01-11\nNaN\n\n\n2021-01-12\nNaN\n\n\n2021-01-13\nNaN\n\n\n2021-01-14\nNaN\n\n\n2021-01-15\nNaN\n\n\n2021-01-16\nNaN\n\n\n2021-01-17\nNaN\n\n\n2021-01-18\nNaN\n\n\n2021-01-19\nNaN\n\n\n2021-01-20\nNaN\n\n\n2021-01-21\nNaN\n\n\n2021-01-22\nNaN\n\n\n2021-01-23\nNaN\n\n\n2021-01-24\nNaN\n\n\n2021-01-25\nNaN\n\n\n2021-01-26\nNaN\n\n\n2021-01-27\nNaN\n\n\n2021-01-28\nNaN\n\n\n2021-01-29\nNaN\n\n\n2021-01-30\nNaN\n\n\n2021-01-31\nNaN\n\n\n2021-02-01\n21.65\n\n\n2021-02-02\nNaN"
  },
  {
    "objectID": "resampling/upsampling.html#forwardbackward-fill",
    "href": "resampling/upsampling.html#forwardbackward-fill",
    "title": "7  upsampling, interpolation",
    "section": "7.2 Forward/Backward fill",
    "text": "7.2 Forward/Backward fill\nWe can forward/backward fill these NaNs:\n\ndf_forw = df['radiation'].resample('D').ffill().to_frame()\ndf_back = df['radiation'].resample('D').bfill().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_forw['radiation'], color='tab:blue', label=\"forward fill\")\nax.plot(df_back['radiation'], color='tab:orange', label=\"backward fill\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThis does the job, but I want something better, not step functions. The radiation should vary smoothly from day to day. Let’s use interpolation."
  },
  {
    "objectID": "resampling/upsampling.html#interpolation",
    "href": "resampling/upsampling.html#interpolation",
    "title": "7  upsampling, interpolation",
    "section": "7.3 Interpolation",
    "text": "7.3 Interpolation\n\ndf_linear = df['radiation'].resample('D').interpolate(method='time').to_frame()\ndf_cubic = df['radiation'].resample('D').interpolate(method='cubic').to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_linear['radiation'], color='tab:blue', label=\"linear interpolation\")\nax.plot(df_cubic['radiation'], color='tab:orange', label=\"cubic interpolation\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThere are many ways to fill NaNs and to interpolate. A nice detailed guide can be found here."
  },
  {
    "objectID": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "href": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "title": "8  FAQ",
    "section": "8.1 How to resample by year, but have it end in September?",
    "text": "8.1 How to resample by year, but have it end in September?\nThis is called anchored offset. One possible use to it is to calculate statistics according to the hydrological year that, for example, ends in September.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\nfilename = \"../archive/data/Kinneret_Kvuza_daily_rainfall.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Date': 'date',\n                   'Daily Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace\ndf\n\n\n\n\n\n\n\n\nStation\nrain\n\n\ndate\n\n\n\n\n\n\n1980-01-02\nKinneret Kvuza 09/1977-08/2023\n0.0\n\n\n1980-01-03\n0\n0.0\n\n\n1980-01-04\n0\n0.0\n\n\n1980-01-05\nKinneret Kvuza 09/1977-08/2023\n35.5\n\n\n1980-01-06\nKinneret Kvuza 09/1977-08/2023\n2.2\n\n\n...\n...\n...\n\n\n2019-12-26\nKinneret Kvuza 09/1977-08/2023\n39.4\n\n\n2019-12-27\nKinneret Kvuza 09/1977-08/2023\n5.2\n\n\n2019-12-28\nKinneret Kvuza 09/1977-08/2023\n1.6\n\n\n2019-12-29\n0\n0.0\n\n\n2019-12-30\nKinneret Kvuza 09/1977-08/2023\n0.1\n\n\n\n\n14608 rows × 2 columns\n\n\n\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(df['rain'], color='black')\nax[1].plot(df.loc['1998':'2000', 'rain'], color='black')\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\nfig.text(0.02, 0.5, 'daily precipitation (mm)', va='center', rotation='vertical')\nax[0].set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')\n\n\n\n\n\nWe see a marked dry season during the summer, so let’s assume the Hydrological Year ends in September.\n\ndf_year = df.resample('A-SEP').sum()\ndf_year = df_year.loc['1980':'2003']\ndf_year\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_94063/2047090134.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_year = df.resample('A-SEP').sum()\n\n\n\n\n\n\n\n\n\nrain\n\n\ndate\n\n\n\n\n\n1980-09-30\n355.5\n\n\n1981-09-30\n463.1\n\n\n1982-09-30\n221.7\n\n\n1983-09-30\n557.1\n\n\n1984-09-30\n335.3\n\n\n1985-09-30\n379.8\n\n\n1986-09-30\n300.7\n\n\n1987-09-30\n424.7\n\n\n1988-09-30\n421.6\n\n\n1989-09-30\n251.6\n\n\n1990-09-30\n432.5\n\n\n1991-09-30\n328.3\n\n\n1992-09-30\n738.4\n\n\n1993-09-30\n434.9\n\n\n1994-09-30\n255.4\n\n\n1995-09-30\n408.6\n\n\n1996-09-30\n373.0\n\n\n1997-09-30\n416.2\n\n\n1998-09-30\n451.9\n\n\n1999-09-30\n227.8\n\n\n2000-09-30\n378.9\n\n\n2001-09-30\n273.9\n\n\n2002-09-30\n445.2\n\n\n2003-09-30\n602.4\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(df_year.index, df_year['rain'], color='black',\n       width=365)\nax.set_ylabel(\"yearly precipitation (mm)\")\nax.set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')"
  },
  {
    "objectID": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "href": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "title": "8  FAQ",
    "section": "8.2 When upsampling, how to fill missing values with zero?",
    "text": "8.2 When upsampling, how to fill missing values with zero?\nWe did that in the example above, like this:\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace"
  },
  {
    "objectID": "outliers/motivation.html",
    "href": "outliers/motivation.html",
    "title": "9  motivation",
    "section": "",
    "text": "Outliers are observations significantly different from all other observations. Consider, for example, this temperature graph:\n\nWhile most measured points are between 20 and 30 °C, there is obviously something very wrong with the one data point above 80 °C.\nHow could such a thing come about? This could be the result of non-natural causes, such as measurement errors, wrong data collection, or wrong data entry. On the other hand, this point could have natural sources, such as a very hot spark flying next to the temperature sensor.\nIdentifying outliers is important, because they might greatly impact measures like mean and standard deviation. When left untouched, outliers might make us reach wrong conclusions about our data. See what happens to the slope of this linear regression with and without the outliers.\n\n\n\nSource: Zhang (2020)\n\n\n\n\n\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.” ouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "outliers/zscore.html",
    "href": "outliers/zscore.html",
    "title": "10  Z-score",
    "section": "",
    "text": "\\[\nz  = \\frac{x-\\mu}{\\sigma},\n\\]\n\n\nwhere\n\n\\(x=\\) data point,\n\n\\(\\mu=\\) time series mean\n\n\\(\\sigma=\\) time series standard deviation.\n\nLet’s write a function that identifies outliers according to the Z-score.\ndef zscore(df, degree=3):\n    data = df.copy()\n    data['zscore'] = (data - data.mean())/data.std()\n    outliers = data[(data['zscore'] &lt;= -degree) | (data['zscore'] &gt;= degree)]\n    return outliers['value'], data\nNow we can simply use this function:\nthreshold = 2.5\noutliers, transformed = zscore(tx, threshold)\nSource: Atwan (2022)\n\n\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook: Practical Recipes for Exploratory Data Analysis, Data Preparation, Forecasting, and Model Evaluation. Packt."
  },
  {
    "objectID": "outliers/IQR.html",
    "href": "outliers/IQR.html",
    "title": "11  IQR (Inter Quartile Range)",
    "section": "",
    "text": "The IQR (Inter Quartile Range) is the distance between the 25th percentile (Q1) and the 75th percentile (Q3). In a box plot, the whiskers usually extend \\(1.5\\times\\)IQR beyond Q1 and Q3, see below.\n\n\n\nSource: McDonald (2022)\n\n\nA common outlier detection method is to consider whatever points outside the whisker range as outliers.\n\n\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python Library.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57."
  },
  {
    "objectID": "smoothing/convolution.html#convolution",
    "href": "smoothing/convolution.html#convolution",
    "title": "15  convolution",
    "section": "15.1 convolution",
    "text": "15.1 convolution\nConvolution is a fancy word for averaging a time series using a running window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/convolution.html#kernels",
    "href": "smoothing/convolution.html#kernels",
    "title": "15  convolution",
    "section": "15.2 kernels",
    "text": "15.2 kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation (the window in the animation is 4 standard deviations wide)."
  },
  {
    "objectID": "smoothing/convolution.html#math",
    "href": "smoothing/convolution.html#math",
    "title": "15  convolution",
    "section": "15.3 math",
    "text": "15.3 math\nThe definition of a convolution between signal \\(f(t)\\) and kernel \\(k(t)\\) is\n\\[\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\\]\nThe expression \\(f*k\\) denotes the convolution of these two functions. The argument of \\(k\\) is \\(t-\\tau\\), meaning that the kernel runs from left to right (as \\(t\\) does), and at every point the two signals (\\(f\\) and \\(k\\)) are multiplied together. It is the product of the signal with the weight function \\(k\\) that gives us an average. Because of \\(-\\tau\\), the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\\[\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\\]\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/convolution.html#numerics",
    "href": "smoothing/convolution.html#numerics",
    "title": "15  convolution",
    "section": "15.4 numerics",
    "text": "15.4 numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes\ndf['temperature'].rolling(window='20', center=True).mean()\n\nwindow=20 means that the width of the window is 20 points. Pandas lets us define a window width in time units, for example, window='120min'.\ncenter=True is needed in order to assign the result of averaging to the center of the window. Make it False and see what happens.\nmean() is the actual calculation, the average of temperature over the window. The rolling part does not compute anything, it just creates a moving window, and we are free to calculate whatever we want. Try to calculate the standard deviation or the maximum, for example.\n\nIt is implicit in the command above a “rectangular” kernel. What if we want other shapes?\n\n15.4.1 gaussian\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere\n\nwindow_width is an integer, number of points in your window\nstd_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\n\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package. The graph above was achieved by running:\ng = scipy.signal.gaussian(window_width, std)\nplt.plot(g)\n\n\n15.4.2 triangular\nSame idea as gaussian, but simpler, because we don’t need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "title": "15  convolution",
    "section": "15.5 which window shape and width to choose?",
    "text": "15.5 which window shape and width to choose?\n🤷‍♂️\nSorry, there is not definite answer here… It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above.\n\nOne important question you need to ask is: what are the time scales associated with the processes I’m interested in? For example, if I’m interested in the daily temperature pattern, getting rid of 1-minute-long fluctuations would probably be ok. On the other hand, if we were to smooth the signal so much that all that can be seen are the temperature changes between summer and winter, then my smoothing got out of hand, and I threw away the very process I wanted to study.\nAll this is to say that you need to know in advance a few things about the system you are studying, otherwise you can’t know what is “noise” that can be smoothed away."
  },
  {
    "objectID": "smoothing/perfect-smoother.html",
    "href": "smoothing/perfect-smoother.html",
    "title": "17  a perfect smoother",
    "section": "",
    "text": "Source: Eilers (2003)\nGitHub repository\nNoisy series \\(y\\) of length \\(m\\).\nThe smoothed series is called \\(z\\).\nWe have conflicting interests:\n\nwe want a \\(z\\) series “as smooth as possible”.\nhowever, the smoother \\(z\\) is, the farthest from \\(y\\) it will be (low fidelity).\n\nRoughness:\n\\[\nR = \\displaystyle\\sum_i (z_i - z_{i-1})^2\n\\]\nFit to data:\n\\[\nS = \\displaystyle\\sum_i (y_i - z_i)^2\n\\]\nCost functional to be minimized:\n\\[\nQ = S + \\lambda R\n\\]\n\n\n\n\nEilers, Paul HC. 2003. “A Perfect Smoother.” Analytical Chemistry 75 (14): 3631–36. https://doi.org/10.1021/ac034173t."
  },
  {
    "objectID": "stationarity/autocorrelation.html#question",
    "href": "stationarity/autocorrelation.html#question",
    "title": "20  autocorrelation",
    "section": "20.1 question",
    "text": "20.1 question\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nTo answer this, we need to talk about autocorrelation. Let’s start by introducing the necessary concepts."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "20  autocorrelation",
    "section": "20.2 mean and standard deviation",
    "text": "20.2 mean and standard deviation\nLet’s call our time series from above \\(X\\), and its length \\(N\\). Then:\n\\[\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\\]\nThe mean and standard deviation can be visualized thus:\n\nOne last basic concept we need is the expected value: \\[\nE[X] = \\sum_{i=1}^N X_i p_i\n\\]\nFor our time series, the probability \\(p_i\\) that a given point \\(X_i\\) is in the dataset is simply \\(1/N\\), therefore the expectation becomes\n\\[\nE[X] = \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N}\n\\]"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "20  autocorrelation",
    "section": "20.3 autocorrelation",
    "text": "20.3 autocorrelation\nThe autocorrelation of a time series \\(X\\) is the answer to the following question:\n\nif we shift \\(X\\) by \\(\\tau\\) units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are \\(X(t)\\) and \\(X(t+\\tau)\\)?\n\nUsing the Pearson correlation coefficient\n\n\nPearson correlation coefficient between \\(X\\) and \\(Y\\): \\[\n\\rho_{X,Y} = \\frac{E\\left[ (X - \\mu_X)(X_Y - \\mu_Y) \\right]}{\\sigma_X\\sigma_Y}\n\\]\nwe get\n\\[\n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\\]\nA video is worth a billion words, so let’s see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\(\\tau=0\\) (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "22  cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "24  LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "frequency/fourier.html#basic-wave-concepts",
    "href": "frequency/fourier.html#basic-wave-concepts",
    "title": "26  Fourier transform",
    "section": "26.1 basic wave concepts",
    "text": "26.1 basic wave concepts\nThe function\n\\[\nf(t) = B\\sin(2\\pi f t)\n\\tag{26.1}\\]\nhas two basic characteristics, its amplitude \\(B\\) and frequency \\(f\\).\n\nIn the figure above, the amplitude \\(B=0.6\\) and we see that the distance between two peaks is called period, \\(T=2\\) s. The frequency is defined as the inverse of the period:\n\\[\nf = \\frac{1}{T}.\n\\tag{26.2}\\]\nWhen time is in seconds, then the frequency is measured in Hertz (Hz). For the graph above, therefore, we see a wave whose frequency is \\(f = 1/(2 \\text{ s}) = 0.5\\) Hz.\nIn the figure below, we see what happens when we vary the values of the frequency and amplitude.\n\nThe graph above introduces two new characteristics of a wave, its phase \\(\\phi\\), and its offset \\(B\\). A more general description of a sine wave is\n\\[\nf(t) = B\\sin(2\\pi f t + \\phi) + B_0.\n\\tag{26.3}\\]\nThe offset \\(B_0\\) moves the wave up and down, while changing the value of \\(\\phi\\) makes the sine wave move left and right. When the phase \\(\\phi=2\\pi\\), the sine wave will have shifted a full period, and the resulting wave is identical to the original:\n\\[\nB\\sin(2\\pi f t) = B\\sin(2\\pi f t + 2\\pi).\n\\tag{26.4}\\]\nAll the above can also be said about a cosine, whose general for can be given as\n\\[\nA\\cos(2\\pi f t + \\phi) + A_0\n\\tag{26.5}\\]\nOne final point before we jump into the deep waters is that the sine and cosine functions are related through a simple phase shift:\n\\[\n\\cos\\left(2\\pi f t + \\frac{\\pi}{2}\\right) = \\sin\\left(2\\pi f t\\right)\n\\]"
  },
  {
    "objectID": "frequency/fourier.html#fouriers-theorem",
    "href": "frequency/fourier.html#fouriers-theorem",
    "title": "26  Fourier transform",
    "section": "26.2 Fourier’s theorem",
    "text": "26.2 Fourier’s theorem\nFourier’s theorem states that\n\nAny periodic signal is composed of a superposition of pure sine waves, with suitably chosen amplitudes and phases, whose frequencies are harmonics of the fundamental frequency of the signal.\n\nSee the following animations to visualize the theorem in action.\n\nSource: https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Fourier_synthesis_square_wave_animated.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Sawtooth_Fourier_Animation.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Continuous_Fourier_transform_of_rect_and_sinc_functions.gif"
  },
  {
    "objectID": "frequency/fourier.html#fourier-series",
    "href": "frequency/fourier.html#fourier-series",
    "title": "26  Fourier transform",
    "section": "26.3 Fourier series",
    "text": "26.3 Fourier series\n\na periodic function can be described as a sum of sines and cosines.\n\n\n\nNot any function, but certainly most functions we will deal with in this course. The function has to fullful the Dirichlet conditions\nThe classic examples are usually the square function and the sawtooth function:\n\n\n[Source: https://www.geogebra.org/m/tkajbzmg]\nhttps://www.geogebra.org/m/k4eq4fkr\n\n\n\\[\nF[x(t)] = F(f) = \\int_{-\\infty}^{\\infty}x(t)e^{-2\\pi i f t}dt\n\\]\n\\[\nf(t) = \\int_{-\\infty}^{\\infty}F(f)e^{2\\pi i f t}df\n\\]\nhttps://dibsmethodsmeetings.github.io/fourier-transforms/\nhttps://www.jezzamon.com/fourier/index.html"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "href": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "title": "30  seasonal decomposition",
    "section": "30.1 trends in atmospheric carbon dioxide",
    "text": "30.1 trends in atmospheric carbon dioxide\nMauna Loa CO2 concentration.\ndata from NOAA\n\nurl = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url, header=47, na_values=[-999.99])\n\n# you can first download, and then read the csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename, header=35, na_values=[-999.99])\n\ndf\n\n\n\n\n\n\n\n\n1974\n5\n19\n1974.3795\n333.37\n5.1\n-999.99\n-999.99.1\n50.39\n\n\n\n\n0\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n1\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n2\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n3\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n4\n1974\n6\n23\n1974.4753\n331.73\n5\nNaN\nNaN\n49.72\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2565\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2566\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2567\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2568\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2569\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2570 rows × 9 columns\n\n\n\n\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.40\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.06\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.60\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.65\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-26\n2022\n6\n26\n2022.4836\n420.31\n7\n418.14\n395.36\n138.71\n\n\n2022-07-03\n2022\n7\n3\n2022.5027\n419.73\n6\n417.49\n395.15\n138.64\n\n\n2022-07-10\n2022\n7\n10\n2022.5219\n419.08\n6\n417.25\n394.59\n138.52\n\n\n2022-07-17\n2022\n7\n17\n2022.5411\n418.43\n6\n417.14\n394.64\n138.41\n\n\n2022-07-24\n2022\n7\n24\n2022.5603\n417.84\n6\n415.68\n394.11\n138.36\n\n\n\n\n2515 rows × 9 columns\n\n\n\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,6))\nax.plot(df['average'])\nax.set(xlabel=\"date\",\n       ylabel=\"CO2 concentration (ppm)\",\n       # ylim=[0, 430],\n       title=\"Mauna Loa CO2 concentration\");\n\nKeyError: 'average'\n\n\n\n\n\nfill missing data. interpolate method: ‘time’\ninterpolation methods visualized\n\ndf['co2'] = (df['average'].resample(\"D\") #resample daily\n                          .interpolate(method='time') #interpolate by time\n            )\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\nco2\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.40\n333.37\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.06\n332.95\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.60\n332.35\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.65\n332.20\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n332.37\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-26\n2022\n6\n26\n2022.4836\n420.31\n7\n418.14\n395.36\n138.71\n420.31\n\n\n2022-07-03\n2022\n7\n3\n2022.5027\n419.73\n6\n417.49\n395.15\n138.64\n419.73\n\n\n2022-07-10\n2022\n7\n10\n2022.5219\n419.08\n6\n417.25\n394.59\n138.52\n419.08\n\n\n2022-07-17\n2022\n7\n17\n2022.5411\n418.43\n6\n417.14\n394.64\n138.41\n418.43\n\n\n2022-07-24\n2022\n7\n24\n2022.5603\n417.84\n6\n415.68\n394.11\n138.36\n417.84\n\n\n\n\n2515 rows × 10 columns"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#decompose-data",
    "href": "seasonality/seasonal-decomposition.html#decompose-data",
    "title": "30  seasonal decomposition",
    "section": "30.2 decompose data",
    "text": "30.2 decompose data\nseasonal_decompose returns an object with four components:\n\nobserved: \\(Y(t)\\)\ntrend: \\(T(t)\\)\nseasonal: \\(S(t)\\)\nresid: \\(e(t)\\)\n\nAdditive model: \\[\nY(t) = T(t) + S(t) + e(t)\n\\]\nMultiplicative model: \\[\nY(t) = T(t) \\times S(t) \\times e(t)\n\\]\n\n30.2.0.1 Interlude\nlearn how to use zip in a loop\n\nletters = ['a', 'b', 'c', 'd', 'e']\nnumbers = [1, 2, 3, 4, 5]\n# zip let's us iterate over to lists at the same time\nfor l, n in zip(letters, numbers):\n    print(f\"{l} = {n}\")\n\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n\nPlot each component separately.\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\ndecomposed_m = seasonal_decompose(df['co2'], model='multiplicative')\ndecomposed_a = seasonal_decompose(df['co2'], model='additive')\ndecomposed = decomposed_m\npos = (0.5, 0.9)\ncomponents =[\"observed\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(decomposed, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\n\n\n\n\n\n# %matplotlib widget\n\ndecomposed = decomposed_m\n\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df['co2'], color=\"tab:blue\", label=\"observed\")\nax[0].plot(decomposed.trend * decomposed.resid, color=\"tab:orange\", label=\"trend*resid\")\nax[0].plot(decomposed.trend * decomposed.seasonal, color=\"tab:red\", label=\"trend*seasonal\")\nax[0].plot(decomposed.trend, color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"CO$_2$ concentration (ppm)\",\n          title=\"Mauna Loa CO$_2$ concentration\")\nax[0].legend(frameon=False)\n\nstart = \"2000-01-01\"\nend = \"2003-01-01\"\nzoom = slice(start, end)\nax[1].plot(df.loc[zoom, 'co2'], color=\"tab:blue\", label=\"observed\")\nax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=\"tab:orange\", label=\"trend*resid\")\nax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=\"tab:red\", label=\"trend*seasonal\")\nax[1].plot(decomposed.trend[zoom], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"Components, 2000--2003\");"
  },
  {
    "objectID": "rates-of-change/finite-differences.html",
    "href": "rates-of-change/finite-differences.html",
    "title": "34  finite differences",
    "section": "",
    "text": "Definition of a derivative:\n\\[\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n\\]\nNumerically, we can approximate the derivative \\(f'(t)\\) of a time series \\(f(t)\\) as\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{34.1}\\]\n\n\nThe expression \\(\\mathcal{O}(\\Delta t)\\) means that the error associated with the approximation is proportional to \\(\\Delta t\\). This is called “Big O notation”.\nThe expression above is called the two-point forward difference formula. Likewise, we can define the two-point backward difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{34.2}\\]\nIf we sum together Equation 34.1 and Equation 34.2 we get:\n\n\\[\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n&= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n\\tag{34.3}\\]\nDividing both sides by 2 gives the two-point central difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n\\tag{34.4}\\]\nTwo things are worth mentioning about the approximation above:\n\nit is balanced, that is, there is no preference of the future over the past.\nits error is proportional to \\(\\Delta t^2\\), it is a lot more precise than the unbalanced approximations :)\n\n\n\nTo understand why the error is proportional to \\(\\Delta t^2\\), one can subtract the Taylor expansion of \\(f(t-\\Delta t)\\) from the Taylor expansion of \\(f(t+\\Delta t)\\). See this, pages 3 and 4.\n\nThe function np.gradient calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n\nThe “gradient” usually refers to a first derivative with respect to space, and it is denoted as \\(\\nabla f(x)=\\frac{df(x)}{dx}\\). However, it doesn’t really matter if we call the independent variable \\(x\\) or \\(t\\), the derivative operator is exactly the same.\nCheck out this nice example."
  },
  {
    "objectID": "rates-of-change/fourier-based-derivatives.html",
    "href": "rates-of-change/fourier-based-derivatives.html",
    "title": "35  Fourier-based derivatives",
    "section": "",
    "text": "This tutorial is based on Pelliccia (2019).\nnice trick: https://math.stackexchange.com/questions/430858/fourier-transform-of-derivative\n\n\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing Method.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "technical stuff",
    "section": "operating systems",
    "text": "operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "technical stuff",
    "section": "software",
    "text": "software\nAnaconda’s Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "technical stuff",
    "section": "python packages",
    "text": "python packages\nKats — a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "references.html#books",
    "href": "references.html#books",
    "title": "sources",
    "section": "books",
    "text": "books\nfrom Data to Viz\nFundamentals of Data Visualization, by Claus O. Wilke\nPyNotes in Agriscience\nForecasting: Principles and Practice (3rd ed), by Rob J Hyndman and George Athanasopoulos\nPython for Finance Cookbook 2nd Edition - Code Repository\nPractical time series analysis,: prediction with statistics and machine learning, by Aileen Nielsen\nThe online edition of this book is available for Hebrew University staff and students.\nTime series analysis with Python cookbook : practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation, by Tarek A. Atwan\nThe online edition of this book is available for Hebrew University staff and students.\nHands-on Time Series Analysis with Python: From Basics to Bleeding Edge Techniques, by B V Vishwas, Ashish Patel\nThe online edition of this book is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#videos",
    "href": "references.html#videos",
    "title": "sources",
    "section": "videos",
    "text": "videos\nTimes Series Analysis for Everyone, by Bruno Goncalves\nThis series is available for Hebrew University staff and students.\nTime Series Analysis with Pandas, by Joshua Malina This video is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "sources",
    "section": "references",
    "text": "references\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook:\nPractical Recipes for Exploratory Data Analysis, Data Preparation,\nForecasting, and Model Evaluation. Packt.\n\n\nEilers, Paul HC. 2003. “A Perfect Smoother.” Analytical\nChemistry 75 (14): 3631–36. https://doi.org/10.1021/ac034173t.\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python\nLibrary.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57.\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing\nMethod.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/.\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.”\nouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  }
]