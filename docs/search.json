[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "About"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "Disclaimer",
    "text": "Disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#when-and-where",
    "href": "index.html#when-and-where",
    "title": "Time Series Analysis",
    "section": "When and Where?",
    "text": "When and Where?\n Day of the week, from 00:00 to 24:00\n Computer classroom #16"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "Syllabus",
    "text": "Syllabus\n\nCourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\nCourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nLearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\nCourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nBooks and other sources\n\n\nCourse evaluation\nThere will be 2 projects during the semester (each worth 25% of the final grade), and one final project (50%)."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "Weekly program",
    "text": "Weekly program\n\nWeek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nWeek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nWeek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nWeek 4\n\nLecture: Time series plotting: best practices. Dos and don‚Äôts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nWeek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nWeek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nWeek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nWeek 8\nSame as lecture 7 above\n\n\nWeek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nWeek 10\n\nLecture: Fourier decomposition, filtering, Nyquist‚ÄìShannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nWeek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nWeek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nWeek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "outliers/outliers.html",
    "href": "outliers/outliers.html",
    "title": "Outliers and Gaps",
    "section": "",
    "text": "Z-score\n\\[\nz  = \\frac{x-\\mu}{\\sigma},\n\\]\nwhere \\(x\\) is a data point, \\(\\mu\\) is the time series mean, and \\(\\sigma\\) is the time series standard deviation.\ndef zscore(df, degree=3):\n    data = df.copy()\n    data['zscore'] = (data - data.mean())/data.std()\n    outliers = data[(data['zscore'] <= -degree) | (data['zscore'] >= degree)]\n    return outliers['value'], data\nNow we can simply use this function:\nthreshold = 2.5\noutliers, transformed = zscore(tx, threshold)\nSource: Atwan (2022)\n\n\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook: Practical Recipes for Exploratory Data Analysis, Data Preparation, Forecasting, and Model Evaluation. Packt."
  },
  {
    "objectID": "outliers/zscore.html",
    "href": "outliers/zscore.html",
    "title": "1¬† Z-score",
    "section": "",
    "text": "\\[\nz  = \\frac{x-\\mu}{\\sigma},\n\\]\n\n\nwhere\n\n\\(x=\\) data point,\n\n\\(\\mu=\\) time series mean\n\n\\(\\sigma=\\) time series standard deviation.\n\nLet‚Äôs write a function that identifies outliers according to the Z-score.\ndef zscore(df, degree=3):\n    data = df.copy()\n    data['zscore'] = (data - data.mean())/data.std()\n    outliers = data[(data['zscore'] <= -degree) | (data['zscore'] >= degree)]\n    return outliers['value'], data\nNow we can simply use this function:\nthreshold = 2.5\noutliers, transformed = zscore(tx, threshold)\nSource: Atwan (2022)\n\n\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook: Practical Recipes for Exploratory Data Analysis, Data Preparation, Forecasting, and Model Evaluation. Packt."
  },
  {
    "objectID": "introduction/first-steps.html#nan-missing-data-outliers",
    "href": "introduction/first-steps.html#nan-missing-data-outliers",
    "title": "2¬† First Steps ‚Äî basic time series analysis",
    "section": "2.1 NaN, Missing data, Outliers",
    "text": "2.1 NaN, Missing data, Outliers\n\n# %matplotlib widget\n\nstart = \"2022-05-03 12:00:00\"\nend = \"2022-05-06 00:00:00\"\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# plot using pandas' plot method\ndf.loc[start:end, 'T2'].plot(ax=ax,\n                             linestyle='-',\n                             marker='o',\n                             color=\"tab:blue\",\n                             label=\"data\")\n\n# annotate examples here:\n# https://jakevdp.github.io/PythonDataScienceHandbook/04.09-text-and-annotation.html\nax.annotate(\"NaN\",                             # text to write, if nothing, then \"\"\n            xy=('2022-05-03 20:30:00', 25),    # (x,y coordinates for the tip of the arrow)\n            xycoords='data',                   # xy as 'data' coordinates\n            xytext=(-20, 60),                  # xy coordinates for the text\n            textcoords='offset points',        # xytext relative to xy\n            arrowprops=dict(arrowstyle=\"->\")   # pretty arrow\n           )\nax.annotate(\"outlier\",\n            xy=('2022-05-03 22:30:00', 85),\n            xycoords='data',\n            xytext=(40, -20),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\nax.annotate(\"missing rows\",\n            xy=('2022-05-05 00:00:00', 25),\n            xycoords='data',\n            xytext=(0, 40),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\n\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d %b, %H:00'))\nplt.gcf().autofmt_xdate()\nax.set(xlabel=\"\",\n       ylabel=\"Temperature (deg C)\")\n\n[Text(0.5, 0, ''), Text(0, 0.5, 'Temperature (deg C)')]\n\n\n\n\n\nThe arrows (annotate) work because the plot was\ndf['column'].plot()\nIf you use the usual\nax.plot(df['column'])\nthen you matplotlib will not understand timestamps as x-positions. In this case follow the instructions below.\n\n# %matplotlib widget\n\nstart = \"2022-05-03 12:00:00\"\nend = \"2022-05-06 00:00:00\"\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\nax.plot(df.loc[start:end, 'T2'], linestyle='-', marker='o', color=\"tab:blue\", label=\"data\")\n\nt_nan = '2022-05-03 20:30:00'\nx_nan = mdates.date2num(dt.datetime.strptime(t_nan, \"%Y-%m-%d %H:%M:%S\"))\nax.annotate(\"NaN\",\n            xy=(x_nan, 25),\n            xycoords='data',\n            xytext=(-20, 60),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\nt_outlier = '2022-05-03 22:30:00'\nx_outlier = mdates.date2num(dt.datetime.strptime(t_outlier, \"%Y-%m-%d %H:%M:%S\"))\nax.annotate(\"outlier\",\n            xy=(x_outlier, 85),\n            xycoords='data',\n            xytext=(40, -20),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\nt_missing = '2022-05-05 00:00:00'\nx_missing = mdates.date2num(dt.datetime.strptime(t_missing, \"%Y-%m-%d %H:%M:%S\"))\nax.annotate(\"missing rows\",\n            xy=(x_missing, 25),\n            xycoords='data',\n            xytext=(0, 40),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\")\n           )\n# code for hours, days, etc\n# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d %b, %H:00'))\nplt.gcf().autofmt_xdate()\nax.set(xlabel=\"\",\n       ylabel=\"Temperature (deg C)\")\n\n[Text(0.5, 0, ''), Text(0, 0.5, 'Temperature (deg C)')]\n\n\n\n\n\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\ndelta_index = (df.index.to_series().diff() / pd.Timedelta('1 sec') ).values\nax.plot(delta_index)\nax.set(ylim=[0, 100],\n       xlabel=\"running index\",\n       ylabel=r\"$\\Delta t$ (s)\",\n       title=\"Time difference between consecutive rows\")\n\n[(0.0, 100.0),\n Text(0.5, 0, 'running index'),\n Text(0, 0.5, '$\\\\Delta t$ (s)'),\n Text(0.5, 1.0, 'Time difference between consecutive rows')]"
  },
  {
    "objectID": "introduction/first-steps.html#resample",
    "href": "introduction/first-steps.html#resample",
    "title": "2¬† First Steps ‚Äî basic time series analysis",
    "section": "2.2 Resample",
    "text": "2.2 Resample\n\n2.2.1 Downsampling\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# Downsample to spaced out data points. Change the number below, see what happens.\nwindow_size = '15min'\ndf_resampled = (df['T2'].resample(window_size)  # resample doesn't do anything yet, just divides data into buckets\n                        .mean()                 # this is where stuff happens. you can also choose \"sum\", \"max\", etc\n               )\n# optional, add half a window size to timestamp\ndf_resampled.index = df_resampled.index + to_offset(window_size) / 2\n\nax.plot(df['T2'], color=\"tab:blue\", label=\"original data\")\nax.plot(df_resampled, marker='x', color=\"tab:orange\", zorder=-1,\n        label=f\"resampled {window_size} data\")\nax.legend()\n\nax.set(xlabel=\"time\",\n       ylabel=\"temperature (deg C)\")\n\n[Text(0.5, 0, 'time'), Text(0, 0.5, 'temperature (deg C)')]\n\n\n\n\n\n\n\n2.2.2 Filling missing data\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# see options for interpolation methods here:\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\ndf_interpolated1 = df_resampled.interpolate(method='time')\ndf_interpolated2 = df_resampled.interpolate(method='nearest')\n\nax.plot(df_resampled, color=\"tab:orange\", label=\"resampled\")\nax.plot(df_interpolated1, '.', color=\"tab:purple\", zorder=-1,\n        label=f\"time-interpolated\")\nax.plot(df_interpolated2, '.', color=\"tab:cyan\", zorder=-2,\n        label=f\"nearest-interpolated\")\nax.legend()\n\nax.set(xlabel=\"time\",\n       ylabel=\"temperature (deg C)\")\n\n[Text(0.5, 0, 'time'), Text(0, 0.5, 'temperature (deg C)')]"
  },
  {
    "objectID": "introduction/first-steps.html#smoothing-noisy-data",
    "href": "introduction/first-steps.html#smoothing-noisy-data",
    "title": "2¬† First Steps ‚Äî basic time series analysis",
    "section": "2.3 Smoothing noisy data",
    "text": "2.3 Smoothing noisy data\nLet‚Äôs first download data from a different project.\n\nfilename2 = \"test_peleg.csv\"\n# if file is not there, go fetch it from thingspeak\nif not os.path.isfile(filename2):\n    # define what to download\n    channels = \"1708067\"\n    fields = \"1,2,3,4,5\"\n    minutes = \"30\"\n\n    # https://www.mathworks.com/help/thingspeak/readdata.html\n    # format YYYY-MM-DD%20HH:NN:SS\n    start = \"2022-05-15%2000:00:00\"\n    end = \"2022-05-25%2000:00:00\"\n\n    # download using Thingspeak's API\n    # url = f\"https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?minutes={minutes}\"\n    url = f\"https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?start={start}&end={end}\"\n    data = urllib.request.urlopen(url)\n    d = data.read()\n\n    # save data to csv\n    file = open(filename2, \"w\")\n    file.write(d.decode('UTF-8'))\n    file.close()\n\n\n# load data\ndf = pd.read_csv(filename2)\n# rename columns\ndf = df.rename(columns={\"created_at\": \"timestamp\",\n                        \"field1\": \"T\",\n                        \"field2\": \"Tw\",\n                        \"field3\": \"RH\",\n                        \"field4\": \"VPD\",\n                        \"field5\": \"dist\",\n                        })\n# set timestamp as index\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.set_index('timestamp')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      entry_id\n      T\n      Tw\n      RH\n      VPD\n      dist\n    \n    \n      timestamp\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-05-18 20:09:31+00:00\n      24716\n      23.85\n      23.3125\n      65.32\n      1.02532\n      7.208\n    \n    \n      2022-05-18 20:10:32+00:00\n      24717\n      23.88\n      23.2500\n      65.32\n      1.02717\n      7.208\n    \n    \n      2022-05-18 20:11:33+00:00\n      24718\n      23.90\n      23.2500\n      65.23\n      1.03107\n      7.276\n    \n    \n      2022-05-18 20:12:33+00:00\n      24719\n      23.90\n      23.2500\n      65.19\n      1.03226\n      7.208\n    \n    \n      2022-05-18 20:13:34+00:00\n      24720\n      23.89\n      23.2500\n      65.15\n      1.03282\n      7.633\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-05-24 12:18:35+00:00\n      32711\n      27.47\n      26.1250\n      47.49\n      1.92397\n      8.925\n    \n    \n      2022-05-24 12:19:36+00:00\n      32712\n      27.47\n      26.1250\n      47.62\n      1.91921\n      8.925\n    \n    \n      2022-05-24 12:20:39+00:00\n      32713\n      27.47\n      26.1250\n      47.96\n      1.90675\n      8.925\n    \n    \n      2022-05-24 12:21:40+00:00\n      32714\n      27.47\n      26.1875\n      47.75\n      1.91444\n      8.925\n    \n    \n      2022-05-24 12:22:41+00:00\n      32715\n      27.49\n      26.1875\n      47.94\n      1.90971\n      8.925\n    \n  \n\n8000 rows √ó 6 columns"
  },
  {
    "objectID": "introduction/first-steps.html#smoothing-noisy-data-1",
    "href": "introduction/first-steps.html#smoothing-noisy-data-1",
    "title": "2¬† First Steps ‚Äî basic time series analysis",
    "section": "2.4 Smoothing noisy data",
    "text": "2.4 Smoothing noisy data\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\nax.plot(df['RH'], '.')\n# add labels and title\nax.set(xlabel = \"time\",\n       ylabel = \"RH (%)\",\n       title = \"Relative Humidity\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()  \n\n\n\n\n\n2.4.1 Moving average and SavGol\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,4))\n\n# apply a rolling average of size \"window_size\",\n# it can be either by number of points, or by window time\n# window_size = 30  # number of measurements\nwindow_size = '120min'  # minutes\nRH_smooth = df['RH'].rolling(window_size, center=True).mean().to_frame()\nRH_smooth.rename(columns={'RH': 'rolling_avg'}, inplace=True)\n\nRH_smooth['SG'] = savgol_filter(df['RH'], window_length=121, polyorder=2)\n\nax.plot(df['RH'], color=\"tab:blue\", label=\"data\")\nax.plot(RH_smooth['rolling_avg'], color=\"tab:orange\", label=\"moving average\")\nax.plot(RH_smooth['SG'], color=\"tab:red\", label=\"Savitzky-Golay filter\")\n# add labels and title\nax.set(xlabel = \"time\",\n       ylabel = \"RH (%)\",\n       title = \"Relative Humidity\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()\nax.legend()\n\n<matplotlib.legend.Legend at 0x7fe6a0525730>"
  },
  {
    "objectID": "smoothing/convolution.html#convolution",
    "href": "smoothing/convolution.html#convolution",
    "title": "3¬† Convolution",
    "section": "3.1 Convolution",
    "text": "3.1 Convolution\nConvolution is a fancy word for averaging a time series using a running window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/convolution.html#kernels",
    "href": "smoothing/convolution.html#kernels",
    "title": "3¬† Convolution",
    "section": "3.2 Kernels",
    "text": "3.2 Kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation (the window in the animation is 4 standard deviations wide)."
  },
  {
    "objectID": "smoothing/convolution.html#math",
    "href": "smoothing/convolution.html#math",
    "title": "3¬† Convolution",
    "section": "3.3 Math",
    "text": "3.3 Math\nThe definition of a convolution between signal \\(f(t)\\) and kernel \\(k(t)\\) is\n\\[\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\\]\nThe expression \\(f*k\\) denotes the convolution of these two functions. The argument of \\(k\\) is \\(t-\\tau\\), meaning that the kernel runs from left to right (as \\(t\\) does), and at every point the two signals (\\(f\\) and \\(k\\)) are multiplied together. It is the product of the signal with the weight function \\(k\\) that gives us an average. Because of \\(-\\tau\\), the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\\[\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\\]\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/convolution.html#numerics",
    "href": "smoothing/convolution.html#numerics",
    "title": "3¬† Convolution",
    "section": "3.4 Numerics",
    "text": "3.4 Numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes\ndf['temperature'].rolling(window='20', center=True).mean()\n\nwindow=20 means that the width of the window is 20 points. Pandas lets us define a window width in time units, for example, window='120min'.\ncenter=True is needed in order to assign the result of averaging to the center of the window. Make it False and see what happens.\nmean() is the actual calculation, the average of temperature over the window. The rolling part does not compute anything, it just creates a moving window, and we are free to calculate whatever we want. Try to calculate the standard deviation or the maximum, for example.\n\nIt is implicit in the command above a ‚Äúrectangular‚Äù kernel. What if we want other shapes?\n\n3.4.1 Gaussian\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere * window_width is an integer, number of points in your window * std_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package. The graph above was achieved by running:\ng = scipy.signal.gaussian(window_width, std)\nplt.plot(g)\n\n\n3.4.2 Triangular\nSame idea as gaussian, but simpler, because we don‚Äôt need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/convolution.html#which-window-shape-and-width-to-choose",
    "title": "3¬† Convolution",
    "section": "3.5 ü§∑‚Äç‚ôÇÔ∏è Which window shape and width to choose?",
    "text": "3.5 ü§∑‚Äç‚ôÇÔ∏è Which window shape and width to choose?\nSorry, there is not definite answer here‚Ä¶ It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above."
  },
  {
    "objectID": "stationarity/autocorrelation.html#question",
    "href": "stationarity/autocorrelation.html#question",
    "title": "4¬† Autocorrelation",
    "section": "4.1 Question",
    "text": "4.1 Question\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nTo answer this, we need to talk about autocorrelation. Let‚Äôs start by introducing the necessary concepts."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "4¬† Autocorrelation",
    "section": "4.2 Mean and standard deviation",
    "text": "4.2 Mean and standard deviation\nLet‚Äôs call our time series from above \\(X\\), and its length \\(N\\). Then:\n\\[\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\\]\nThe mean and standard deviation can be visualized thus:\n\nOne last basic concept we need is the expected value: \\[\nE[X] = \\sum_{i=1}^N X_i p_i\n\\]\nFor our time series, the probability \\(p_i\\) that a given point \\(X_i\\) is in the dataset is simply \\(1/N\\), therefore the expectation becomes\n\\[\nE[X] = \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N}\n\\]"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "4¬† Autocorrelation",
    "section": "4.3 Autocorrelation",
    "text": "4.3 Autocorrelation\nThe autocorrelation of a time series \\(X\\) is the answer to the following question:\n\nif we shift \\(X\\) by \\(\\tau\\) units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are \\(X(t)\\) and \\(X(t+\\tau)\\)?\n\nUsing the Pearson correlation coefficient\n\n\nPearson correlation coefficient between \\(X\\) and \\(Y\\): \\[\n\\rho_{X,Y} = \\frac{E\\left[ (X - \\mu_X)(X_Y - \\mu_Y) \\right]}{\\sigma_X\\sigma_Y}\n\\]\nwe get\n\\[\n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\\]\nA video is worth a billion words, so let‚Äôs see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\(\\tau=0\\) (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "5¬† Cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "7¬† LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html",
    "href": "seasonality/seasonal-decomposition.html",
    "title": "8¬† Seasonal Decomposition",
    "section": "",
    "text": "9 Trends in Atmospheric Carbon Dioxide\nMauna Loa CO2 concentration.\ndata from NOAA\nfill missing data. interpolate method: ‚Äòtime‚Äô\ninterpolation methods visualized"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#decompose-data",
    "href": "seasonality/seasonal-decomposition.html#decompose-data",
    "title": "8¬† Seasonal Decomposition",
    "section": "9.1 decompose data",
    "text": "9.1 decompose data\nseasonal_decompose returns an object with four components:\n\nobserved: \\(Y(t)\\)\ntrend: \\(T(t)\\)\nseasonal: \\(S(t)\\)\nresid: \\(e(t)\\)\n\nAdditive model: \\[\nY(t) = T(t) + S(t) + e(t)\n\\]\nMultiplicative model: \\[\nY(t) = T(t) \\times S(t) \\times e(t)\n\\]\n\n9.1.0.1 Interlude\nlearn how to use zip in a loop\n\nletters = ['a', 'b', 'c', 'd', 'e']\nnumbers = [1, 2, 3, 4, 5]\n# zip let's us iterate over to lists at the same time\nfor l, n in zip(letters, numbers):\n    print(f\"{l} = {n}\")\n\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n\nPlot each component separately.\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\ndecomposed_m = seasonal_decompose(df['co2'], model='multiplicative')\ndecomposed_a = seasonal_decompose(df['co2'], model='additive')\ndecomposed = decomposed_m\npos = (0.5, 0.9)\ncomponents =[\"observed\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(decomposed, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\n\n\n\n\n\n# %matplotlib widget\n\ndecomposed = decomposed_m\n\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df['co2'], color=\"tab:blue\", label=\"observed\")\nax[0].plot(decomposed.trend * decomposed.resid, color=\"tab:orange\", label=\"trend*resid\")\nax[0].plot(decomposed.trend * decomposed.seasonal, color=\"tab:red\", label=\"trend*seasonal\")\nax[0].plot(decomposed.trend, color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"CO$_2$ concentration (ppm)\",\n          title=\"Mauna Loa CO$_2$ concentration\")\nax[0].legend(frameon=False)\n\nstart = \"2000-01-01\"\nend = \"2003-01-01\"\nzoom = slice(start, end)\nax[1].plot(df.loc[zoom, 'co2'], color=\"tab:blue\", label=\"observed\")\nax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=\"tab:orange\", label=\"trend*resid\")\nax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=\"tab:red\", label=\"trend*seasonal\")\nax[1].plot(decomposed.trend[zoom], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"Components, 2000--2003\");"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "Technical Stuff",
    "section": "Operating systems",
    "text": "Operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "Technical Stuff",
    "section": "Software",
    "text": "Software\nAnaconda‚Äôs Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "Technical Stuff",
    "section": "Python packages",
    "text": "Python packages\nKats ‚Äî a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "references.html#books",
    "href": "references.html#books",
    "title": "Sources",
    "section": "Books",
    "text": "Books\nfrom Data to Viz\nFundamentals of Data Visualization, by Claus O. Wilke\nPyNotes in Agriscience\nForecasting: Principles and Practice (3rd ed), by Rob J Hyndman and George Athanasopoulos\nPython for Finance Cookbook 2nd Edition - Code Repository\nPractical time series analysis,: prediction with statistics and machine learning, by Aileen Nielsen\nThe online edition of this book is available for Hebrew University staff and students.\nTime series analysis with Python cookbook : practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation, by Tarek A. Atwan\nThe online edition of this book is available for Hebrew University staff and students.\nHands-on Time Series Analysis with Python: From Basics to Bleeding Edge Techniques, by B V Vishwas, Ashish Patel\nThe online edition of this book is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#videos",
    "href": "references.html#videos",
    "title": "Sources",
    "section": "Videos",
    "text": "Videos\nTimes Series Analysis for Everyone, by Bruno Goncalves\nThis series is available for Hebrew University staff and students.\nTime Series Analysis with Pandas, by Joshua Malina This video is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "Sources",
    "section": "References",
    "text": "References\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook:\nPractical Recipes for Exploratory Data Analysis, Data Preparation,\nForecasting, and Model Evaluation. Packt."
  }
]