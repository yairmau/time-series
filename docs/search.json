[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "about\nWelcome to Time Series Analysis for Environmental Sciences (71606) at the Hebrew University of Jerusalem. This is Yair Mau, your host for today. I am a senior lecturer at the Institute of Environmental Sciences, at the Faculty of Agriculture, Food and Environment, in Rehovot, Israel.\nThis website contains (almost) all the material you‚Äôll need for the course. If you find any mistakes, or have any comments, please email me."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "disclaimer",
    "text": "disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#what-who-when-and-where",
    "href": "index.html#what-who-when-and-where",
    "title": "Time Series Analysis",
    "section": "what, who, when and where?",
    "text": "what, who, when and where?\n Course number 71606, 3 academic points\n Yair Mau (lecturer), Erez Feuer (TA)\n Tuesdays, from 14:15 to 17:00\n Computer classroom #18\n Office hours: upon request"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "syllabus",
    "text": "syllabus\n\ncourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\ncourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nlearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\ncourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nbooks and other sources\nClick here.\n\n\ncourse evaluation\nThere will be assignments during the semester (totaling 50% of the final grade), and one final project (50%).\n\n\nEvaluation policy\n\nIndividual Work: While we support helping your peers, it‚Äôs important to remember that all assignments must be completed individually. This means that your submissions should be your own unique work and not contain code or text that is identical to someone else‚Äôs.\nZero Plagiarism: Do not copy text verbatim from any source. Always express ideas in your own words.\nOn-Time Submission: Assignments must be turned in by the specified deadline. Late submissions will receive a grade of 0. If you require an extension, requests will only be considered if made at least 24 hours before the due date.\nNon-Compliance Consequence: Assignments that do not adhere to these guidelines will automatically receive a grade of 0."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "weekly program",
    "text": "weekly program\n\nThis year‚Äôs course will be a bit different that planned due to the shortening of the academic semester. The information below is NOT up to date. Ask Yair what is relevant this year.\n\n\nweek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nweek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nweek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nweek 4\n\nLecture: Time series plotting: best practices. Dos and don‚Äôts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nweek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nweek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nweek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nweek 8\nSame as lecture 7 above\n\n\nweek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nweek 10\n\nLecture: Fourier decomposition, filtering, Nyquist‚ÄìShannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nweek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "who-cares.html#why-time-series-analysis",
    "href": "who-cares.html#why-time-series-analysis",
    "title": "who cares?",
    "section": "why ‚ÄúTime Series Analysis?‚Äù",
    "text": "why ‚ÄúTime Series Analysis?‚Äù\n\nTime has two aspects. There is the arrow, the running river, without which there is no change, no progress, or direction, or creation. And there is the circle or the cycle, without which there is chaos, meaningless succession of instants, a world without clocks or seasons or promises.\nURSULA K. LE GUIN\n\nYou are here because you are interested in how things change, evolve. In this course I want to discuss with you how to make sense of data whose temporal nature is in its very essence. We will talk about randomness, cycles, frequencies, correlations, and more."
  },
  {
    "objectID": "who-cares.html#why-environmental-sciences",
    "href": "who-cares.html#why-environmental-sciences",
    "title": "who cares?",
    "section": "why ‚ÄúEnvironmental Sciences‚Äù",
    "text": "why ‚ÄúEnvironmental Sciences‚Äù\nThis same time series analysis (TSA) course could be called instead ‚ÄúTSA for finance‚Äù, ‚ÄúTSA for Biology‚Äù, or any other application. The emphasis in this course is not Environmental Sciences, but the concepts and tools of TSA. Because my research is in Environmental Science, and many of the graduate students at HUJI-Rehovot research this, I chose to use examples ‚Äúclose to home‚Äù. The same toolset should be useful for students of other disciplines."
  },
  {
    "objectID": "who-cares.html#what-is-it-good-for",
    "href": "who-cares.html#what-is-it-good-for",
    "title": "who cares?",
    "section": "what is it good for?",
    "text": "what is it good for?\nIn many fields of science we are flooded by data, and it‚Äôs hard to see the forest for the trees. I hope that the topics we‚Äôll discuss in this course can help you find meaningful patterns in your data, formulate interesting hypotheses, and design better experiments."
  },
  {
    "objectID": "who-cares.html#do-i-need-it",
    "href": "who-cares.html#do-i-need-it",
    "title": "who cares?",
    "section": "do I need it?",
    "text": "do I need it?\nMaybe. If you are a grad student and you have temporal data to analyze, then probably yes. However, I have very fond memories of courses that I took as a grad student that were completely unrelated to my research. Sometimes ‚Äúbecause it‚Äôs fun‚Äù is a perfectly good answer."
  },
  {
    "objectID": "who-cares.html#what-will-i-actually-gain-from-it",
    "href": "who-cares.html#what-will-i-actually-gain-from-it",
    "title": "who cares?",
    "section": "what will I actually gain from it?",
    "text": "what will I actually gain from it?\nBy the end of this course you will have gained:\n\na hands-on experience of fundamental time-series analysis tools\nan intuition regarding the basic concepts\ntechnical abilities\na springboard for learning more about the subject by yourself"
  },
  {
    "objectID": "basics/boring.html#anaconda",
    "href": "basics/boring.html#anaconda",
    "title": "1¬† the boring stuff you absolutely need to do",
    "section": "1.1 Anaconda",
    "text": "1.1 Anaconda\nInstall Anaconda‚Äôs Python distribution. The Anaconda installation brings with it all the main python packages we will need to use. In order to install extra packages, refer to these two tutorials: tutorial 1, tutorial 2."
  },
  {
    "objectID": "basics/boring.html#vscode",
    "href": "basics/boring.html#vscode",
    "title": "1¬† the boring stuff you absolutely need to do",
    "section": "1.2 VSCode",
    "text": "1.2 VSCode\nInstall VSCode. Visual Studio Code is a very nice IDE (Integrated Development Environment) made by Microsoft, available to all operating systems. Contrary to the title of this page, it is not absolutely necessary to use it, but I like VSCode, and as my student, so do you üòâ."
  },
  {
    "objectID": "basics/boring.html#jupyter-notebooks",
    "href": "basics/boring.html#jupyter-notebooks",
    "title": "1¬† the boring stuff you absolutely need to do",
    "section": "1.3 jupyter notebooks",
    "text": "1.3 jupyter notebooks\nWe will code exclusively in Jupyter Notebooks. Get acquainted with them. Make sure you can point VSCode to the Anaconda environment of your choice (‚Äúbase‚Äù by default). Don‚Äôt worry, this is easier than it sounds.\nOne failproof way of making sure VSCode uses the Anaconda installation is the following:\n\nOpen Anaconda Navigator\nIf you are using HUJI‚Äôs computers, in ‚ÄúEnvironments‚Äù, choose ‚Äúasgard‚Äù. If you are using your own computer, ignore this step.\nopen VSCode from inside Anaconda Navigator (see image below).\n\n\nSometimes you will need to manualy install the Jupyter extension on VSCode. In this case follow this tutorial."
  },
  {
    "objectID": "basics/boring.html#folder-structure",
    "href": "basics/boring.html#folder-structure",
    "title": "1¬† the boring stuff you absolutely need to do",
    "section": "1.4 folder structure",
    "text": "1.4 folder structure\nYou NEED to be confortable with you computer‚Äôs folder (or directory) structure. Where are files located? How to navigate through different folders? How is my stuff organized? If you don‚Äôt feel absolutely comfortable with this, then read this, Windows, MacOS. If you use Linux then you surely know this stuff. Make yourself a ‚Äútime-series‚Äù folder wherever you want, and have it backed up regularly (use Google Drive, Dropbox, do it manually, etc). ‚ÄúMy dog deleted my files‚Äù is not an excuse."
  },
  {
    "objectID": "basics/numpy-pandas-matplotlib.html#pandas",
    "href": "basics/numpy-pandas-matplotlib.html#pandas",
    "title": "2¬† numpy, pandas, matplotlib",
    "section": "2.1 pandas",
    "text": "2.1 pandas\nWe will primarily use the Pandas package to deal with data. Pandas has become the standard Python tool to manipulate time series, and you should get acquainted with its basic usage. This course will provide you the opportunity to learn by example, but I‚Äôm sure we will only scratch the surface, and you‚Äôll be left with lots of questions.\nI provide below a (non-comprehensive) list of useful tutorials, they are a good reference for the beginner and for the experienced user.\n\nPython Data Science Handbook, by Jake VanderPlas\nData Wrangling with pandas Cheat Sheet\nWorking with Dates and Times in Python\nCheat Sheet: The pandas DataFrame Object\nYouTube tutorials by Corey Schafer"
  },
  {
    "objectID": "basics/numpy-pandas-matplotlib.html#pyplot",
    "href": "basics/numpy-pandas-matplotlib.html#pyplot",
    "title": "2¬† numpy, pandas, matplotlib",
    "section": "2.2 pyplot",
    "text": "2.2 pyplot\nMatplotlib, and its submodule pyplot, are probably the most common Python plotting tool. Pyplot is both great and horrible:\n\nGreat: you‚Äôll have absolutely full control of everything you want to plot. The sky is the limit.\nHorrible: you‚Äôll cry as you do it, because there is so much to know, and it is not the most friendly plotting package.\n\nPyplot is object oriented, so you will usually manipulate the axes object like this.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 2, 0, 3]\n\n# Figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 6))\n# plot on the left\nax1.plot(x, y, color=\"tab:blue\")\nax1.plot(x, y[::-1], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n        ylabel=\"something\",\n        title=\"left panel\")\n# plot on the right\nax2.plot(x, y[::-1])\nax2.set(xlabel=\"date\",\n        ylabel=\"something else\",\n        title=\"right panel\")\n\n[Text(0.5, 0, 'date'),\n Text(0, 0.5, 'something else'),\n Text(0.5, 1.0, 'right panel')]\n\n\n\n\n\nFor the very beginners, you need to know that figure refers to the whole white canvas, and axes means the rectangle inside which something will be plotted:\n\nThe image above is good because it has 2 panels, and it‚Äôs easy to understand what going on. Sadly, they mixed the two terms, axis and axes.\n\naxes is where the whole plot will be drawn. In the figure above it is the same as each panel.\naxis is each of the vertical and horizontal lines, where you have ticks and numbers.\n\n\nIf you are new to all this, I recommend that you go to:\n\nEarth Lab‚Äôs Introduction to Plotting in Python Using Matplotlib\nJake VanderPlas‚Äôs Python Data Science Handbook"
  },
  {
    "objectID": "basics/example.html#open-a-new-jupyter-notebook",
    "href": "basics/example.html#open-a-new-jupyter-notebook",
    "title": "3¬† learn by example",
    "section": "3.1 open a new Jupyter Notebook",
    "text": "3.1 open a new Jupyter Notebook\n\nOn your computer, open the program Anaconda Navigator (it may take a while to load).\nFind the white box called VS Code and click Launch.\nNow go to File &gt; Open Folder, and open the folder you created for this course. VS Code may ask you if you trust the authors, and the answer is ‚Äúyes‚Äù (it‚Äôs your computer).\nFile &gt; New File, and call it example.ipynb\nYou can start copying and pasting code from this website to your Jupyter Notebook. To run a cell, press Shift+Enter.\nYou may be asked to choose to Select Kernel. This is VS Code wanting to know which python installation to use. Click on ‚ÄúPython Environments‚Äù, and then choose the option with the word anaconda in it.\nThat‚Äôs all! Congratulations!"
  },
  {
    "objectID": "basics/example.html#import-packages",
    "href": "basics/example.html#import-packages",
    "title": "3¬† learn by example",
    "section": "3.2 import packages",
    "text": "3.2 import packages\nFirst, import packages to be used. They should all be already included in the Anaconda distribution you installed.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters"
  },
  {
    "objectID": "basics/example.html#load-data",
    "href": "basics/example.html#load-data",
    "title": "3¬† learn by example",
    "section": "3.3 load data",
    "text": "3.3 load data\nLoad CO2 data into a Pandas dataframe. You can load it directly from the URL (option 1), or first download the CSV to your computer and then load it (option 2). The link to download the data directly form NOAA is this. If for some reason this doesn‚Äôt work, download here.\n\n# option 1: load data directly from URL\n# url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url,\n#                  header=34,\n#                  na_values=[-999.99]\n#                  )\n\n# option 2: download first (use the URL above and save it to your computer), then load csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename,\n                comment='#',  # will ignore rows starting with #\n                 na_values=[-999.99]  # substitute -999.99 for NaN (Not a Number), data not available\n                 )\n# check how the dataframe (table) looks like\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\n\n\n0\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.39\n\n\n1\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n2\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n3\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n4\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2566\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2567\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2568\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2569\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2570\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2571 rows √ó 9 columns"
  },
  {
    "objectID": "basics/example.html#dealing-with-dates",
    "href": "basics/example.html#dealing-with-dates",
    "title": "3¬† learn by example",
    "section": "3.4 dealing with dates",
    "text": "3.4 dealing with dates\nCreate a new column called date, that combines the information from three separate columns: year, month, day.\n\n# function to_datetime translates the full date into a pandas datetime object,\n# that is, pandas knows this is a date, it's not just a string\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n# make 'date' column the dataframe index\ndf = df.set_index('date')\n# now see if everything is ok\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.39\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-07-23\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2023-07-30\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2023-08-06\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2023-08-13\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2023-08-20\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2571 rows √ó 9 columns"
  },
  {
    "objectID": "basics/example.html#first-plot",
    "href": "basics/example.html#first-plot",
    "title": "3¬† learn by example",
    "section": "3.5 first plot",
    "text": "3.5 first plot\nWe are now ready for our first plot! Let‚Äôs see the weekly CO2 average.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()"
  },
  {
    "objectID": "basics/example.html#first-plot-v2.0",
    "href": "basics/example.html#first-plot-v2.0",
    "title": "3¬† learn by example",
    "section": "3.6 first plot, v2.0",
    "text": "3.6 first plot, v2.0\nThe dates in the x-label are not great. Let‚Äôs try to make them prettier.\nWe need to import a few more packages first.\n\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()  # datetime converter for a matplotlib\n\nNow let‚Äôs replot.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2011-12-25', 389),  xycoords='data',\n    xytext=(-10, -80), textcoords='offset points',\n    arrowprops=dict(arrowstyle=\"-&gt;\",\n                    color=\"black\",\n                    connectionstyle=\"arc3,rad=0.2\"))\nfig.savefig(\"CO2-graph.png\", dpi=300)\n\n/var/folders/hc/jhnmlst937d27zzq9fhfks780000gn/T/ipykernel_10652/850389963.py:42: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.savefig(\"CO2-graph.png\", dpi=300)\n/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nThe dates on the horizontal axis are determined thus:\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nThis deremines the location of the ticks (between 3 and 5 ticks, whatever ‚Äúworks best‚Äù)\nax1.xaxis.set_major_locator(locator)\nThis actually puts the ticks in the positions determined above\nformatter = mdates.ConciseDateFormatter(locator)\nThis says that the labels will be placed at the locations determined in 1.\nax1.xaxis.set_major_formatter(formatter)\nFinally, labels are written down\n\nThe arrow is placed in the graph using annotate. It has a tricky syntax and a million options. Read Jake VanderPlas‚Äôs excellent examples to learn more."
  },
  {
    "objectID": "basics/example.html#modifications",
    "href": "basics/example.html#modifications",
    "title": "3¬† learn by example",
    "section": "3.7 modifications",
    "text": "3.7 modifications\nLet‚Äôs change a lot of plotting options to see how things could be different.\n\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,4)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"tab:blue\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax2.set(xlabel=\"date\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=5, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2010-12-25', 395),  xycoords='data',\n    xytext=(-100, 40), textcoords='offset points',\n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"white\"),\n    arrowprops=dict(arrowstyle=\"-&gt;\",\n                    color=\"black\",\n                    connectionstyle=\"angle,angleA=0,angleB=-90,rad=40\"))\n\nText(-100, 40, '2010/11')\n\n\n\n\n\nThe main changes were:\n\nUsing the Seaborn package, we changed the fontsize and the overall plot style. Read more.\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\nWe changed the colors of the lineplots. To know what colors exist, click here.\nThe arrow annotation has a different style. Read more."
  },
  {
    "objectID": "basics/example.html#playing-with-the-code",
    "href": "basics/example.html#playing-with-the-code",
    "title": "3¬† learn by example",
    "section": "3.8 playing with the code",
    "text": "3.8 playing with the code\nI encourage you to play with the code you just ran. An easy way of learning what each line does is to comment something out and see what changes in the output you see. If you feel brave, try to modify the code a little bit."
  },
  {
    "objectID": "basics/AI-policy.html",
    "href": "basics/AI-policy.html",
    "title": "4¬† AI policy",
    "section": "",
    "text": "The guidelines below are an adaptation of Ethan Mollick‚Äôs extremely useful ideas on AI as an assistant tool for teaching.\nI EXPECT YOU to use LLMs (large language models) such as ChatGPT, Bing AI, Google Bard, or whatever else springs up since the time of this writing. You should familiarize yourself with the AI‚Äôs capabilities and limitations.\nUse LLMs to help you learn, chat with them about what you want to accomplish and learn from them how to do it. Ask your LLM what each part of the code means, copy and pasting blindly is unacceptable. You are here to learn.\nConsider the following important points:\n\nUltimately, you, the student, are responsible for the assignment.\nAcknowledge the use of AI in your assignment. Be transparent about your use of the tool and the extent of assistance it provided."
  },
  {
    "objectID": "resampling/motivation.html#jerusalem-2019",
    "href": "resampling/motivation.html#jerusalem-2019",
    "title": "5¬† motivation",
    "section": "5.1 Jerusalem, 2019",
    "text": "5.1 Jerusalem, 2019\nData from the Israel Meteorological Service, IMS.\nSee the temperature at a weather station in Jerusalem, for the whole 2019 year. This is an interactive graph: to zoom in, play with the bottom panel.\n\n\n\n\n\n\n\n\n discussion\nThe temperature fluctuates on various time scales, from daily to yearly. Let‚Äôs think together a few questions we‚Äôd like to ask about the data above.\n\nNow let‚Äôs see precipitation data:\n\n\n\n\n\n\n\n\n discussion\nWhat would be interesting to know about precipitation?\n\nWe have not talked about what kind of data we have in our hands here. The csv file provided by the IMS looks like this:\n\n\n\n\n\n\n\n\n\nStation\nDate & Time (Winter)\nDiffused radiation (W/m^2)\nGlobal radiation (W/m^2)\nDirect radiation (W/m^2)\nRelative humidity (%)\nTemperature (¬∞C)\nMaximum temperature (¬∞C)\nMinimum temperature (¬∞C)\nWind direction (¬∞)\nGust wind direction (¬∞)\nWind speed (m/s)\nMaximum 1 minute wind speed (m/s)\nMaximum 10 minutes wind speed (m/s)\nTime ending maximum 10 minutes wind speed (hhmm)\nGust wind speed (m/s)\nStandard deviation wind direction (¬∞)\nRainfall (mm)\n\n\n\n\n0\nJerusalem Givat Ram\n01/01/2019 00:00\n0.0\n0.0\n0.0\n80.0\n8.7\n8.8\n8.6\n75.0\n84.0\n3.3\n4.3\n3.5\n23:58\n6.0\n15.6\n0.0\n\n\n1\nJerusalem Givat Ram\n01/01/2019 00:10\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n74.0\n82.0\n3.3\n4.1\n3.3\n00:01\n4.9\n14.3\n0.0\n\n\n2\nJerusalem Givat Ram\n01/01/2019 00:20\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n76.0\n82.0\n3.2\n4.1\n3.3\n00:19\n4.9\n9.9\n0.0\n\n\n3\nJerusalem Givat Ram\n01/01/2019 00:30\n0.0\n0.0\n0.0\n79.0\n8.7\n8.7\n8.6\n78.0\n73.0\n3.6\n4.2\n3.6\n00:30\n5.2\n11.7\n0.0\n\n\n4\nJerusalem Givat Ram\n01/01/2019 00:40\n0.0\n0.0\n0.0\n79.0\n8.6\n8.7\n8.5\n80.0\n74.0\n3.6\n4.4\n3.8\n00:35\n5.4\n10.5\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n52549\nJerusalem Givat Ram\n31/12/2019 22:20\n0.0\n0.0\n1.0\n81.0\n7.4\n7.6\n7.3\n222.0\n255.0\n0.5\n0.9\n1.0\n22:11\n1.0\n47.9\n0.0\n\n\n52550\nJerusalem Givat Ram\n31/12/2019 22:30\n0.0\n0.0\n1.0\n83.0\n7.3\n7.4\n7.3\n266.0\n259.0\n0.6\n0.8\n0.6\n22:28\n1.1\n22.8\n0.0\n\n\n52551\nJerusalem Givat Ram\n31/12/2019 22:40\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.3\n331.0\n317.0\n0.5\n0.8\n0.6\n22:35\n1.0\n31.6\n0.0\n\n\n52552\nJerusalem Givat Ram\n31/12/2019 22:50\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.4\n312.0\n285.0\n0.6\n1.0\n0.6\n22:50\n1.4\n31.3\n0.0\n\n\n52553\nJerusalem Givat Ram\n31/12/2019 23:00\n0.0\n0.0\n1.0\n83.0\n7.6\n7.7\n7.4\n315.0\n321.0\n0.7\n1.0\n0.8\n22:54\n1.3\n23.5\n0.0\n\n\n\n\n52554 rows √ó 18 columns\n\n\n\nWe see that we have data points spaced out evenly every 10 minutes."
  },
  {
    "objectID": "resampling/motivation.html#challenges",
    "href": "resampling/motivation.html#challenges",
    "title": "5¬† motivation",
    "section": "5.2 Challenges",
    "text": "5.2 Challenges\nLet‚Äôs try to answer the following questions:\n\n\n\n\n\n\n What is the mean temperature for each month?\n\n\n\n\n\nFirst we have to divide temperature data by month, and then take the average for each month.\n\n\na possible solution\n\ndf_month = df['temperature'].resample('M').mean()\n\n\n\n\n\n\n\n\n\n\n For each month, what is the mean of the daily maximum temperature? What about the minimun?\n\n\n\n\n\nThis is a bit trickier.\n\nWe need to find the maximum/minimum temperature for each day.\nOnly then we split the daily data by month and take the average.\n\n\n\na possible solution\n\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_month['max temp'] = df_day['max temp'].resample('MS').mean()\n\n\n\n\n\n\n\n\n\n\n What is the average night temperature for every season? What about the day temperature?\n\n\n\n\n\n\nWe need to filter our data to contain only night times.\nWe need to divide rain data by seasons (3 months), and then take the mean for each season.\n\n\n\na possible solution\n\n# filter only night data\ndf_night = df.loc[((df.index.hour &lt; 6) | (df.index.hour &gt;= 18))]\nseason_average_night_temp = df_night['temperature'].resample('Q').mean()\n\n\n\nanother possible solution\n\n# filter using between_time\ndf_night = df.between_time('18:00', '06:00', inclusive='left')\nseason_average_night_temp = df_night['temperature'].resample('Q').mean()\n\n\n\n\n\n\n\n\n\n\n What is the daily precipitation?\n\n\n\n\n\nFirst we have to divide rain data by day, and then take the sum for each day.\n\n\na possible solution\n\ndaily_precipitation = df['rain'].resample('D').sum()\n\n\n\n\n\n\n\n\n\n\n How much rain was there every month?\n\n\n\n\n\nWe have to divide rain data by month, and then sum the totals of each month.\n\n\na possible solution\n\nmonthly_precipitation = df['rain'].resample('M').sum()\n\n\n\n\n\n\n\n\n\n\n How many rainy days were there each month?\n\n\n\n\n\n\nWe need to sum rain by day.\nWe need to count how many days are there each month where rain &gt; 0.\n\n\n\na possible solution\n\ndaily_precipitation = df['rain'].resample('D').sum()\nonly_rainy_days = daily_precipitation.loc[daily_precipitation &gt; 0]\nrain_days_per_month = only_rainy_days.resample('M').count()\n\n\n\n\n\n\n\n\n\n\n How many days, hours, and minutes were between the last rain of the season (Malkosh) to the first (Yoreh)?\n\n\n\n\n\n\nWe need to divide our data into two: rainy_season_1 and rainy_season_2.\nWe need to find the time of the last rain in rainy_season_1.\nWe need to find the time of the first rain in rainy_season_2.\nWe need to compute the time difference between the two dates.\n\n\n\na possible solution\n\nsplit_date = '2019-08-01'\nrainy_season_1 = df[:split_date]  # everything before split date\nrainy_season_2 = df[split_date:]  # everything after split date\nmalkosh = rainy_season_1['rain'].loc[rainy_season_1['rain'] &gt; 0].last_valid_index()\nyoreh = rainy_season_2['rain'].loc[rainy_season_2['rain'] &gt; 0].first_valid_index()\ndry_period = yoreh - malkosh\n# extracting days, hours, and minutes\ndays = dry_period.days\nhours = dry_period.components.hours\nminutes = dry_period.components.minutes\nprint(f'The dry period of 2019 was {days} days, {hours} hours and {minutes} minutes.')\n\n\n\n\n\n\n\n\n\n\n What was the rainiest morning (6am-12pm) of the year? Bonus, what about the rainiest night (6pm-6am)?\n\n\n\n\n\n\nWe need to filter our data to contain only morning times.\nWe need to sum rain by day.\nWe need to find the day with the maximum value.\n\n\n\na possible solution\n\n# filter to only day data\nmorning_df = df.loc[((df.index.hour &gt;= 6) & (df.index.hour &lt; 18))]\nmorning_rain = morning_df['rain'].resample('D').sum()\nrainiest_morning = morning_rain.idxmax()\n# plot\nmorning_rain.plot()\nplt.axvline(rainiest_morning, c='r', alpha=0.5, linestyle='--')\n\n\n\nbonus solution\n\n# filter to only night data\ndf_night = df.loc[((df.index.hour &lt; 6) | (df.index.hour &gt;= 18))]\n# resampling night for each day is tricky because the date changes at 12:00. We can do this trick:\n# we shift the time back by 6 hours so all the data for the same night will have the same date.\ndf_shifted = df_night.tshift(-6, freq='H')\nnight_rain = df_shifted['rain'].resample('D').sum()\nrainiest_night = night_rain.idxmax()\n# plot\nnight_rain.plot()\nplt.axvline(rainiest_night, c='r', alpha=0.5, linestyle='--')\n\n\n\n\nNote: this whole webpage is actually a Jupyter Notebook rendered as html. If you want to know how to make interactive graphs, go to the top of the page and click on ‚Äú Code‚Äù\nUseful functions compatible with pandas.resample() can be found here. The full list of resampling frequencies can be found here."
  },
  {
    "objectID": "resampling/resampling.html",
    "href": "resampling/resampling.html",
    "title": "6¬† resampling",
    "section": "",
    "text": "We can only really understand how to calculate monthly means if we do it ourselves.\nFirst, let‚Äôs import a bunch of packages we need to use.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\nNow we load the csv file for Jerusalem (2019), provided by the IMS.\n\ndiscussion\nWe will go to the IMS website together and see what are the options available and how to download. If you just need the csv right away, download it here.\n\n\nWe substitute every occurence of - for NaN (not a number, that is, the data is missing).\nWe call the columns Temperature (¬∞C) and Rainfall (mm) with more convenient names, since we will be using them a lot.\nWe interpret the column Date & Time (Winter) as a date, saying to python that day comes first.\nWe make date the index of the dataframe.\n\n\nfilename = \"../archive/data/jerusalem2019.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Temperature (¬∞C)': 'temperature',\n                   'Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nStation\nDate & Time (Winter)\nDiffused radiation (W/m^2)\nGlobal radiation (W/m^2)\nDirect radiation (W/m^2)\nRelative humidity (%)\ntemperature\nMaximum temperature (¬∞C)\nMinimum temperature (¬∞C)\nWind direction (¬∞)\nGust wind direction (¬∞)\nWind speed (m/s)\nMaximum 1 minute wind speed (m/s)\nMaximum 10 minutes wind speed (m/s)\nTime ending maximum 10 minutes wind speed (hhmm)\nGust wind speed (m/s)\nStandard deviation wind direction (¬∞)\nrain\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-01-01 00:00:00\nJerusalem Givat Ram\n01/01/2019 00:00\n0.0\n0.0\n0.0\n80.0\n8.7\n8.8\n8.6\n75.0\n84.0\n3.3\n4.3\n3.5\n23:58\n6.0\n15.6\n0.0\n\n\n2019-01-01 00:10:00\nJerusalem Givat Ram\n01/01/2019 00:10\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n74.0\n82.0\n3.3\n4.1\n3.3\n00:01\n4.9\n14.3\n0.0\n\n\n2019-01-01 00:20:00\nJerusalem Givat Ram\n01/01/2019 00:20\n0.0\n0.0\n0.0\n79.0\n8.7\n8.8\n8.7\n76.0\n82.0\n3.2\n4.1\n3.3\n00:19\n4.9\n9.9\n0.0\n\n\n2019-01-01 00:30:00\nJerusalem Givat Ram\n01/01/2019 00:30\n0.0\n0.0\n0.0\n79.0\n8.7\n8.7\n8.6\n78.0\n73.0\n3.6\n4.2\n3.6\n00:30\n5.2\n11.7\n0.0\n\n\n2019-01-01 00:40:00\nJerusalem Givat Ram\n01/01/2019 00:40\n0.0\n0.0\n0.0\n79.0\n8.6\n8.7\n8.5\n80.0\n74.0\n3.6\n4.4\n3.8\n00:35\n5.4\n10.5\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019-12-31 22:20:00\nJerusalem Givat Ram\n31/12/2019 22:20\n0.0\n0.0\n1.0\n81.0\n7.4\n7.6\n7.3\n222.0\n255.0\n0.5\n0.9\n1.0\n22:11\n1.0\n47.9\n0.0\n\n\n2019-12-31 22:30:00\nJerusalem Givat Ram\n31/12/2019 22:30\n0.0\n0.0\n1.0\n83.0\n7.3\n7.4\n7.3\n266.0\n259.0\n0.6\n0.8\n0.6\n22:28\n1.1\n22.8\n0.0\n\n\n2019-12-31 22:40:00\nJerusalem Givat Ram\n31/12/2019 22:40\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.3\n331.0\n317.0\n0.5\n0.8\n0.6\n22:35\n1.0\n31.6\n0.0\n\n\n2019-12-31 22:50:00\nJerusalem Givat Ram\n31/12/2019 22:50\n0.0\n0.0\n1.0\n83.0\n7.5\n7.6\n7.4\n312.0\n285.0\n0.6\n1.0\n0.6\n22:50\n1.4\n31.3\n0.0\n\n\n2019-12-31 23:00:00\nJerusalem Givat Ram\n31/12/2019 23:00\n0.0\n0.0\n1.0\n83.0\n7.6\n7.7\n7.4\n315.0\n321.0\n0.7\n1.0\n0.8\n22:54\n1.3\n23.5\n0.0\n\n\n\n\n52554 rows √ó 18 columns\n\n\n\nWith resample it‚Äôs easy to compute monthly averages. Resample by itself only divides the data into buckets (in this case monthly buckets), and waits for a further instruction. Here, the next instruction is mean.\n\ndf_month = df['temperature'].resample('M').mean()\ndf_month\n\ndate\n2019-01-31     9.119937\n2019-02-28     9.629812\n2019-03-31    10.731571\n2019-04-30    14.514329\n2019-05-31    22.916894\n2019-06-30    23.587361\n2019-07-31    24.019403\n2019-08-31    24.050822\n2019-09-30    22.313287\n2019-10-31    20.641868\n2019-11-30    17.257153\n2019-12-31    11.224131\nFreq: M, Name: temperature, dtype: float64\n\n\nInstead of M for month, which other options do I have? The full list can be found here, but the most commonly used are:\nM         month end frequency\nMS        month start frequency\nA         year end frequency\nAS, YS    year start frequency\nD         calendar day frequency\nH         hourly frequency\nT, min    minutely frequency\nS         secondly frequency\nThe results we got for the monthly means were given as a pandas series, not dataframe. Let‚Äôs correct this:\n\ndf_month = (df['temperature'].resample('M')         # resample by month\n                             .mean()                # take the mean\n                             .to_frame('mean temp') # make output a dafaframe\n           )\ndf_month\n\n\n\n\n\n\n\n\nmean temp\n\n\ndate\n\n\n\n\n\n2019-01-31\n9.119937\n\n\n2019-02-28\n9.629812\n\n\n2019-03-31\n10.731571\n\n\n2019-04-30\n14.514329\n\n\n2019-05-31\n22.916894\n\n\n2019-06-30\n23.587361\n\n\n2019-07-31\n24.019403\n\n\n2019-08-31\n24.050822\n\n\n2019-09-30\n22.313287\n\n\n2019-10-31\n20.641868\n\n\n2019-11-30\n17.257153\n\n\n2019-12-31\n11.224131\n\n\n\n\n\n\n\n\nhot tip\nSometimes, a line of code can get too long and messy. In the code above, we broke line for every step, which makes the process so much cleaner. We highly advise you to do the same. Attention: This trick works as long as all the elements are inside the same parenthesis.\n\nNow it‚Äôs time to plot!\n\nfig, ax = plt.subplots()\nax.plot(df_month['mean temp'], color='black')\nax.set(ylabel='Temperature (¬∞C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\")\n\n[Text(0, 0.5, 'Temperature (¬∞C)'),\n [&lt;matplotlib.axis.YTick at 0x7faf784c6d60&gt;,\n  &lt;matplotlib.axis.YTick at 0x7faf7843a220&gt;,\n  &lt;matplotlib.axis.YTick at 0x7faf784c62b0&gt;,\n  &lt;matplotlib.axis.YTick at 0x7faf784f3400&gt;,\n  &lt;matplotlib.axis.YTick at 0x7faf784f3760&gt;,\n  &lt;matplotlib.axis.YTick at 0x7faf784fa5b0&gt;],\n Text(0.5, 1.0, 'Jerusalem, 2019')]\n\n\n\n\n\nThe dates in the horizontal axis are not great. An easy fix is to use the month numbers instead of dates. \n\nfig, ax = plt.subplots()\nax.plot(df_month.index.month, df_month['mean temp'], color='black')\nax.set(xlabel=\"month\",\n       ylabel='Temperature (¬∞C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\",);\n\n\n\n\n\ndiscussion\nWhen you have datetime as the dataframe index, you don‚Äôt need to give the function plot two arguments, date and values. You can just tell plot to use the column you want, the function will take the dates by itself.\n What does this line mean?\ndf_month['mean temp'].index.month\nPrint on the screen the following, and see yourself what each thing is:\n\ndf_month\ndf_month.index\ndf_month.index.month\ndf_month.index.day\n\n\nWe‚Äôre done! Congratulations :)\nNow we need to calculate the average minimum/maximum daily temperatures. We start by creating an empty dataframe.\n\ndf_day = pd.DataFrame()\n\nNow resample data by day (D), and take the min/max of each day.\n\ndf_day['min temp'] = df['temperature'].resample('D').min()\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_day\n\n\n\n\n\n\n\n\nmin temp\nmax temp\n\n\ndate\n\n\n\n\n\n\n2019-01-01\n7.5\n14.1\n\n\n2019-01-02\n6.6\n11.5\n\n\n2019-01-03\n6.3\n10.7\n\n\n2019-01-04\n6.6\n14.6\n\n\n2019-01-05\n7.0\n11.4\n\n\n...\n...\n...\n\n\n2019-12-27\n4.4\n7.4\n\n\n2019-12-28\n6.6\n10.3\n\n\n2019-12-29\n8.1\n12.5\n\n\n2019-12-30\n6.9\n13.0\n\n\n2019-12-31\n5.2\n13.3\n\n\n\n\n365 rows √ó 2 columns\n\n\n\nThe next step is to calculate the average minimum/maximum for each month. This is similar to what we did above.\n\ndf_month['min temp'] = df_day['min temp'].resample('M').mean()\ndf_month['max temp'] = df_day['max temp'].resample('M').mean()\ndf_month\n\n\n\n\n\n\n\n\nmean temp\nmin temp\nmax temp\n\n\ndate\n\n\n\n\n\n\n\n2019-01-31\n9.119937\n5.922581\n12.470968\n\n\n2019-02-28\n9.629812\n6.825000\n13.089286\n\n\n2019-03-31\n10.731571\n7.532258\n14.661290\n\n\n2019-04-30\n14.514329\n10.866667\n19.113333\n\n\n2019-05-31\n22.916894\n17.296774\n29.038710\n\n\n2019-06-30\n23.587361\n19.163333\n28.860000\n\n\n2019-07-31\n24.019403\n19.367742\n29.564516\n\n\n2019-08-31\n24.050822\n19.903226\n29.767742\n\n\n2019-09-30\n22.313287\n18.430000\n28.456667\n\n\n2019-10-31\n20.641868\n16.945161\n26.190323\n\n\n2019-11-30\n17.257153\n14.066667\n21.436667\n\n\n2019-12-31\n11.224131\n8.806452\n14.448387\n\n\n\n\n\n\n\nLet‚Äôs plot‚Ä¶\n\nfig, ax = plt.subplots()\nax.plot(df_month['max temp'], color='tab:red', label='max')\nax.plot(df_month['mean temp'], color='black', label='mean')\nax.plot(df_month['min temp'], color='tab:blue', label='min')\nax.set(ylabel='Temperature (¬∞C)',\n       yticks=np.arange(10,35,5),\n       title=\"Jerusalem, 2019\")\nax.xaxis.set_major_locator(mdates.MonthLocator(range(1, 13, 2), bymonthday=15))\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nax.legend(fontsize=12, frameon=False);\n\n\n\n\nVoil√†! You made a beautiful graph!\n\ndiscussion\nThis time we did not put month numbers in the horizontal axis, we now have month names. How did we do this black magic, you ask? See lines 8‚Äì10 above. Matplotlib gives you absolute power over what to put in the axis, if you can only know how to tell it to‚Ä¶ Wanna know more? Click here."
  },
  {
    "objectID": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "href": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "title": "7¬† upsampling",
    "section": "7.1 Potential Evapotranspiration using Penman‚Äôs equation",
    "text": "7.1 Potential Evapotranspiration using Penman‚Äôs equation\nWe want to calculate the daily potential evapotranspiration using Penman‚Äôs equation. Part of the calculation involves characterizing the energy budget on soil surface. When direct solar radiation measurements are not available, we can estimate the energy balance by knowing the ‚Äúcloudless skies mean solar radiation‚Äù, R_{so}. This is the amount of energy (MJ/m^2/d) that hits the surface, assuming no clouds. This radiation depends on the season and on the latitude you are. For Israel, located at latitude 32¬∞ N, we can use the following data for 30¬∞:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\ndates = pd.date_range(start='2021-01-01', periods=13, freq='MS')\nvalues = [17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04, 17.46]\ndf = pd.DataFrame({'date': dates, 'radiation': values})\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nradiation\n\n\ndate\n\n\n\n\n\n2021-01-01\n17.46\n\n\n2021-02-01\n21.65\n\n\n2021-03-01\n25.96\n\n\n2021-04-01\n29.85\n\n\n2021-05-01\n32.11\n\n\n2021-06-01\n33.20\n\n\n2021-07-01\n32.66\n\n\n2021-08-01\n30.44\n\n\n2021-09-01\n26.67\n\n\n2021-10-01\n22.48\n\n\n2021-11-01\n18.30\n\n\n2021-12-01\n16.04\n\n\n2022-01-01\n17.46\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None')\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30¬∞ N\")\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nWe only have 12 values for the whole year, and we can‚Äôt use this dataframe to compute daily ET. We need to upsample!\nIn the example below, we resample the monthly data into daily data, and do nothing else. Pandas doesn‚Äôt know what to do with the new points, so it fills them with NaN.\n\ndf_nan = df['radiation'].resample('D').asfreq().to_frame()\ndf_nan.head(33)\n\n\n\n\n\n\n\n\nradiation\n\n\ndate\n\n\n\n\n\n2021-01-01\n17.46\n\n\n2021-01-02\nNaN\n\n\n2021-01-03\nNaN\n\n\n2021-01-04\nNaN\n\n\n2021-01-05\nNaN\n\n\n2021-01-06\nNaN\n\n\n2021-01-07\nNaN\n\n\n2021-01-08\nNaN\n\n\n2021-01-09\nNaN\n\n\n2021-01-10\nNaN\n\n\n2021-01-11\nNaN\n\n\n2021-01-12\nNaN\n\n\n2021-01-13\nNaN\n\n\n2021-01-14\nNaN\n\n\n2021-01-15\nNaN\n\n\n2021-01-16\nNaN\n\n\n2021-01-17\nNaN\n\n\n2021-01-18\nNaN\n\n\n2021-01-19\nNaN\n\n\n2021-01-20\nNaN\n\n\n2021-01-21\nNaN\n\n\n2021-01-22\nNaN\n\n\n2021-01-23\nNaN\n\n\n2021-01-24\nNaN\n\n\n2021-01-25\nNaN\n\n\n2021-01-26\nNaN\n\n\n2021-01-27\nNaN\n\n\n2021-01-28\nNaN\n\n\n2021-01-29\nNaN\n\n\n2021-01-30\nNaN\n\n\n2021-01-31\nNaN\n\n\n2021-02-01\n21.65\n\n\n2021-02-02\nNaN"
  },
  {
    "objectID": "resampling/upsampling.html#forwardbackward-fill",
    "href": "resampling/upsampling.html#forwardbackward-fill",
    "title": "7¬† upsampling",
    "section": "7.2 Forward/Backward fill",
    "text": "7.2 Forward/Backward fill\nWe can forward/backward fill these NaNs:\n\ndf_forw = df['radiation'].resample('D').ffill().to_frame()\ndf_back = df['radiation'].resample('D').bfill().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_forw['radiation'], color='tab:blue', label=\"forward fill\")\nax.plot(df_back['radiation'], color='tab:orange', label=\"backward fill\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30¬∞ N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThis does the job, but I want something better, not step functions. The radiation should vary smoothly from day to day. Let‚Äôs use interpolation."
  },
  {
    "objectID": "resampling/upsampling.html#interpolation",
    "href": "resampling/upsampling.html#interpolation",
    "title": "7¬† upsampling",
    "section": "7.3 Interpolation",
    "text": "7.3 Interpolation\n\ndf_linear = df['radiation'].resample('D').interpolate(method='time').to_frame()\ndf_cubic = df['radiation'].resample('D').interpolate(method='cubic').to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_linear['radiation'], color='tab:blue', label=\"linear interpolation\")\nax.plot(df_cubic['radiation'], color='tab:orange', label=\"cubic interpolation\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30¬∞ N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThere are many ways to fill NaNs and to interpolate. A nice detailed guide can be found here."
  },
  {
    "objectID": "resampling/interpolation.html",
    "href": "resampling/interpolation.html",
    "title": "8¬† interpolation",
    "section": "",
    "text": "Interpolation is the act of getting data you don‚Äôt have from data you alreay have. We used some interpolation when upsampling, and now it is time to talk about it a little bit more in depth.\nThere is no one correct way of interpolating, the method you use depends in the end on what you want to accomplish, what are your (hidden or explicit) assumptions, etc. Let‚Äôs see a few examples."
  },
  {
    "objectID": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "href": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "title": "9¬† FAQ",
    "section": "9.1 How to resample by year, but have it end in September?",
    "text": "9.1 How to resample by year, but have it end in September?\nThis is called anchored offset. One possible use to it is to calculate statistics according to the hydrological year that, for example, ends in September.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\nfilename = \"../archive/data/Kinneret_Kvuza_daily_rainfall.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Date': 'date',\n                   'Daily Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace\ndf\n\n\n\n\n\n\n\n\nStation\nrain\n\n\ndate\n\n\n\n\n\n\n1980-01-02\nKinneret Kvuza 09/1977-08/2023\n0.0\n\n\n1980-01-03\n0\n0.0\n\n\n1980-01-04\n0\n0.0\n\n\n1980-01-05\nKinneret Kvuza 09/1977-08/2023\n35.5\n\n\n1980-01-06\nKinneret Kvuza 09/1977-08/2023\n2.2\n\n\n...\n...\n...\n\n\n2019-12-26\nKinneret Kvuza 09/1977-08/2023\n39.4\n\n\n2019-12-27\nKinneret Kvuza 09/1977-08/2023\n5.2\n\n\n2019-12-28\nKinneret Kvuza 09/1977-08/2023\n1.6\n\n\n2019-12-29\n0\n0.0\n\n\n2019-12-30\nKinneret Kvuza 09/1977-08/2023\n0.1\n\n\n\n\n14608 rows √ó 2 columns\n\n\n\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(df['rain'], color='black')\nax[1].plot(df.loc['1998':'2000', 'rain'], color='black')\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\nfig.text(0.02, 0.5, 'daily precipitation (mm)', va='center', rotation='vertical')\nax[0].set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')\n\n\n\n\n\nWe see a marked dry season during the summer, so let‚Äôs assume the Hydrological Year ends in September.\n\ndf_year = df.resample('A-SEP').sum()\ndf_year = df_year.loc['1980':'2003']\ndf_year\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_94063/2047090134.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_year = df.resample('A-SEP').sum()\n\n\n\n\n\n\n\n\n\nrain\n\n\ndate\n\n\n\n\n\n1980-09-30\n355.5\n\n\n1981-09-30\n463.1\n\n\n1982-09-30\n221.7\n\n\n1983-09-30\n557.1\n\n\n1984-09-30\n335.3\n\n\n1985-09-30\n379.8\n\n\n1986-09-30\n300.7\n\n\n1987-09-30\n424.7\n\n\n1988-09-30\n421.6\n\n\n1989-09-30\n251.6\n\n\n1990-09-30\n432.5\n\n\n1991-09-30\n328.3\n\n\n1992-09-30\n738.4\n\n\n1993-09-30\n434.9\n\n\n1994-09-30\n255.4\n\n\n1995-09-30\n408.6\n\n\n1996-09-30\n373.0\n\n\n1997-09-30\n416.2\n\n\n1998-09-30\n451.9\n\n\n1999-09-30\n227.8\n\n\n2000-09-30\n378.9\n\n\n2001-09-30\n273.9\n\n\n2002-09-30\n445.2\n\n\n2003-09-30\n602.4\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(df_year.index, df_year['rain'], color='black',\n       width=365)\nax.set_ylabel(\"yearly precipitation (mm)\")\nax.set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')"
  },
  {
    "objectID": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "href": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "title": "9¬† FAQ",
    "section": "9.2 When upsampling, how to fill missing values with zero?",
    "text": "9.2 When upsampling, how to fill missing values with zero?\nWe did that in the example above, like this:\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace"
  },
  {
    "objectID": "smoothing/motivation.html#tumbling-vs-sliding",
    "href": "smoothing/motivation.html#tumbling-vs-sliding",
    "title": "10¬† motivation",
    "section": "10.1 Tumbling vs Sliding",
    "text": "10.1 Tumbling vs Sliding"
  },
  {
    "objectID": "smoothing/sliding.html#convolution",
    "href": "smoothing/sliding.html#convolution",
    "title": "11¬† sliding window",
    "section": "11.1 convolution",
    "text": "11.1 convolution\nConvolution is a fancy word for averaging a time series using a sliding window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/sliding.html#kernels",
    "href": "smoothing/sliding.html#kernels",
    "title": "11¬† sliding window",
    "section": "11.2 kernels",
    "text": "11.2 kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation.\n\n\n\n\nSee how the three kernel shapes compare. There are many kernels to chose from."
  },
  {
    "objectID": "smoothing/sliding.html#math",
    "href": "smoothing/sliding.html#math",
    "title": "11¬† sliding window",
    "section": "11.3 math",
    "text": "11.3 math\nThe definition of a convolution between signal f(t) and kernel k(t) is\n\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\nThe expression f*k denotes the convolution of these two functions. The argument of k is t-\\tau, meaning that the kernel runs from left to right (as t does), and at every point the two signals (f and k) are multiplied together. It is the product of the signal with the weight function k that gives us an average. Because of -\\tau, the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/sliding.html#numerics",
    "href": "smoothing/sliding.html#numerics",
    "title": "11¬† sliding window",
    "section": "11.4 numerics",
    "text": "11.4 numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes:\n\ndf['temp_smoothed'] = (\n                       df['TD'].rolling(window='500min',\n                                        min_periods=50   # comment this to see what happens\n                                        )\n                               .mean()\n                      )\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(df['TD'], color='black')\nax.plot(df['temp_smoothed'], color='xkcd:hot pink')\nax.set(**plot_settings)\ncentered_dates(ax)\n\n\n\n\nThe pink curve looks smooth, but why does it lag behind the data?! What‚Äôs going on?\n\n11.4.1 7-day average of COVID-19 infections\nDuring the COVID-19 pandemic, we would see graphs like this all the time in the news:\n\n\nimport COVID-19 data for Israel, process it\n# data from https://health.google.com/covid-19/open-data/raw-data?loc=IL\n# define the local file path\nlocal_file_path = 'COVID_19_israel.csv'\n# check if the local file exists\nif os.path.exists(local_file_path):\n    # if the local file exists, load it\n    covid_IL = pd.read_csv(local_file_path, parse_dates=['date'], index_col='date')\nelse:\n    # if the local file doesn't exist, download from the URL\n    url = \"https://storage.googleapis.com/covid19-open-data/v3/location/IL.csv\"\n    covid_IL = pd.read_csv(url, parse_dates=['date'], index_col='date')\n    # save the downloaded data to the local file for future use\n    covid_IL.to_csv(local_file_path)\n\ndf_covid = covid_IL['new_confirmed'].to_frame()\ndf_covid['7d_avg'] = df_covid['new_confirmed'].rolling(window='7D').mean()\n\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(8,5))\nst = '2022-01-01'\nen = '2022-03-30'\nnew_cases = ax.bar(df_covid[st:en].index, df_covid.loc[st:en,'new_confirmed'],\n       color=\"tab:blue\", width=1)\nmov_avg, = ax.plot(df_covid.loc[st:en,'7d_avg'],\n        color='xkcd:hot pink')\nax.legend(handles=[new_cases, mov_avg],\n          labels=['new confirmed cases', '7-day moving average'],\n          frameon=False)\nweird_day = \"2022-02-12\"\nweird_day_x = mdates.date2num(dt.datetime.strptime(weird_day, \"%Y-%m-%d\"))\nax.text(weird_day_x, df_covid.loc[weird_day,'new_confirmed'], \"?\")\n# formating dates on x axis\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\nax.xaxis.set_major_locator(locator)\nax.xaxis.set_major_formatter(formatter)\n\n\n\n\n\nTake a look at the moving average next to the question mark. How can it be that high, when all the bars around that date are lower? Is the calculation right?\nThe answer is that the result of the moving average is assigned to the right-most date in the running window. This is reasonable for COVID-19 cases: for a given day, I can only calculate a 7-day average based on past values, I don‚Äôt know what the future will be.\nThere is a simple way of assigning the result to the center of the window:\n\ndf_covid['7d_avg_center'] = (\n                             df_covid['new_confirmed']\n                                 .rolling(window='7D',\n                                          center=True)  # THIS\n                                 .mean()\n                            )\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(8,5))\nst = '2022-01-01'\nen = '2022-03-30'\nnew_cases = ax.bar(df_covid[st:en].index, df_covid.loc[st:en,'new_confirmed'],\n       color=\"tab:blue\", width=1)\nmov_avg, = ax.plot(df_covid.loc[st:en,'7d_avg'],\n        color='xkcd:hot pink')\nmov_avg_center, = ax.plot(df_covid.loc[st:en,'7d_avg_center'],\n                          color='xkcd:mustard')\nax.legend(handles=[new_cases, mov_avg, mov_avg_center],\n          labels=['new confirmed cases',\n                  '7-day moving average',\n                  'CENTERED 7-day\\nmoving average'],\n          frameon=False)\n# formating dates on x axis\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\nax.xaxis.set_major_locator(locator)\nax.xaxis.set_major_formatter(formatter)\n\n\n\n\n\nAs a rule, we will used a centered moving average (center=True), unless stated otherwise. Also, only use min_periods if you know what you are doing.\n\n\n11.4.2 gaussian\nYou can easily change the kernel shape by using the win_type argument. See how to perform a rolling mean with a gaussian kernel:\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere\n\nwindow_width is an integer, number of points in your window\nstd_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\n\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nwindow_width = 50  # in points = 500 min\nstd = 6  # in points = 60 min\nfig, ax = plt.subplots(figsize=(8,5))\ng = scipy.signal.gaussian(window_width, std)\nax.plot(g)\nax.set(xlabel=\"window width (points)\",\n       ylabel=\"kernel weights\",\n       title=\"gaussian kernel\");\n\n\n\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package.\n\n\n11.4.3 triangular\nSame idea as gaussian, but simpler, because we don‚Äôt need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/sliding.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/sliding.html#which-window-shape-and-width-to-choose",
    "title": "11¬† sliding window",
    "section": "11.5 which window shape and width to choose?",
    "text": "11.5 which window shape and width to choose?\nü§∑‚Äç‚ôÇÔ∏è\nSorry, there is not definite answer here‚Ä¶ It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above.\n\nOne important question you need to ask is: what are the time scales associated with the processes I‚Äôm interested in? For example, if I‚Äôm interested in the daily temperature pattern, getting rid of 1-minute-long fluctuations would probably be ok. On the other hand, if we were to smooth the signal so much that all that can be seen are the temperature changes between summer and winter, then my smoothing got out of hand, and I threw away the very process I wanted to study.\nAll this is to say that you need to know in advance a few things about the system you are studying, otherwise you can‚Äôt know what is ‚Äúnoise‚Äù that can be smoothed away."
  },
  {
    "objectID": "smoothing/not-only-averages.html#confidence-interval",
    "href": "smoothing/not-only-averages.html#confidence-interval",
    "title": "12¬† not only averages",
    "section": "12.1 Confidence Interval",
    "text": "12.1 Confidence Interval\nWe can calculate anything we want inside the sliding window. One good example is the Confidence Interval of the Mean, given by:\n\nCI(\\alpha) = Z(\\alpha) \\cdot SE.\n\n\n\nThis is called ‚Äú◊®◊ï◊ï◊ó ◊ë◊®-◊°÷∂◊û÷∂◊ö‚Äù in hebrew.\n\nZ(\\alpha)= Z-score.\nSE = standard error.\n\nZ(\\alpha) is the Z-score corresponding to the chosen confidence level \\alpha. The most commonly used confidence level is 95%, which corresponds to a Z-score of 1.96. What does this mean? This means that we expect to find 95% of the points within \\pm 1.96 standard deviations away from the mean.\n\n\n\nSource: Dhaval Raval‚Äôs Medium article\nYou can find the Z-score using the following python code:\n\nfrom scipy.stats import norm\n\nconfidence_level = 0.95\n# 5% outside\nout = 1 - confidence_level\n# 0.975 of points to the left of right boundary\np = 1 - out/2\n# inverse of cdf: 0.975 of the points will be smaller than what distance (in sigma units)?\nz_score = norm.ppf(p)\nprint(f\"z-score = {z_score}\")\n\nz-score = 1.959963984540054\n\n\nIf you are still not convinced why we need 0.975 instead of 0.95, read this excellent response on stackoverflow.\nSE is the standard error:\n\nSE = \\frac{\\sigma}{ \\sqrt{N} }.\n\n\n\n\n\\sigma= standard deviation.\nN= number of points.\n\nWe can write a function to calculate the confidence interval of the mean, and use it with the sliding window:\n\ndef std_error_of_the_mean(window):\n    return window.std() / np.sqrt(window.count())\n\ndef confidence_interval(window):\n    return z_score * std_error_of_the_mean(window)\n\ndf['std_error'] = (\n                   df['temp'].rolling('3H',\n                                      center=True)\n                             .apply(std_error_of_the_mean)\n                  )\ndf['confidence_int'] = (\n                        df['temp'].rolling('3H',\n                                           center=True)\n                                  .apply(confidence_interval)\n                       )\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(8,5))\n\nplot_std = ax.fill_between(df.index,\n                            df['mean'] + df['confidence_int'],\n                            df['mean'] - df['confidence_int'],\n                            color=\"xkcd:pink\", alpha=0.5)\nplot_data, = ax.plot(df['temp'], color='black', alpha=0.3)\nplot_mean, =ax.plot(df['mean'], color='xkcd:hot pink')\n\nax.legend([plot_data, plot_mean, plot_std],\n          ['data', '3-hour running average', r\"95% confidence interval\"],\n          frameon=False)\n\n# applying the settings to the ax object\nax.set(**plot_settings)\ncentered_dates(ax)\n# fig.savefig(\"YF-temperature_2022_jan.png\", dpi=300)\n\n\n\n\n\nWhen the time series has a regular sampling frequency, all positions of the running window will have the same number of data points in them. Because the Confidence Interval is proportional to the Standard Error, and the SE is proportional to the Standard Deviation (\\sqrt{N} is constant), then the envelope created by the CI is identical to the envelope created by the standard deviation, up to a multiplying constant. Nice.\n\n\nCI and std\nfig, ax = plt.subplots(figsize=(8,5))\nplot_ci, = ax.plot(df['confidence_int'], color='tab:red')\nplot_std, = ax.plot(df['std'], color=\"black\")\nax.legend([plot_ci, plot_std],\n          ['confidence interval', 'standard deviation'],\n          frameon=False)\n\n# applying the settings to the ax object\n# ax.set(**plot_settings)\nax.set(xlim=[df.index[0], df.index[-1]])\ncentered_dates(ax)"
  },
  {
    "objectID": "smoothing/fit.html#linear-fit",
    "href": "smoothing/fit.html#linear-fit",
    "title": "13¬† fit",
    "section": "13.1 linear fit",
    "text": "13.1 linear fit\nThe following is a very short introduction to curve fitting. The natural place to start is with a linear fit.\n\n\nlinear fit\n# the \"fit\" process can't deal with datetimes\n# we therefore make a new column 'minutes', that will be used here\ndf_fit['minutes'] = (df_fit.index - df_fit.index[0]).total_seconds() / 60\n# linear Fit (degree 1)\ndegree = 1\ncoeffs = np.polyfit(df_fit['minutes'], df_fit['T_in'], degree)\n# linear Function\nlinear_function = np.poly1d(coeffs)\n\n\n\n\nsee result of linear fit\nfig, ax = plt.subplots(figsize=(8,5))\n\nax.scatter(df_fit['minutes'], df_fit['T_in'],\n           color='tab:green', label='data')\nax.plot(df_fit['minutes'], linear_function(df_fit['minutes']),\n        color='black', label='linear fit')\n\nax.set(xlabel='minutes',\n       ylabel='temperature (¬∞C)',\n       title=\"temperature inside the greenhouse\")\n\nax.legend(frameon=False)\nprint(f\"starting at {coeffs[1]:.2f} degrees,\\nthe temperature decreases by {-coeffs[0]:.2f} degrees every minute.\")\n\n\nstarting at 19.80 degrees,\nthe temperature decreases by 0.05 degrees every minute.\n\n\n\n\n\nThe line above is the ‚Äúbest‚Äù straight line that describes our data. Defining the residual as the difference between our data and our model (straight line),\n\ne = T_{\\text{data}} - T_{\\text{model}},\n\nthe straight line above is the one that minimizes the sum of the squares of residuals. For this reason, the method used above to fit a curve to the data is called ‚Äúleast-squares method‚Äù.\n\n\nit minimizes the sum\n\nS = \\sum_i e_i^2\n\nCan we do better than a straight line?"
  },
  {
    "objectID": "smoothing/fit.html#polynomial-fit",
    "href": "smoothing/fit.html#polynomial-fit",
    "title": "13¬† fit",
    "section": "13.2 polynomial fit",
    "text": "13.2 polynomial fit\n\n\npolynomial fit\n# polynomial fit (degree 2)\ndegree = 2\ncoeffs2 = np.polyfit(df_fit['minutes'], df_fit['T_in'], degree)\nquad_function = np.poly1d(coeffs2)\n\n# polynomial fit (degree 2)\ndegree = 3\ncoeffs3 = np.polyfit(df_fit['minutes'], df_fit['T_in'], degree)\ncubic_function = np.poly1d(coeffs3)\n\n\n\n\nsee result of polynomial fit\nfig, ax = plt.subplots(figsize=(8,5))\n\nax.scatter(df_fit['minutes'], df_fit['T_in'],\n           color='tab:green', label='data')\nax.plot(df_fit['minutes'], quad_function(df_fit['minutes']),\n        color='black', label='order = 2')\nax.plot(df_fit['minutes'], cubic_function(df_fit['minutes']),\n        color='tab:olive', label='order = 3')\n\nax.set(xlabel='minutes',\n       ylabel='temperature (¬∞C)',\n       title=\"temperature inside the greenhouse\")\nax.legend(frameon=False)\n\n\n&lt;matplotlib.legend.Legend at 0x7fd0a0833eb0&gt;"
  },
  {
    "objectID": "smoothing/fit.html#any-function-you-want",
    "href": "smoothing/fit.html#any-function-you-want",
    "title": "13¬† fit",
    "section": "13.3 any function you want",
    "text": "13.3 any function you want\nNow let‚Äôs get back to our original assumption, that the greenhouse cools according to Newton‚Äôs cooling law. We can still use the least-squares method for any function we want!\n\n\ndefine new function\ndef cooling(t, T_env, T0, r):\n    \"\"\"\n    t = time\n    other stuff = parameters to be fitted\n    \"\"\"\n    return T_env + (T0 - T_env)*np.exp(-r*t)\n\n\n\n\nuse scipy‚Äôs curve_fit\nt = df_fit['minutes'].values\ny = df_fit['T_in'].values\n\nT_init = df_fit['T_in'][0]\n\npopt, pcov = curve_fit(f=cooling,             # model function\n                     xdata=t,                 # x data\n                     ydata=y,                 # y data\n                     p0=(2, T_init, 0.5),     # initial guess of the parameters\n                     )\nprint(f\"the optimal parameters are {popt}\")\n\n\nthe optimal parameters are [14.01663586 21.0074623   0.02121802]\n\n\n\n\nsee result of exponential fit\nfig, ax = plt.subplots(sharex=True)\n\nax.scatter(df_fit['minutes'], df_fit['T_in'],\n           color='tab:green', label='data')\nax.plot(t, cooling(t, *popt),\n        color='black', label='exponential fit')\n\nax.set(xlabel='minutes',\n       ylabel='temperature (¬∞C)',\n       title=\"temperature inside the greenhouse\")\n\nax.legend(frameon=False)\n\n\n&lt;matplotlib.legend.Legend at 0x7fd0b0140850&gt;\n\n\n\n\n\nThat looks really good :)\nWe can use curve fitting to retrieve important parameters from our data. Let‚Äôs write a function that executes the fit and returns two of the fitted parameters: T_env and r.\n\n\ndefine function to retrieve parameters\ndef run_fit(data):\n    data['minutes'] = (data.index - data.index[0]).total_seconds() / 60\n    t = data['minutes'].values\n    y = data['T_in'].values\n    T_init = data['T_in'][0]\n    popt, pcov = curve_fit(f=cooling,             # model function\n                        xdata=t,              # x data\n                        ydata=y,              # y data\n                        p0=(2, T_init, 0.5),   # initial guess of the parameters\n                        )\n    return popt[0],popt[2]\n\n\nWe now apply this function to several consecutive evenings, and we keep the results in a new dataframe.\n\n\ndefine function to retrieve parameters\ndf_night = df.between_time('20:01', '22:01', inclusive='left')\n\n# group by day and apply the function\n# this is where the magic happens.\n# if you are not familiar with \"groupby\", this will be hard to understand\nresult_series = df_night.groupby(df_night.index.date).apply(run_fit)\n\n# convert the series to a dataframe\nresult_df = pd.DataFrame(result_series.tolist(), index=result_series.index, columns=['T_env', 'r'])\nresult_df.index = pd.to_datetime(result_df.index)\nresult_df\n\n\n\n\n\n\n\n\n\nT_env\nr\n\n\n\n\n2023-06-25\n13.275540\n0.019354\n\n\n2023-06-26\n13.331949\n0.027034\n\n\n2023-06-27\n13.254827\n0.018753\n\n\n2023-06-28\n13.392919\n0.020449\n\n\n2023-06-29\n14.016636\n0.021218\n\n\n2023-06-30\n13.807517\n0.021749\n\n\n2023-07-01\n14.994207\n0.023504\n\n\n2023-07-02\n14.314220\n0.023705\n\n\n2023-07-03\n14.585848\n0.019438\n\n\n2023-07-04\n14.377220\n0.019504\n\n\n2023-07-05\n14.814939\n0.021202\n\n\n2023-07-06\n14.667792\n0.022264\n\n\n2023-07-07\n15.535115\n0.024421\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(3,1,sharex=True, figsize=(8,8))\n\nax[0].plot(df['T_in'], c='tab:blue', label='inside')\nax[0].plot(df['T_out'], c='tab:orange', label='outside')\nax[0].set(ylabel='temperature (¬∞C)',\n          title=\"actual temperatures\",\n          ylim=[10,45])\n\n# formating dates on x axis\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\nax[0].xaxis.set_major_locator(locator)\nax[0].xaxis.set_major_formatter(formatter)\n\nax[0].legend(ncol=2, loc='upper center', frameon=False)\n\nax[1].plot(result_df['r'], color='black')\nax[1].set(ylabel=r\"parameter $r$\",\n          ylim=[0, 0.04])\n\nax[2].plot(result_df['T_env'], color='black')\nax2b = ax[2].twinx()\nax2b.plot(df_night['T_out'].resample('D').mean(), color='tab:orange')\nax[2].set(ylim=[12, 17])\nax2b.set(ylim=[19, 24],\n        ylabel='outside temperature')\n# color the xticks\nfor tick in ax[2].get_yticklabels():\n    tick.set_color('tab:orange')\n# color the xlabel\nax[2].set_ylabel(r'\"outside\" temp.'+'\\ninferred from\\nanalysis', color='tab:orange')\n\n\nText(0, 0.5, '\"outside\" temp.\\ninferred from\\nanalysis')\n\n\n\n\n\nConclusions:\n\nThe cooling coefficient r seems quite stable throughout the two weeks of measurements. This probably says that the greenhouse and AC properties did not change much. For instance, the greenhouse thermal insulation stayed constant, and the AC power output stayed constant.\nThe AC tracks very well the outside temperature! This is to say: the AC works better (more easily) when temperatures outsides are low, and vice-versa."
  },
  {
    "objectID": "smoothing/savgol.html",
    "href": "smoothing/savgol.html",
    "title": "14¬† Savitzky‚ÄìGolay",
    "section": "",
    "text": "The Savitzky-Golay filter, also known as LOESS, smoothes a noisy signal by performing a polynomial fit over a sliding window.\nPolynomial fit of order 3, window size = 51 pts\n\n\n\n\nPolynomial fit of order 2, window size = 51 pts\n\n\n\n\nThe simulations look different because the order of the polynomial makes a very different impression on us, but in reality the outcome of the two filtering is almost identical:\n\n\nimport stuff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport datetime as dt\nimport matplotlib.ticker as ticker\nfrom scipy.signal import savgol_filter\nimport os\nimport warnings\nimport scipy\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n# %matplotlib widget\n\n\n\n\ndefine useful functions\n# dirty trick to have dates in the middle of the 24-hour period\n# make minor ticks in the middle, put the labels there!\n# from https://matplotlib.org/stable/gallery/ticks/centered_ticklabels.html\n\ndef centered_dates(ax):\n    date_form = DateFormatter(\"%d %b\")  # %d 3-letter-Month\n    # major ticks at midnight, every day\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n    ax.xaxis.set_major_formatter(date_form)\n    # minor ticks at noon, every day\n    ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=[12]))\n    # erase major tick labels\n    ax.xaxis.set_major_formatter(ticker.NullFormatter())\n    # set minor tick labels as define above\n    ax.xaxis.set_minor_formatter(date_form)\n    # completely erase minor ticks, center tick labels\n    for tick in ax.xaxis.get_minor_ticks():\n        tick.tick1line.set_markersize(0)\n        tick.tick2line.set_markersize(0)\n        tick.label1.set_horizontalalignment('center')\n\n\n\n\nload data\ndf = pd.read_csv('shani_2022_january.csv', parse_dates=['date'], index_col='date')\nstart = \"2022-01-02\"\nend = \"2022-01-05\"\ndf = df.loc[start:end]\n\n\n\ndf['sg_3_51'] = savgol_filter(df['TD'], window_length=51, polyorder=3)\ndf['sg_2_51'] = savgol_filter(df['TD'], window_length=51, polyorder=2)\n\n\n\nplot temperature data\nfig, ax = plt.subplots(figsize=(8,5))\n\nplot_data, = ax.plot(df['TD'], color='black')\nplot_sg2, = ax.plot(df['sg_2_51'], color='xkcd:hot pink')\nplot_sg3, = ax.plot(df['sg_3_51'], color='xkcd:mustard')\n\nax.legend(handles=[plot_data, plot_sg2, plot_sg3],\n          labels=['data', 'sg order 2', 'sg order 3'],\n          frameon=False)\n\nplot_settings = {\n    'ylim': [5, 17.5],\n    'xlim': [df.index[0], df.index[-1]],\n    'ylabel': \"Temperature (¬∞C)\",\n    'title': \"Yatir Forest, 2022\",\n    'yticks': [5, 10, 15]\n}\n\nax.set(**plot_settings)\ncentered_dates(ax)\n\n\n\n\n\nTo really see the difference between window width and polynomial order, we need to play with their ratio,\n\n\\text{ratio} = \\frac{w}{p} = \\frac{\\text{window width}}{\\text{polynomial order}}\n\n\n\nchose only one day\nstart = \"2022-01-02 00:00:00\"\nend = \"2022-01-02 23:50:00\"\ndf = df.loc[start:end]\n\n\n\n# window_length, polyorder\ndf['sg_1'] = savgol_filter(df['TD'], 5, 3)\ndf['sg_2'] = savgol_filter(df['TD'], 11, 2)\ndf['sg_3'] = savgol_filter(df['TD'], 25, 3)\n\n\n\ncompare different ratio choices\nfig, ax = plt.subplots(figsize=(8,5))\n\nplot_data, = ax.plot(df['TD'], color='black')\nplot_sg1, = ax.plot(df['sg_1'], color='xkcd:hot pink')\nplot_sg2, = ax.plot(df['sg_2'], color='xkcd:mustard')\nplot_sg3, = ax.plot(df['sg_3'], color='xkcd:royal blue')\n\nax.legend(handles=[plot_data, plot_sg1, plot_sg2, plot_sg3],\n          labels=['data', r'$w/p=1.5$', r'$w/p=5.5$', r'$w/p=8.3$'],\n          frameon=False)\n\nplot_settings = {\n    'ylim': [5, 17.5],\n    'xlim': [df.index[0], df.index[-1]],\n    'ylabel': \"Temperature (¬∞C)\",\n    'title': \"Yatir Forest, 2022\",\n    'yticks': [5, 10, 15]\n}\n\nax.set(**plot_settings)\n\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\n\nax.xaxis.set_major_locator(locator)\nax.xaxis.set_major_formatter(formatter)\n\n\n\n\n\nThe higher the ratio, the more aggressive the smoothing.\nThere is a lot more about the Savitzky-Golay filter, but for our purposes this is enough. If you want some more discussion about how to choose the parameters of the filter, read this."
  },
  {
    "objectID": "outliers/motivation.html",
    "href": "outliers/motivation.html",
    "title": "15¬† motivation",
    "section": "",
    "text": "Outliers are observations significantly different from all other observations. Consider, for example, this temperature graph:\n\nWhile most measured points are between 20 and 30 ¬∞C, there is obviously something very wrong with the one data point above 80 ¬∞C.\nHow could such a thing come about? This could be the result of non-natural causes, such as measurement errors, wrong data collection, or wrong data entry. On the other hand, this point could have natural sources, such as a very hot spark flying next to the temperature sensor.\nIdentifying outliers is important, because they might greatly impact measures like mean and standard deviation. When left untouched, outliers might make us reach wrong conclusions about our data. See what happens to the slope of this linear regression with and without the outliers.\n\n\n\nSource: Zhang (2020)\n\n\n\n\n\n\n\nZhang, Ou. 2020. ‚ÄúOutliers-Part 3:outliers in Regression.‚Äù ouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "outliers/outlier-identification.html#visual-inspection",
    "href": "outliers/outlier-identification.html#visual-inspection",
    "title": "16¬† outlier identification",
    "section": "16.1 visual inspection",
    "text": "16.1 visual inspection\nI produced a stationary signal and added to it a few ouliers. Can you tell where just by looking at the graph? \nThe easiest way of identifying the outliers is:\n\nFirst plot the time series.\nChoose upper and lower boundaries. Whatever falls outside these boundaries is an outlier.\n\nEasy.\n\nIf all you have is this one time series, you‚Äôre done, congratulations. However, it is often the case that one has very long time series, or a great number of time series to analyze. In this case it is impractical to use the visual inspection method. We would like to devise an algorithm to automate this task."
  },
  {
    "objectID": "outliers/outlier-identification.html#z-score",
    "href": "outliers/outlier-identification.html#z-score",
    "title": "16¬† outlier identification",
    "section": "16.2 Z-score",
    "text": "16.2 Z-score\nThe Z-score is the distance, in units of 1 standard deviation, of a point in the series with respect to the mean:\n\nz  = \\frac{x-\\mu}{\\sigma},\n\n\n\nwhere\n\nx= data point,\n\n\\mu= time series mean\n\n\\sigma= time series standard deviation.\n\nA common choice is to consider an outlier a point whose Z-score is greater that 3, in absolute value. In other words: If a point is more than 3 standard deviations away form the mean, then we call it an outlier.\n\nYou can now use this algorithm to any number of time series, let the computer do the hard work.\nOf course, there is nothing sacred about the number 3. You can choose any Z-score you want to perform an analysis on your own data, depending on your needs."
  },
  {
    "objectID": "outliers/outlier-identification.html#iqr",
    "href": "outliers/outlier-identification.html#iqr",
    "title": "16¬† outlier identification",
    "section": "16.3 IQR",
    "text": "16.3 IQR\nAnother super common criterion for identifying outliers is the IQR, or InterQuartile Range.\nTake a look at the statistics below of the time series we have been working with so far. The IQR is the distance between the first quartile (Q1) and the third quartile (Q3), where exactly 50% of the data is.\nThe algorithm here is to determine two thresholds, whose distance is 1.5 times the IQR from Q1 and Q3. Whatever falls outside these two thresholds is an outlier.\n\nWe are used to see this in box plots:\n\n\n\nSource: McDonald (2022)\nAgain, the distance 1.5 is not sacred, it‚Äôs only the most common. You might want to choose other values depending on your needs. Let‚Äôs now apply the IQR method to our time series.\n\nIt works pretty well! Notice that now we have an additional outlier (a bit before 06:00). What do we do with that?"
  },
  {
    "objectID": "outliers/outlier-identification.html#non-stationary-time-series",
    "href": "outliers/outlier-identification.html#non-stationary-time-series",
    "title": "16¬† outlier identification",
    "section": "16.4 non-stationary time series",
    "text": "16.4 non-stationary time series\nI have produced a new time series, one that on average goes up with time. Can you point in the graph where are the outliers?\n\nNow, see what happens when we apply the previous two methods to this time series.\nZ-score\n\nIQR\n\nWhat happened? Do you have ideas how to solve this?"
  },
  {
    "objectID": "outliers/outlier-identification.html#sources",
    "href": "outliers/outlier-identification.html#sources",
    "title": "16¬† outlier identification",
    "section": "16.5 Sources",
    "text": "16.5 Sources\n\n\n\n\nMcDonald, Andy. 2022. ‚ÄúCreating Boxplots with the Seaborn Python Library.‚Äù Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57."
  },
  {
    "objectID": "outliers/robust-analysis.html#mad",
    "href": "outliers/robust-analysis.html#mad",
    "title": "17¬† robust analysis",
    "section": "17.1 MAD",
    "text": "17.1 MAD\nAnother rubust method is MAD, the Median Absolute Deviation, given by\n\n\\text{MAD} = \\text{median}(\\left| x_i - \\text{median}(x)  \\right|),\n\nwhere |\\cdot| is the absolute value.\nApplying MAD to the stationary time series from before, yields\n\nHere, the threshold is the median \\pm3k\\cdot MAD, where the value k=1.4826 scales MAD so that when the data is gaussianly distributed, 3k equals 1 standard deviation."
  },
  {
    "objectID": "outliers/sliding-algorithms.html#sliding-z-score",
    "href": "outliers/sliding-algorithms.html#sliding-z-score",
    "title": "18¬† sliding algorithms",
    "section": "18.1 Sliding Z-score",
    "text": "18.1 Sliding Z-score\n\nNow the Z-score seems to give really nice results (but not perfect). Maybe playing with the window width and Z-score threshold would give better results?\nIn any case, we clearly see why the Z-score is not a robust algorithm. See how the standard deviation is sensitive to outliers?"
  },
  {
    "objectID": "outliers/sliding-algorithms.html#sliding-iqr",
    "href": "outliers/sliding-algorithms.html#sliding-iqr",
    "title": "18¬† sliding algorithms",
    "section": "18.2 Sliding IQR",
    "text": "18.2 Sliding IQR\nLet‚Äôs see how well the sliding IQR method fares.\n\nIt identified all the outliers, but also found that a few other points should be considered outliers? What do you think of that?\nSee that the threshold does not jump abruptly when the sliding window includes an outlier. In fact, the threshold doesn‚Äôt even care! This is what it means to be robust.\nHowever, we do see large fluctuations in the threshold. When does this happen? Why?"
  },
  {
    "objectID": "outliers/sliding-algorithms.html#sliding-mad",
    "href": "outliers/sliding-algorithms.html#sliding-mad",
    "title": "18¬† sliding algorithms",
    "section": "18.3 Sliding MAD",
    "text": "18.3 Sliding MAD\nNow it‚Äôs MAD‚Äôs time to shine.\n\nCompare this result to the previous two. Which yields best results?\nMAD is robust to outliers, and again we see that the threshold envelope widens when there is a rising or falling trend in the data."
  },
  {
    "objectID": "outliers/sliding-algorithms.html#challenges",
    "href": "outliers/sliding-algorithms.html#challenges",
    "title": "18¬† sliding algorithms",
    "section": "18.4 Challenges",
    "text": "18.4 Challenges\nNow it‚Äôs your turn to work, I‚Äôm tired! Write algorithms for the following outlier identification methods:\n\nvisual inspection\nZ-score\nIQR\nMAD\n\nExcluding the visual inspection method, write first an algorithm that operates on a full time series, and then write a new version that can work with sliding windows."
  },
  {
    "objectID": "outliers/substituting-outliers.html#do-nothing",
    "href": "outliers/substituting-outliers.html#do-nothing",
    "title": "19¬† substituting outliers",
    "section": "19.1 Do nothing",
    "text": "19.1 Do nothing\nAssuming the outlier indeed happened in real life, and is not the result of faulty data transmission or bad data recording, then excluding an outlier might be the last thing you want to do. Sometimes extreme events do happen, such as a one-in-a-hundred-year storm, and they have a disproportionate weight on the system you are studying. The outliers might actually be the most interesting points in your data for all you know!\nIn case the outliers are not of interest to you, if you are using robust methods to analyze your data, you don‚Äôt necessarily need to do anything either. For instance, let‚Äôs say that you want to smooth your time series. If instead of taking the mean inside a sliding window you choose to calculate the median, then outliers shouldn‚Äôt be a problem. Test it and see if it‚Äôs true. Go on.\nFor many things you need to do (not only smoothing), you might be able to find robust methods. What do you do if you have to use a non-robust method? Well, then you can substitute the outlier for two things: NaN or imputated values."
  },
  {
    "objectID": "outliers/substituting-outliers.html#nan",
    "href": "outliers/substituting-outliers.html#nan",
    "title": "19¬† substituting outliers",
    "section": "19.2 NaN",
    "text": "19.2 NaN\nSubstitute outliers for NaN.\nNaN means ‚ÄúNot a Number‚Äù, and is what you get when you try to perform a mathematical operation like 0/0. It is common to see NaN in dataset rows when data was not collected for some reason.\nThis might seem like a neutral solution, but it actually can generate problems down the line. See this example:\n\n\nimport stuff\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\nfrom scipy.signal import savgol_filter\n\n\n\n# example using numpy\nseries = np.array([2, 4, 5, np.nan, 8, 15])\nmean = np.mean(series)\nprint(f\"the series average is {mean}\")\n\nthe series average is nan\n\n\nA single NaN in your time series ruins the whole calculation! There is a workaround though:\n\nmean = np.nanmean(series)\nprint(f\"the series average is {mean}\")\n\nthe series average is 6.8\n\n\nYou have to make sure what is the behavior of each function you use with respect to NaNs, and if possible, use a suitable substitute.\nThe same example in pandas would not fail:\n\ndate_range = pd.date_range(start='2024-01-01', periods=len(series), freq='1D')\ndf = pd.DataFrame({'series': series}, index=date_range)\nmean = df['series'].mean()\nprint(f\"the series average is {mean}\")\n\nthe series average is 6.8"
  },
  {
    "objectID": "outliers/substituting-outliers.html#imputate-values",
    "href": "outliers/substituting-outliers.html#imputate-values",
    "title": "19¬† substituting outliers",
    "section": "19.3 imputate values",
    "text": "19.3 imputate values\nTo ‚Äúimputate values‚Äù means to fill in the missing value with a guess, an estimation of what this data point ‚Äúshould have been‚Äù if it were measured in the first place. Why should we bother to do so? Because many tools that we know and love don‚Äôt do well with missing values.\nWe learned about the Savitzky-Golay filter for smoothing data. See what happens when there is a single NaN in the series:\n\n\ncreate time series\nsteps = np.random.randint(low=-2, high=2, size=100)\ndata = steps.cumsum()\ndate_range = pd.date_range(start='2023-01-01', periods=len(data), freq='1D')\ndf = pd.DataFrame({'series': data}, index=date_range)\ndf.loc['2023-02-05', 'series'] = np.nan\n\n\n\n\nsmooth it and then plot\ndf['sg'] = savgol_filter(df['series'], window_length=15, polyorder=2)\n\ndef concise(ax):\n    locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n    formatter = mdates.ConciseDateFormatter(locator)\n    ax.xaxis.set_major_locator(locator)\n    ax.xaxis.set_major_formatter(formatter)\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(df['series'], color=\"tab:blue\", label=\"series with 1 NaN\")\nax.plot(df['sg'], color=\"tab:orange\", label=\"SavGol filter has many more NaNs\")\nconcise(ax)\nax.legend(frameon=False);\n\n\n\n\n\nWe will deal with this topic in the next chapter, ‚Äúinterpolation‚Äù. There, we will learn a few methods to fill in missing data, and basic NaN operations you should be acquainted with."
  },
  {
    "objectID": "outliers/interpolation.html",
    "href": "outliers/interpolation.html",
    "title": "20¬† interpolation",
    "section": "",
    "text": "Interpolation is the act of getting data you don‚Äôt have from data you alreay have."
  },
  {
    "objectID": "best-practices/date-formatting.html",
    "href": "best-practices/date-formatting.html",
    "title": "22¬† date formatting",
    "section": "",
    "text": "Here you will find several examples of how to format dates in your plots. Not many explanations are provided.\nHow to use this page? Find first an example of a plot you like, only then go to the code and see how it‚Äôs done.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\n\n\nimport pandas as pd\n\nstart_date = '2018-01-01'\nend_date = '2018-04-30'\n\n# create date range with 1-hour intervals\ndates = pd.date_range(start_date, end_date, freq='1H')\n# create a random variable to plot\nvar = np.random.rand(len(dates)) - 0.51\nvar = var.cumsum()\nvar = var - var.min()\n# create dataframe, make \"date\" the index\ndf = pd.DataFrame({'date': dates, 'variable': var})\ndf.set_index(df['date'], inplace=True)\ndf\n\n\n\n\n\n\n\n\ndate\nvariable\n\n\ndate\n\n\n\n\n\n\n2018-01-01 00:00:00\n2018-01-01 00:00:00\n28.317035\n\n\n2018-01-01 01:00:00\n2018-01-01 01:00:00\n28.120523\n\n\n2018-01-01 02:00:00\n2018-01-01 02:00:00\n28.596894\n\n\n2018-01-01 03:00:00\n2018-01-01 03:00:00\n28.931941\n\n\n2018-01-01 04:00:00\n2018-01-01 04:00:00\n28.561778\n\n\n...\n...\n...\n\n\n2018-04-29 20:00:00\n2018-04-29 20:00:00\n1.914343\n\n\n2018-04-29 21:00:00\n2018-04-29 21:00:00\n1.648757\n\n\n2018-04-29 22:00:00\n2018-04-29 22:00:00\n1.992956\n\n\n2018-04-29 23:00:00\n2018-04-29 23:00:00\n1.500860\n\n\n2018-04-30 00:00:00\n2018-04-30 00:00:00\n1.650439\n\n\n\n\n2857 rows √ó 2 columns\n\n\n\ndefine a useful function to plot the graphs below\n\ndef explanation(ax, text, letter):\n    ax.text(0.99, 0.97, text,\n            transform=ax.transAxes,\n            horizontalalignment='right', verticalalignment='top',\n            fontweight=\"bold\")\n    ax.text(0.01, 0.01, letter,\n            transform=ax.transAxes,\n            horizontalalignment='left', verticalalignment='bottom',\n            fontweight=\"bold\")\n    ax.set(ylabel=\"variable (units)\")\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nax.plot(df['variable'])\nplt.gcf().autofmt_xdate()  # makes slated dates\nexplanation(ax, \"slanted dates\", \"\")\nfig.savefig(\"dates1.png\")\n\n\n\n\n\nfig, ax = plt.subplots(4, 1, figsize=(10, 16),\n                       gridspec_kw={'hspace': 0.3})\n\n### plot a ###\nax[0].plot(df['variable'])\ndate_form = DateFormatter(\"%b\")\nax[0].xaxis.set_major_locator(mdates.MonthLocator(interval=2))\nax[0].xaxis.set_major_formatter(date_form)\n\n### plot b ###\nax[1].plot(df['variable'])\ndate_form = DateFormatter(\"%B\")\nax[1].xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax[1].xaxis.set_major_formatter(date_form)\n\n### plot c ###\nax[2].plot(df['variable'])\nax[2].xaxis.set_major_locator(mdates.MonthLocator())\n# 16 is a slight approximation for the center, since months differ in number of days.\nax[2].xaxis.set_minor_locator(mdates.MonthLocator(bymonthday=16))\nax[2].xaxis.set_major_formatter(ticker.NullFormatter())\nax[2].xaxis.set_minor_formatter(DateFormatter('%B'))\nfor tick in ax[2].xaxis.get_minor_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n\n### plot d ###\nax[3].plot(df['variable'])\ndate_form = DateFormatter(\"%d %b\")\nax[3].xaxis.set_major_locator(mdates.DayLocator(interval=15))\nax[3].xaxis.set_major_formatter(date_form)\n\nexplanation(ax[0], \"month abbreviations, every 2 months\", \"a\")\nexplanation(ax[1], \"full month names\", \"b\")\nexplanation(ax[2], \"full month names centered between the 1st of the month\", \"c\")\nexplanation(ax[3], \"day + month abbr. --- every 15 days\", \"d\")\n\nfig.savefig(\"dates2.png\")\n\n\n\n\n\nfig, ax = plt.subplots(4, 1, figsize=(10, 16),\n                       gridspec_kw={'hspace': 0.3})\n\n### plot e ###\nax[0].plot(df['variable'])\ndate_form = DateFormatter(\"%d/%m\")\nax[0].xaxis.set_major_locator(mdates.DayLocator(bymonthday=[5, 20]))\nax[0].xaxis.set_major_formatter(date_form)\n\n### plot f ###\nax[1].plot(df['variable'])\nlocator = mdates.AutoDateLocator(minticks=11, maxticks=17)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\n\n### plot g ###\nax[2].plot(df.loc['2018-01-01':'2018-03-01', 'variable'])\nlocator = mdates.AutoDateLocator(minticks=6, maxticks=14)\nformatter = mdates.ConciseDateFormatter(locator)\nax[2].xaxis.set_major_locator(locator)\nax[2].xaxis.set_major_formatter(formatter)\n\n### plot h ###\nax[3].plot(df.loc['2018-01-01':'2018-01-02', 'variable'])\nlocator = mdates.AutoDateLocator(minticks=6, maxticks=10)\nformatter = mdates.ConciseDateFormatter(locator)\nax[3].xaxis.set_major_locator(locator)\nax[3].xaxis.set_major_formatter(formatter)\n\nexplanation(ax[0], \"exactly on days 05 and 20 of each month\", \"e\")\nexplanation(ax[1], \"ConciseDateFormatter\", \"f\")\nexplanation(ax[2], \"ConciseDateFormatter\", \"g\")\nexplanation(ax[3], \"ConciseDateFormatter\", \"h\")\n\nfig.savefig(\"dates3.png\")\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4),\n                       gridspec_kw={'hspace': 0.3})\n\n# import constants for the days of the week\nfrom matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\nax.plot(df['variable'])\n# tick on sundays every third week\nloc = mdates.WeekdayLocator(byweekday=SU, interval=3)\nax.xaxis.set_major_locator(loc)\ndate_form = DateFormatter(\"%a, %b %d\")\nax.xaxis.set_major_formatter(date_form)\nfig.autofmt_xdate(bottom=0.2, rotation=30, ha='right')\nexplanation(ax, \"every 3 Sundays, rotate labels\", \"\")\n\n\n\n\n\n\n\nCode\nExplanation\n\n\n\n\n%Y\n4-digit year (e.g., 2022)\n\n\n%y\n2-digit year (e.g., 22)\n\n\n%m\n2-digit month (e.g., 12)\n\n\n%B\nFull month name (e.g., December)\n\n\n%b\nAbbreviated month name (e.g., Dec)\n\n\n%d\n2-digit day of the month (e.g., 09)\n\n\n%A\nFull weekday name (e.g., Tuesday)\n\n\n%a\nAbbreviated weekday name (e.g., Tue)\n\n\n%H\n24-hour clock hour (e.g., 23)\n\n\n%I\n12-hour clock hour (e.g., 11)\n\n\n%M\n2-digit minute (e.g., 59)\n\n\n%S\n2-digit second (e.g., 59)\n\n\n%p\n‚ÄúAM‚Äù or ‚ÄúPM‚Äù\n\n\n%Z\nTime zone name\n\n\n%z\nTime zone offset from UTC (e.g., -0500)"
  },
  {
    "objectID": "stationarity/autocorrelation.html#question",
    "href": "stationarity/autocorrelation.html#question",
    "title": "25¬† autocorrelation",
    "section": "25.1 question",
    "text": "25.1 question\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nTo answer this, we need to talk about autocorrelation. Let‚Äôs start by introducing the necessary concepts."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "25¬† autocorrelation",
    "section": "25.2 mean and standard deviation",
    "text": "25.2 mean and standard deviation\nLet‚Äôs call our time series from above X, and its length N. Then:\n\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\nThe mean and standard deviation can be visualized thus:\n\nOne last basic concept we need is the expected value: \nE[X] = \\sum_{i=1}^N X_i p_i\n\nFor our time series, the probability p_i that a given point X_i is in the dataset is simply 1/N, therefore the expectation becomes\n\nE[X] = \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N}"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "25¬† autocorrelation",
    "section": "25.3 autocorrelation",
    "text": "25.3 autocorrelation\nThe autocorrelation of a time series X is the answer to the following question:\n\nif we shift X by \\tau units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are X(t) and X(t+\\tau)?\n\nUsing the Pearson correlation coefficient\n\n\nPearson correlation coefficient between X and Y: \n\\rho_{X,Y} = \\frac{E\\left[ (X - \\mu_X)(X_Y - \\mu_Y) \\right]}{\\sigma_X\\sigma_Y}\n\nwe get\n\n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\nA video is worth a billion words, so let‚Äôs see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\tau=0 (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "27¬† cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "29¬† LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "frequency/fourier.html#basic-wave-concepts",
    "href": "frequency/fourier.html#basic-wave-concepts",
    "title": "31¬† Fourier transform",
    "section": "31.1 basic wave concepts",
    "text": "31.1 basic wave concepts\nThe function\n\nf(t) = B\\sin(2\\pi f t)\n\\tag{31.1}\nhas two basic characteristics, its amplitude B and frequency f.\n\nIn the figure above, the amplitude B=0.6 and we see that the distance between two peaks is called period, T=2 s. The frequency is defined as the inverse of the period:\n\nf = \\frac{1}{T}.\n\\tag{31.2}\nWhen time is in seconds, then the frequency is measured in Hertz (Hz). For the graph above, therefore, we see a wave whose frequency is f = 1/(2 \\text{ s}) = 0.5 Hz.\nIn the figure below, we see what happens when we vary the values of the frequency and amplitude.\n\nThe graph above introduces two new characteristics of a wave, its phase \\phi, and its offset B. A more general description of a sine wave is\n\nf(t) = B\\sin(2\\pi f t + \\phi) + B_0.\n\\tag{31.3}\nThe offset B_0 moves the wave up and down, while changing the value of \\phi makes the sine wave move left and right. When the phase \\phi=2\\pi, the sine wave will have shifted a full period, and the resulting wave is identical to the original:\n\nB\\sin(2\\pi f t) = B\\sin(2\\pi f t + 2\\pi).\n\\tag{31.4}\nAll the above can also be said about a cosine, whose general for can be given as\n\nA\\cos(2\\pi f t + \\phi) + A_0\n\\tag{31.5}\nOne final point before we jump into the deep waters is that the sine and cosine functions are related through a simple phase shift:\n\n\\cos\\left(2\\pi f t + \\frac{\\pi}{2}\\right) = \\sin\\left(2\\pi f t\\right)"
  },
  {
    "objectID": "frequency/fourier.html#fouriers-theorem",
    "href": "frequency/fourier.html#fouriers-theorem",
    "title": "31¬† Fourier transform",
    "section": "31.2 Fourier‚Äôs theorem",
    "text": "31.2 Fourier‚Äôs theorem\nFourier‚Äôs theorem states that\n\nAny periodic signal is composed of a superposition of pure sine waves, with suitably chosen amplitudes and phases, whose frequencies are harmonics of the fundamental frequency of the signal.\n\nSee the following animations to visualize the theorem in action.\n\nSource: https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Fourier_synthesis_square_wave_animated.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Sawtooth_Fourier_Animation.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Continuous_Fourier_transform_of_rect_and_sinc_functions.gif"
  },
  {
    "objectID": "frequency/fourier.html#fourier-series",
    "href": "frequency/fourier.html#fourier-series",
    "title": "31¬† Fourier transform",
    "section": "31.3 Fourier series",
    "text": "31.3 Fourier series\n\na periodic function can be described as a sum of sines and cosines.\n\n\n\nNot any function, but certainly most functions we will deal with in this course. The function has to fullful the Dirichlet conditions\nThe classic examples are usually the square function and the sawtooth function:\n\n\n[Source: https://www.geogebra.org/m/tkajbzmg]\nhttps://www.geogebra.org/m/k4eq4fkr\n\n\n\nF[x(t)] = F(f) = \\int_{-\\infty}^{\\infty}x(t)e^{-2\\pi i f t}dt\n\n\nf(t) = \\int_{-\\infty}^{\\infty}F(f)e^{2\\pi i f t}df\n\nhttps://dibsmethodsmeetings.github.io/fourier-transforms/\nhttps://www.jezzamon.com/fourier/index.html"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "href": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "title": "35¬† seasonal decomposition",
    "section": "35.1 trends in atmospheric carbon dioxide",
    "text": "35.1 trends in atmospheric carbon dioxide\nMauna Loa CO2 concentration.\ndata from NOAA\n\nurl = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url, header=47, na_values=[-999.99])\n\n# you can first download, and then read the csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename, header=35, na_values=[-999.99])\n\ndf\n\n\n\n\n\n\n\n\n1974\n5\n19\n1974.3795\n333.37\n5.1\n-999.99\n-999.99.1\n50.39\n\n\n\n\n0\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.05\n\n\n1\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.59\n\n\n2\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.64\n\n\n3\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n4\n1974\n6\n23\n1974.4753\n331.73\n5\nNaN\nNaN\n49.72\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2565\n2023\n7\n23\n2023.5575\n421.28\n4\n418.03\n397.30\n141.60\n\n\n2566\n2023\n7\n30\n2023.5767\n420.83\n6\n418.10\n396.80\n141.69\n\n\n2567\n2023\n8\n6\n2023.5959\n420.02\n6\n417.36\n395.65\n141.41\n\n\n2568\n2023\n8\n13\n2023.6151\n418.98\n4\n417.25\n395.24\n140.89\n\n\n2569\n2023\n8\n20\n2023.6342\n419.31\n2\n416.64\n395.22\n141.71\n\n\n\n\n2570 rows √ó 9 columns\n\n\n\n\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.40\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.06\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.60\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.65\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-26\n2022\n6\n26\n2022.4836\n420.31\n7\n418.14\n395.36\n138.71\n\n\n2022-07-03\n2022\n7\n3\n2022.5027\n419.73\n6\n417.49\n395.15\n138.64\n\n\n2022-07-10\n2022\n7\n10\n2022.5219\n419.08\n6\n417.25\n394.59\n138.52\n\n\n2022-07-17\n2022\n7\n17\n2022.5411\n418.43\n6\n417.14\n394.64\n138.41\n\n\n2022-07-24\n2022\n7\n24\n2022.5603\n417.84\n6\n415.68\n394.11\n138.36\n\n\n\n\n2515 rows √ó 9 columns\n\n\n\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,6))\nax.plot(df['average'])\nax.set(xlabel=\"date\",\n       ylabel=\"CO2 concentration (ppm)\",\n       # ylim=[0, 430],\n       title=\"Mauna Loa CO2 concentration\");\n\nKeyError: 'average'\n\n\n\n\n\nfill missing data. interpolate method: ‚Äòtime‚Äô\ninterpolation methods visualized\n\ndf['co2'] = (df['average'].resample(\"D\") #resample daily\n                          .interpolate(method='time') #interpolate by time\n            )\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndecimal\naverage\nndays\n1 year ago\n10 years ago\nincrease since 1800\nco2\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1974-05-19\n1974\n5\n19\n1974.3795\n333.37\n5\nNaN\nNaN\n50.40\n333.37\n\n\n1974-05-26\n1974\n5\n26\n1974.3986\n332.95\n6\nNaN\nNaN\n50.06\n332.95\n\n\n1974-06-02\n1974\n6\n2\n1974.4178\n332.35\n5\nNaN\nNaN\n49.60\n332.35\n\n\n1974-06-09\n1974\n6\n9\n1974.4370\n332.20\n7\nNaN\nNaN\n49.65\n332.20\n\n\n1974-06-16\n1974\n6\n16\n1974.4562\n332.37\n7\nNaN\nNaN\n50.06\n332.37\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-26\n2022\n6\n26\n2022.4836\n420.31\n7\n418.14\n395.36\n138.71\n420.31\n\n\n2022-07-03\n2022\n7\n3\n2022.5027\n419.73\n6\n417.49\n395.15\n138.64\n419.73\n\n\n2022-07-10\n2022\n7\n10\n2022.5219\n419.08\n6\n417.25\n394.59\n138.52\n419.08\n\n\n2022-07-17\n2022\n7\n17\n2022.5411\n418.43\n6\n417.14\n394.64\n138.41\n418.43\n\n\n2022-07-24\n2022\n7\n24\n2022.5603\n417.84\n6\n415.68\n394.11\n138.36\n417.84\n\n\n\n\n2515 rows √ó 10 columns"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#decompose-data",
    "href": "seasonality/seasonal-decomposition.html#decompose-data",
    "title": "35¬† seasonal decomposition",
    "section": "35.2 decompose data",
    "text": "35.2 decompose data\nseasonal_decompose returns an object with four components:\n\nobserved: Y(t)\ntrend: T(t)\nseasonal: S(t)\nresid: e(t)\n\nAdditive model: \nY(t) = T(t) + S(t) + e(t)\n\nMultiplicative model: \nY(t) = T(t) \\times S(t) \\times e(t)\n\n\n35.2.0.1 Interlude\nlearn how to use zip in a loop\n\nletters = ['a', 'b', 'c', 'd', 'e']\nnumbers = [1, 2, 3, 4, 5]\n# zip let's us iterate over to lists at the same time\nfor l, n in zip(letters, numbers):\n    print(f\"{l} = {n}\")\n\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n\nPlot each component separately.\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\ndecomposed_m = seasonal_decompose(df['co2'], model='multiplicative')\ndecomposed_a = seasonal_decompose(df['co2'], model='additive')\ndecomposed = decomposed_m\npos = (0.5, 0.9)\ncomponents =[\"observed\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(decomposed, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\n\n\n\n\n\n# %matplotlib widget\n\ndecomposed = decomposed_m\n\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df['co2'], color=\"tab:blue\", label=\"observed\")\nax[0].plot(decomposed.trend * decomposed.resid, color=\"tab:orange\", label=\"trend*resid\")\nax[0].plot(decomposed.trend * decomposed.seasonal, color=\"tab:red\", label=\"trend*seasonal\")\nax[0].plot(decomposed.trend, color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"CO$_2$ concentration (ppm)\",\n          title=\"Mauna Loa CO$_2$ concentration\")\nax[0].legend(frameon=False)\n\nstart = \"2000-01-01\"\nend = \"2003-01-01\"\nzoom = slice(start, end)\nax[1].plot(df.loc[zoom, 'co2'], color=\"tab:blue\", label=\"observed\")\nax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=\"tab:orange\", label=\"trend*resid\")\nax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=\"tab:red\", label=\"trend*seasonal\")\nax[1].plot(decomposed.trend[zoom], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"Components, 2000--2003\");"
  },
  {
    "objectID": "rates-of-change/motivation.html",
    "href": "rates-of-change/motivation.html",
    "title": "37¬† motivation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n%matplotlib widget\n\n\nfilename = \"../archive/data/kinneret_cleaned.csv\"\ndf = pd.read_csv(filename)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n\n\n\nlevel\n\n\ndate\n\n\n\n\n\n2023-09-12\n-211.115\n\n\n2023-09-11\n-211.105\n\n\n2023-09-10\n-211.095\n\n\n2023-09-09\n-211.085\n\n\n2023-09-08\n-211.070\n\n\n...\n...\n\n\n1966-11-01\n-210.390\n\n\n1966-10-15\n-210.320\n\n\n1966-10-01\n-210.270\n\n\n1966-09-15\n-210.130\n\n\n1966-09-01\n-210.020\n\n\n\n\n10286 rows √ó 1 columns\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['level'], color=\"tab:blue\")\nax.set(title=\"Kinneret Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThe data seems ok, until we take a closer look. Data points are not evenly spaced in time.\n\nfig, ax = plt.subplots()\nax.plot(df.loc[\"1993\":\"1995\", 'level'], color=\"tab:blue\", marker=\"o\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_3777/934261896.py:2: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n  ax.plot(df.loc[\"1993\":\"1995\", 'level'], color=\"tab:blue\", marker=\"o\")\n\n\n\n\n\nWe can resample by day (a much higher rate than the original), and linearly interpolate:\n\ndf2 = df['level'].resample('D').interpolate('time').to_frame()\ndf2['level_sm'] = df2['level'].rolling('30D', center=True).mean()\ndf3 = df2['level'].resample('W').mean().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df2.loc[\"1993\":\"1995\", 'level_sm'],\n        color=\"tab:red\",\n        label=\"daily resapled\")\nax.plot(df3.loc[\"1993\":\"1995\", 'level'],\n        color=\"black\",\n        label=\"daily resapled\")\nax.plot(df2.loc[\"1993\":\"1995\", 'level'],\n        color=\"tab:orange\",\n        label=\"daily resapled\")\nax.plot(df.loc[\"1993\":\"1995\", 'level'],\n        color=\"tab:blue\",\n        marker=\"o\",\n        linestyle=\"None\",\n        label=\"original\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\nax.legend(frameon=False)\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_3777/2583247388.py:11: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n  ax.plot(df.loc[\"1993\":\"1995\", 'level'],\n\n\n&lt;matplotlib.legend.Legend at 0x7fa71e8b0bb0&gt;\n\n\n\n\n\n\ndf2['naive'] = df2['level'].diff()\ndf2['gradient'] = np.gradient(df2['level'])\n\ndf3['naive'] = df3['level'].diff()\ndf3['gradient'] = np.gradient(df3['level'])\n\n\nfig, ax = plt.subplots()\nax.plot(df3.loc[\"1980\":\"2020\", 'naive'], color=\"tab:blue\")\nax.plot(df3.loc[\"1980\":\"2020\", 'gradient'], color=\"tab:red\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\n\n[Text(0.5, 1.0, 'Dead Sea Level'), Text(0, 0.5, 'level (m)')]\n\n\n\n\n\n\ndf3 = df2[\"level\"].rolling('365.24D', center=True).mean().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df3.loc[\"1980\":\"2020\", 'level'], color=\"tab:blue\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\n\n[Text(0.5, 1.0, 'Dead Sea Level'), Text(0, 0.5, 'level (m)')]"
  },
  {
    "objectID": "rates-of-change/finite-differences.html",
    "href": "rates-of-change/finite-differences.html",
    "title": "39¬† finite differences",
    "section": "",
    "text": "Definition of a derivative:\n\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n\nNumerically, we can approximate the derivative f'(t) of a time series f(t) as\n\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{39.1}\n\n\nThe expression \\mathcal{O}(\\Delta t) means that the error associated with the approximation is proportional to \\Delta t. This is called ‚ÄúBig O notation‚Äù.\nThe expression above is called the two-point forward difference formula. Likewise, we can define the two-point backward difference formula:\n\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{39.2}\nIf we sum together Equation¬†39.1 and Equation¬†39.2 we get:\n\n\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n&= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n\\tag{39.3}\nDividing both sides by 2 gives the two-point central difference formula:\n\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n\\tag{39.4}\nTwo things are worth mentioning about the approximation above:\n\nit is balanced, that is, there is no preference of the future over the past.\nits error is proportional to \\Delta t^2, it is a lot more precise than the unbalanced approximations :)\n\n\n\nTo understand why the error is proportional to \\Delta t^2, one can subtract the Taylor expansion of f(t-\\Delta t) from the Taylor expansion of f(t+\\Delta t). See this, pages 3 and 4.\n\nThe function np.gradient calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n\nThe ‚Äúgradient‚Äù usually refers to a first derivative with respect to space, and it is denoted as \\nabla f(x)=\\frac{df(x)}{dx}. However, it doesn‚Äôt really matter if we call the independent variable x or t, the derivative operator is exactly the same.\nCheck out this nice example."
  },
  {
    "objectID": "rates-of-change/fourier-based-derivatives.html",
    "href": "rates-of-change/fourier-based-derivatives.html",
    "title": "40¬† Fourier-based derivatives",
    "section": "",
    "text": "This tutorial is based on Pelliccia (2019).\nnice trick: https://math.stackexchange.com/questions/430858/fourier-transform-of-derivative\n\n\n\n\nPelliccia, Daniel. 2019. ‚ÄúFourier Spectral Smoothing Method.‚Äù 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  },
  {
    "objectID": "assignments/assignment1.html#task",
    "href": "assignments/assignment1.html#task",
    "title": "44¬† assignment 1",
    "section": "44.1 task",
    "text": "44.1 task\nGo to the IMS website, and choose another weather station we have not worked with yet. Download 10-minute data for a full year, any year.\nMake 3 graphs:\n\nDaily maximum humidity. Bonus: add another line to the graph, the daily minimum humidity.\nThe number of rainy dais for each month\nFor each day of the year, show the number of hours when global solar radiation was above, on average, the threshold 10 W/m^2. Now add another line, for the threshold 500 W/m^2.\n\nMake 5 more graphs (total of 8 graphs) of whatever you find interesting. You have the liberty to explore various facets of your dataset that capture your interest. It‚Äôs essential, however, to maintain a focus on resampling. Each of your plots should effectively showcase and emphasize different aspects or techniques of resampling in your data analysis. To ensure diversity in your visualizations, avoid repetitive themes; for instance, if your first plot illustrates daily wind speed, then your second plot should not simply be a monthly resampling of wind speed. Aim for variety and innovation in each plot to fully explore the potential of resampling in data visualization.\nYou must download this Jupyter Notebook template. Create a zip file with your Jupyter notebook and with the .csv you used. Upload this zip file to the moodle task we created."
  },
  {
    "objectID": "assignments/assignment1.html#guidelines",
    "href": "assignments/assignment1.html#guidelines",
    "title": "44¬† assignment 1",
    "section": "44.2 guidelines",
    "text": "44.2 guidelines\n\nAlways name the axes and add units when relevant.\nAlways give a title to the plot.\nMake sure that all axis tick labels (the numbers/dates on the axes) are readable.\nInclude a legend if you have multiple lines, colors, or groups.\nUse appropriate scales for the axes (linear, logarithmic, etc.) depending on the data‚Äôs nature.\nEnsure that the plot is adequately sized for all elements to be clear and visible.\nChoose colors and markers that are distinguishable, especially for plots with multiple elements.\nIf applicable, include error bars to indicate the variability or uncertainty in the data.\nUse grid lines sparingly; they should not overshadow the data."
  },
  {
    "objectID": "assignments/assignment1.html#evaluation",
    "href": "assignments/assignment1.html#evaluation",
    "title": "44¬† assignment 1",
    "section": "44.3 evaluation",
    "text": "44.3 evaluation\nAll your assignments will be evaluated according to the following criteria:\n\nPresentation. How the graphs look, labels, general organization, markdown, clean code.\nDiscussion. This is where you explain what you did, what you found out, etc.\nDepth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration.\nReplicability: Your code runs flawlessly.\nCode commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself!\nBonus: for originality, creative problem solving, or notable analysis."
  },
  {
    "objectID": "assignments/assignment2.html#smoothing",
    "href": "assignments/assignment2.html#smoothing",
    "title": "45¬† assignment 2",
    "section": "45.1 Smoothing",
    "text": "45.1 Smoothing\nIn this assignment, you will delve into the application of different smoothing techniques on time series data. Utilizing meteorological data, your task is to create a series of plots that demonstrate the effects of various smoothing methods.\n\n45.1.1 1. Comparative Smoothing Methods Analysis\n\nGoal: Showcase three smoothing techniques ‚Äì Rolling Average, Savitzky-Golay, and Resampling ‚Äì on the same time series data.\nTask: Overlay these methods over the actual data in a single plot. Ensure each method uses the same window size for consistency. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n45.1.2 2. Rolling Average Window Size Impact\n\nGoal: Analyze the effect of varying window sizes on the Rolling Average method.\nTask: Produce a plot with three lines, each representing the Rolling Average with a different window size. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n45.1.3 3. Savitzky-Golay Polynomial Order Variation\n\nGoal: Investigate how changing the polynomial order affects the Savitzky-Golay smoothing method.\nTask: Create a plot with three lines, where each represents the Savitzky-Golay method with a different polynomial order. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n45.1.4 4. Kernel Shape Influence in Rolling Mean\n\nGoal: Explore the impact of different kernel shapes on the Rolling Mean.\nTask: Generate a plot displaying three lines, each using a different kernel shape in the Rolling Mean. We encorage to use unique kernel shapes that we did not showcase in class. See this list of kernels. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n45.1.5 5. Moving Average with Confidence Interval\n\nGoal: Plot a Moving Average along with a 75% confidence interval.\nTask: Design a plot illustrating both the Moving Average and its 75% confidence interval.\n\n\n# code goes here"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "46¬† technical stuff",
    "section": "46.1 operating systems",
    "text": "46.1 operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "46¬† technical stuff",
    "section": "46.2 software",
    "text": "46.2 software\nAnaconda‚Äôs Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "46¬† technical stuff",
    "section": "46.3 python packages",
    "text": "46.3 python packages\nKats ‚Äî a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "technical-stuff/datasets.html#sunspots",
    "href": "technical-stuff/datasets.html#sunspots",
    "title": "47¬† datasets",
    "section": "47.1 Sunspots",
    "text": "47.1 Sunspots\nThe solar cycle produces varying amounts of sunspots throughout the years.\n\n\n\nSource: https://www.sidc.be/SILSO/monthlyssnplot\nDownload data from the Royal Observatory of Belgium."
  },
  {
    "objectID": "technical-stuff/datasets.html#covid-19-open-data",
    "href": "technical-stuff/datasets.html#covid-19-open-data",
    "title": "47¬† datasets",
    "section": "47.2 Covid-19 Open Data",
    "text": "47.2 Covid-19 Open Data\nDownload the data into your own tools and systems to analyze the virus‚Äôs spread or decline, investigate COVID-related deaths, study the effects of different vaccines, and more in 20,000-plus locations worldwide.\n\n\n\nSource: https://health.google.com/covid-19/open-data/explorer\nClick here to go to the download page. Choose desired region under section ‚ÄúUnderstanding the data‚Äù."
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#rectangular-kernel",
    "href": "behind-the-scenes/sliding-window-video.html#rectangular-kernel",
    "title": "48¬† sliding window video",
    "section": "48.1 Rectangular kernel",
    "text": "48.1 Rectangular kernel\n\n%matplotlib widget\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True,\n                       gridspec_kw={'height_ratios':[1,0.4], 'hspace':0.1})\n\nclass Lines:\n    \"\"\"\n    empty class, later will be populated with graph objects.\n    this is useful to draw and erase lines on demand.\n    \"\"\"\n    pass\nlines = Lines()\n\n# rename axes for convenience\nax0 = ax[0]\nax1 = ax[1]\n# sm = df['TD'].rolling(10, center=True).mean()\n# ga = df['TD'].rolling(10, center=True, win_type=\"gaussian\").mean(std=100.0)\n\n# set graph y limits\nylim = [3, 22]\n# choose here windown width in minutes\nwindow_width_min = 200.0\nwindow_width_min_integer = int(window_width_min)  # same but integer\nwindow_width_int = int(window_width_min // 10 + 1)  # window width in points\nN = len(df)  # df length\n# time range over which the kernel will slide\n# starts at \"start\", minus the width of the window,\n# minus half an hour, so that the window doesn't start sliding right away at the beginning of the video\n# ends an hour after the window has finished sliding\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\n# starting time\nt0 = t_swipe[0]\n# show sliding window on the top panel as a light blue shade\nlines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1)\n# this is our \"boxcart\" kernel (a rectangle)\nkernel_rect = np.ones(window_width_int)\n# calculate the moving average with \"kernel_rect\" as weights\n# this is the same as a convolution, which is just faster to compute\ndf.loc[:, 'con'] = np.convolve(df['TD'].values, kernel_rect, mode='same') / len(kernel_rect)\n# create a new column for the kernel, fill it with zeros\ndf['kernel_plus'] = 0.0\n# populate the kernel column with the window at the very beginning\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_rect\n# plot kernel on the bottom panel\nlines.kernel_line, = ax1.plot(df['kernel_plus'])\n# plot temperature on the top panel\nax0.plot(df.loc[start:end, 'TD'], color=\"black\")\n# make temperature look gray when inside the sliding window\nlines.gray_line, = ax0.plot(df.loc[df['kernel_plus']==1.0, 'TD'],\n                     color=[0.6]*3, lw=3)\n# calculate the middle of the sliding window\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# plot a pink line showing the result of the moving average\n# from the beginning to the middle of the sliding window\nlines.pink_line, = ax0.plot(df.loc[start:window_middle, 'con'], color=\"xkcd:hot pink\", lw=3)\n# emphasize the location of the middle on the window with a circle\nlines.pink_circle, = ax0.plot([window_middle], [df.loc[window_middle, 'con']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\n# some explanation\nax0.text(0.99, 0.97, f\"kernel: boxcar (rectangle)\\nwidth = {window_width_min:.0f} minutes\", transform=ax0.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\n# axis tweaking\nax0.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (¬∞C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\nax1.set(ylim=[-0.2, 1.2],\n        xlim=[start, end],\n        ylabel=\"kernel\"\n       )\n# adjust dates on both panels as defined before\ncenter_dates_two_panels(ax0, ax1)\n\ndef update_swipe(k, lines):\n    \"\"\"\n    updates both panels, given the index k along which the window is sliding\n    \"\"\"\n    # left side of the sliding window\n    t0 = t_swipe[k]\n    # middle position\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    # erase previous blue shade on the top graph\n    lines.fill_bet.remove()\n    # fill again the blue shade in the updated window position\n    lines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                               y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n    # update pink curve\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'con'].values)\n    # update pink circle\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'con']])\n    # update the kernel in its current position\n    lines.kernel_rect = np.ones(window_width_int)\n    df.loc[:, 'kernel_plus'] = 0.0\n    df.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_rect\n    # update gray line\n    lines.gray_line.set_data(df.loc[df['kernel_plus']==1.0, 'TD'].index,\n                             df.loc[df['kernel_plus']==1.0, 'TD'].values)\n    # update kernel line\n    lines.kernel_line.set_data(df['kernel_plus'].index, df['kernel_plus'].values)\n\n# create a tqdm progress bar\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\n# loop over all sliding indices, update graph and then save it\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/boxcar{window_width_min_integer}/boxcar_{window_width_min_integer}min_{fignum:03}.png\", dpi=600)\n    # update the progress bar\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 604/605 [05:27&lt;00:00,  1.85iteration/s]\n\n\n\n\n\nCombine all saved images into one mp4 video.\n\n# Define the path to your PNG images\npngs_path = f\"pngs/boxcar{window_width_min_integer}\"\npngs_name = f\"boxcar_{window_width_min_integer}min_%03d.png\"\n\n# Define the output video file path\nvideo_output = f\"output{window_width_min_integer}.mp4\"\n\n# Use ffmpeg to create a video from PNG images\n# desired framerate. choose 24 if you don't know what to do\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.0 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58.  2.100 / 58.  2.100\n  libavcodec     60.  3.100 / 60.  3.100\n  libavformat    60.  3.100 / 60.  3.100\n  libavdevice    60.  1.100 / 60.  1.100\n  libavfilter     9.  3.100 /  9.  3.100\n  libswscale      7.  1.100 /  7.  1.100\n  libswresample   4. 10.100 /  4. 10.100\n  libpostproc    57.  1.100 / 57.  1.100\nInput #0, image2, from 'pngs/boxcar200/boxcar_200min_%03d.png':\n  Duration: 00:00:50.33, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -&gt; #0:0 (png (native) -&gt; h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7fa027f2e300] using SAR=1/1\n[libx264 @ 0x7fa027f2e300] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7fa027f2e300] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7fa027f2e300] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output200.mp4':\n  Metadata:\n    encoder         : Lavf60.3.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.3.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\nframe=  604 fps= 23 q=-1.0 Lsize=    1412kB time=00:00:50.08 bitrate= 231.0kbits/s speed=1.91x     \nvideo:1404kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.564556%\n[libx264 @ 0x7fa027f2e300] frame I:3     Avg QP: 9.98  size:135751\n[libx264 @ 0x7fa027f2e300] frame P:154   Avg QP:14.75  size:  2507\n[libx264 @ 0x7fa027f2e300] frame B:447   Avg QP:22.66  size:  1440\n[libx264 @ 0x7fa027f2e300] consecutive B-frames:  1.0%  0.7%  1.0% 97.4%\n[libx264 @ 0x7fa027f2e300] mb I  I16..4: 55.5% 38.8%  5.7%\n[libx264 @ 0x7fa027f2e300] mb P  I16..4:  0.4%  0.3%  0.0%  P16..4:  0.2%  0.1%  0.0%  0.0%  0.0%    skip:99.0%\n[libx264 @ 0x7fa027f2e300] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  1.0%  0.2%  0.0%  direct: 0.0%  skip:98.7%  L0:50.0% L1:49.3% BI: 0.7%\n[libx264 @ 0x7fa027f2e300] 8x8 transform intra:37.1% inter:49.0%\n[libx264 @ 0x7fa027f2e300] coded y,u,v intra: 3.6% 0.4% 0.6% inter: 0.1% 0.0% 0.0%\n[libx264 @ 0x7fa027f2e300] i16 v,h,dc,p: 90% 10%  0%  0%\n[libx264 @ 0x7fa027f2e300] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41%  3% 56%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7fa027f2e300] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 51% 14% 20%  3%  2%  3%  2%  3%  2%\n[libx264 @ 0x7fa027f2e300] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7fa027f2e300] ref P L0: 56.6%  3.8% 28.4% 11.3%\n[libx264 @ 0x7fa027f2e300] ref B L0: 85.6% 13.4%  1.0%\n[libx264 @ 0x7fa027f2e300] ref B L1: 95.9%  4.1%\n[libx264 @ 0x7fa027f2e300] kb/s:228.44\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/boxcar200/boxcar_200min_%03d.png -c:v libx264 -vf fps=12 output200.mp4', returncode=0)\n\n\nThe following code does exactly as you see above, but it is not well commented. You are an intelligent person, you‚Äôll figure this out."
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#triangular-kernel",
    "href": "behind-the-scenes/sliding-window-video.html#triangular-kernel",
    "title": "48¬† sliding window video",
    "section": "48.2 Triangular kernel",
    "text": "48.2 Triangular kernel\n\n%matplotlib widget\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True,\n                       gridspec_kw={'height_ratios':[1,0.4], 'hspace':0.1})\n\nclass Lines:\n    pass\nlines = Lines()\n\nax0 = ax[0]\nax1 = ax[1]\nylim = [3, 22]\nwindow_width_min = 500.0\nwindow_width_int = int(window_width_min / 10) + 1\nN = len(df)\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\nt0 = t_swipe[200]\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# fill between blue shade, plot kernel\nlines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1)\nhalf_triang = np.arange(1, window_width_int/2+1, 1)\nkernel_triang = np.hstack([half_triang, half_triang[-2::-1]])\nkernel_triang = kernel_triang / kernel_triang.max()\ndf.loc[:, 'con'] = np.convolve(df['TD'].values, kernel_triang, mode='same') / len(kernel_triang) * 2\ndf['kernel_plus'] = 0.0\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_triang\nlines.kernel_line, = ax1.plot(df['kernel_plus'], color=\"tab:blue\")\nax0.plot(df.loc[start:end, 'TD'], color=\"black\")\nlines.gray_line, = ax0.plot(df.loc[df['kernel_plus']!=0.0, 'TD'],\n                     color=[0.6]*3, lw=3)\nlines.pink_line, = ax0.plot(df.loc[start:window_middle, 'con'], color=\"xkcd:hot pink\", lw=3)\nlines.pink_circle, = ax0.plot([window_middle], [df.loc[window_middle, 'con']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\nax0.text(0.99, 0.97, f\"kernel: triangle\\nwidth = {window_width_min:.0f} minutes\", transform=ax0.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\nax0.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (¬∞C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\nax1.set(ylim=[-0.2, 1.2],\n        xlim=[start, end],\n        ylabel=\"kernel\"\n       )\ncenter_dates_two_panels(ax0, ax1)\n\ndef update_swipe(k, lines):\n    t0 = t_swipe[k]\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    lines.fill_bet.remove()\n    lines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                               y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'con'].values)\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'con']])\n    lines.kernel_rect = np.ones(window_width_int)\n    df['kernel_plus'] = 0.0\n    df.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_triang\n    lines.gray_line.set_data(df.loc[df['kernel_plus']!=0.0,'TD'].index,\n                             df.loc[df['kernel_plus']!=0.0,'TD'].values)\n    lines.kernel_line.set_data(df['kernel_plus'].index, df['kernel_plus'].values)\n\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/triangle/triangle_{fignum:03}.png\", dpi=600)\n    # update the progress bar\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 634/635 [05:35&lt;00:00,  1.89iteration/s]\n\n\n\n\n\n\n# Define the path to your PNG images\npngs_path = \"pngs/triangle\"\npngs_name = \"triangle_%03d.png\"\n\n# Define the output video file path\nvideo_output = \"output_triangle.mp4\"\n\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.0 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58.  2.100 / 58.  2.100\n  libavcodec     60.  3.100 / 60.  3.100\n  libavformat    60.  3.100 / 60.  3.100\n  libavdevice    60.  1.100 / 60.  1.100\n  libavfilter     9.  3.100 /  9.  3.100\n  libswscale      7.  1.100 /  7.  1.100\n  libswresample   4. 10.100 /  4. 10.100\n  libpostproc    57.  1.100 / 57.  1.100\nInput #0, image2, from 'pngs/triangle/triangle_%03d.png':\n  Duration: 00:00:52.83, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -&gt; #0:0 (png (native) -&gt; h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7fa9b0807f80] using SAR=1/1\n[libx264 @ 0x7fa9b0807f80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7fa9b0807f80] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7fa9b0807f80] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output_triangle.mp4':\n  Metadata:\n    encoder         : Lavf60.3.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.3.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\nframe=  634 fps= 23 q=-1.0 Lsize=    1324kB time=00:00:52.58 bitrate= 206.2kbits/s speed=1.94x     \nvideo:1316kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.624018%\n[libx264 @ 0x7fa9b0807f80] frame I:3     Avg QP:10.55  size:133880\n[libx264 @ 0x7fa9b0807f80] frame P:162   Avg QP:12.92  size:  2541\n[libx264 @ 0x7fa9b0807f80] frame B:469   Avg QP:21.63  size:  1137\n[libx264 @ 0x7fa9b0807f80] consecutive B-frames:  0.6%  0.3%  5.7% 93.4%\n[libx264 @ 0x7fa9b0807f80] mb I  I16..4: 51.0% 43.4%  5.6%\n[libx264 @ 0x7fa9b0807f80] mb P  I16..4:  0.4%  0.2%  0.0%  P16..4:  0.2%  0.1%  0.0%  0.0%  0.0%    skip:99.0%\n[libx264 @ 0x7fa9b0807f80] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.0%  0.2%  0.0%  direct: 0.0%  skip:98.8%  L0:50.5% L1:48.9% BI: 0.6%\n[libx264 @ 0x7fa9b0807f80] 8x8 transform intra:39.7% inter:41.2%\n[libx264 @ 0x7fa9b0807f80] coded y,u,v intra: 2.9% 0.5% 0.6% inter: 0.0% 0.0% 0.0%\n[libx264 @ 0x7fa9b0807f80] i16 v,h,dc,p: 91%  9%  0%  0%\n[libx264 @ 0x7fa9b0807f80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41%  3% 55%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7fa9b0807f80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 44% 17% 20%  4%  3%  4%  2%  4%  2%\n[libx264 @ 0x7fa9b0807f80] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7fa9b0807f80] ref P L0: 53.5%  2.7% 25.8% 18.0%\n[libx264 @ 0x7fa9b0807f80] ref B L0: 85.3% 13.2%  1.5%\n[libx264 @ 0x7fa9b0807f80] ref B L1: 96.6%  3.4%\n[libx264 @ 0x7fa9b0807f80] kb/s:203.87\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/triangle/triangle_%03d.png -c:v libx264 -vf fps=12 output_triangle.mp4', returncode=0)"
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#gaussian-kernel",
    "href": "behind-the-scenes/sliding-window-video.html#gaussian-kernel",
    "title": "48¬† sliding window video",
    "section": "48.3 Gaussian kernel",
    "text": "48.3 Gaussian kernel\n\n%matplotlib widget\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True,\n                       gridspec_kw={'height_ratios':[1,0.4], 'hspace':0.1})\n\nclass Lines:\n    pass\nlines = Lines()\n\nax0 = ax[0]\nax1 = ax[1]\nylim = [3, 22]\nwindow_width_min = 500.0\nwindow_width_int = int(window_width_min / 10) + 1\nN = len(df)\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\nt0 = t_swipe[0]\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# fill between blue shade, plot kernel\nhalf_triang = np.arange(1, window_width_int/2+1, 1)\nkernel_triang = np.hstack([half_triang, half_triang[-2::-1]])\nkernel_triang = kernel_triang / kernel_triang.max()\ndf['con'] = np.convolve(df['TD'].values, kernel_triang, mode='same') / len(kernel_triang) * 2\ndf['kernel_plus'] = 0.0\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_triang\n\n# array of minutes. multiply by 10 because data is every 10 minutes\n\nstd_in_minutes = 60\ng = sp.signal.gaussian(window_width_int, std_in_minutes/10)#, sym=True)\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = g\ngaussian_threshold = np.exp(-2**2)  # two sigmas\nlines.kernel_line, = ax1.plot(df['kernel_plus'], color=\"tab:blue\")\nwindow_above_threshold = df.loc[df['kernel_plus'] &gt; gaussian_threshold, 'kernel_plus'].index\nlines.fill_bet = ax0.fill_between([window_above_threshold[0], window_above_threshold[-1]],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n\n# gaussian convolution from here: https://stackoverflow.com/questions/27205402/pandas-rolling-window-function-offsets-data\ndf.loc[:, 'con'] = np.convolve(df['TD'].values, g/g.sum(), mode='same')\nax0.plot(df.loc[start:end, 'TD'], color=\"black\")\nlines.gray_line, = ax0.plot(df.loc[window_above_threshold[0]:window_above_threshold[-1], 'TD'],\n                     color=[0.6]*3, lw=3)\nlines.pink_line, = ax0.plot(df.loc[start:window_middle, 'con'], color=\"xkcd:hot pink\", lw=3)\nlines.pink_circle, = ax0.plot([window_middle], [df.loc[window_middle, 'con']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\nax0.text(0.99, 0.97, f\"kernel: gaussian\\nwidth = {window_width_min:.0f} minutes\\nstd = {std_in_minutes:.0f} minutes\", transform=ax0.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\nax0.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (¬∞C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\nax1.set(ylim=[-0.2, 1.2],\n        xlim=[start, end],\n        ylabel=\"kernel\"\n       )\ngauss = df['TD'].rolling(window=window_width_int, center=True, win_type=\"gaussian\").mean(std=6)#, sym=True)\ncenter_dates_two_panels(ax0, ax1)\n\ndef update_swipe(k, lines):\n    t0 = t_swipe[k]\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    lines.fill_bet.remove()\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'con'].values)\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'con']])\n    lines.kernel_rect = np.ones(window_width_int)\n    df['kernel_plus'] = 0.0\n    df.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = g\n    window_above_threshold = df.loc[df['kernel_plus'] &gt; gaussian_threshold, 'kernel_plus'].index\n    lines.gray_line.set_data(df.loc[window_above_threshold[0]:window_above_threshold[-1], 'TD'].index,\n                             df.loc[window_above_threshold[0]:window_above_threshold[-1], 'TD'].values)\n    lines.kernel_line.set_data(df['kernel_plus'].index, df['kernel_plus'].values)\n    window_above_threshold = df.loc[df['kernel_plus'] &gt; gaussian_threshold, 'kernel_plus'].index\n    lines.fill_bet = ax0.fill_between([window_above_threshold[0], window_above_threshold[-1]],\n                                       y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/gaussian/gaussian_{fignum:03}.png\", dpi=600)\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 634/635 [05:47&lt;00:00,  1.83iteration/s]\n\n\n\n\n\n\n# Define the path to your PNG images\npngs_path = \"pngs/gaussian\"\npngs_name = \"gaussian_%03d.png\"\n\n# Define the output video file path\nvideo_output = \"output_gaussian.mp4\"\n\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.0 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58.  2.100 / 58.  2.100\n  libavcodec     60.  3.100 / 60.  3.100\n  libavformat    60.  3.100 / 60.  3.100\n  libavdevice    60.  1.100 / 60.  1.100\n  libavfilter     9.  3.100 /  9.  3.100\n  libswscale      7.  1.100 /  7.  1.100\n  libswresample   4. 10.100 /  4. 10.100\n  libpostproc    57.  1.100 / 57.  1.100\nInput #0, image2, from 'pngs/gaussian/gaussian_%03d.png':\n  Duration: 00:00:52.83, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -&gt; #0:0 (png (native) -&gt; h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7ff6d8907580] using SAR=1/1\n[libx264 @ 0x7ff6d8907580] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7ff6d8907580] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7ff6d8907580] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output_gaussian.mp4':\n  Metadata:\n    encoder         : Lavf60.3.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.3.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\nframe=  634 fps= 21 q=-1.0 Lsize=    1386kB time=00:00:52.58 bitrate= 215.9kbits/s speed=1.77x     \nvideo:1378kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.602110%\n[libx264 @ 0x7ff6d8907580] frame I:3     Avg QP:10.13  size:140267\n[libx264 @ 0x7ff6d8907580] frame P:161   Avg QP:14.05  size:  2700\n[libx264 @ 0x7ff6d8907580] frame B:470   Avg QP:21.81  size:  1180\n[libx264 @ 0x7ff6d8907580] consecutive B-frames:  0.9%  0.6%  0.0% 98.4%\n[libx264 @ 0x7ff6d8907580] mb I  I16..4: 53.9% 40.2%  5.9%\n[libx264 @ 0x7ff6d8907580] mb P  I16..4:  0.4%  0.3%  0.0%  P16..4:  0.2%  0.1%  0.0%  0.0%  0.0%    skip:99.0%\n[libx264 @ 0x7ff6d8907580] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  1.0%  0.1%  0.0%  direct: 0.0%  skip:98.8%  L0:50.7% L1:48.4% BI: 0.9%\n[libx264 @ 0x7ff6d8907580] 8x8 transform intra:39.0% inter:41.4%\n[libx264 @ 0x7ff6d8907580] coded y,u,v intra: 3.0% 0.5% 0.6% inter: 0.0% 0.0% 0.0%\n[libx264 @ 0x7ff6d8907580] i16 v,h,dc,p: 91%  9%  0%  0%\n[libx264 @ 0x7ff6d8907580] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 40%  5% 55%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7ff6d8907580] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 45% 17% 20%  4%  3%  4%  2%  3%  2%\n[libx264 @ 0x7ff6d8907580] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7ff6d8907580] ref P L0: 60.0%  3.7% 25.1% 11.2%\n[libx264 @ 0x7ff6d8907580] ref B L0: 87.0% 11.7%  1.3%\n[libx264 @ 0x7ff6d8907580] ref B L1: 96.7%  3.3%\n[libx264 @ 0x7ff6d8907580] kb/s:213.50\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/gaussian/gaussian_%03d.png -c:v libx264 -vf fps=12 output_gaussian.mp4', returncode=0)"
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#comparison",
    "href": "behind-the-scenes/sliding-window-video.html#comparison",
    "title": "48¬† sliding window video",
    "section": "48.4 Comparison",
    "text": "48.4 Comparison\nLet‚Äôs plot in one graph the smoothed temperature for each kernel shape we calculated above (rectangular, triangular, gaussian), all of which with a 500-minute-wide window.\n\nwindow_width_min = 500.0\nwindow_width_int = int(window_width_min // 10 + 1)\n\n# rectangular, 500 min\nkernel_rect = np.ones(window_width_int)\nrect = np.convolve(df['TD'].values, kernel_rect, mode='same') / len(kernel_rect)\n\n# triangular\nhalf_triang = np.arange(1, window_width_int/2+1, 1)\nkernel_triang = np.hstack([half_triang, half_triang[-2::-1]])\nkernel_triang = kernel_triang / kernel_triang.max()\ntriang = np.convolve(df['TD'].values, kernel_triang, mode='same') / len(kernel_triang) * 2\n\n# gaussian\ngauss = df['TD'].rolling(window=window_width_int, center=True, win_type=\"gaussian\").mean(std=6)#, sym=True)\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.figure.subplots_adjust(top=0.93, bottom=0.10, left=0.1, right=0.95)\n\nax.plot(df.loc[start:end, 'TD'], color='black', label=\"measured\")\nax.plot(df.index, rect, color=\"tab:blue\", label=\"rectangular kernel\")\nax.plot(df.index, triang, color=\"tab:orange\", label=\"triangular kernel\")\nax.plot(df.index, gauss, color=\"tab:red\", label=\"gaussian kernel\")\nax.legend()\n\nax.set(ylim=[5, 17.5],\n       xlim=['2022-01-04 00:00:00', '2022-01-05 23:50:00'],\n       ylabel=\"Temperature (¬∞C)\",\n       title=\"Yatir Forest, 2022\",\n       yticks=[5,10,15])\ncenter_dates(ax)\nfig.savefig(\"kernel_comparison.png\")\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.figure.subplots_adjust(top=0.93, bottom=0.15, left=0.1, right=0.95)\n\nN=500\nrec_window = np.zeros(800)\nrec_window[150:150+N] = signal.windows.boxcar(N)\ntri_window = np.zeros(800)\ntri_window[150:150+N] = signal.windows.triang(N)\ngau_window = np.zeros(800)\ngau_window[150:150+N] = signal.windows.gaussian(N, std=60)\nt = np.arange(-150, 650)\nax.plot(t, rec_window, color=\"tab:blue\")\nax.plot(t, tri_window, color=\"tab:orange\")\nax.plot(t, gau_window, color=\"tab:red\")\nax.text(0, 0.5, \"rect\", color=\"tab:blue\")\nax.text(373, 0.5, \"triang\", color=\"tab:orange\")\nax.text(150, 0.1, \"gauss\\n\"+r\"$\\sigma=60$ min\", color=\"tab:red\")\nax.set(ylim=[-0.1, 1.1],\n       xlim=[-150, 650],\n       ylabel=\"amplitude\",\n       xlabel=\"minutes\",\n       title=\"kernel shape comparison\",)\nfig.savefig(\"kernel_shapes.png\")"
  },
  {
    "objectID": "behind-the-scenes/savgol-video.html#savgol-filter",
    "href": "behind-the-scenes/savgol-video.html#savgol-filter",
    "title": "49¬† savgol video",
    "section": "49.1 Savgol filter",
    "text": "49.1 Savgol filter\n\n# Function to fit and get polynomial values\ndef fit_polynomial(x, y, degree):\n    coeffs = np.polyfit(x, y, degree)\n    poly = np.poly1d(coeffs)\n    return poly(x), coeffs\n\n# Function to fit and get polynomial values\ndef poly_coeffs(x, y, degree):\n    coeffs = np.polyfit(x, y, degree)\n    return coeffs\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(df.loc[start:end, 'TD'], color='black')\n\nsg = savgol_filter(df['TD'], 13, 2)\n\ni = 500\nax.plot(df.index[:i], sg[:i], color='xkcd:hot pink')\n\nwindow_pts = 31\np_order = 3\n\nwindow_x = np.arange(i - window_pts // 2, i + window_pts // 2)\nwindow_y = df['TD'][i - window_pts // 2:i + window_pts // 2]\n\n# Fit and plot polynomial inside the window\nfitted_y, coeffs = fit_polynomial(window_x, window_y, p_order)\n\nwhole_x = np.arange(len(df))\nwhole_y = df['TD'].values\npoly = np.poly1d(coeffs)\nwhole_poly = poly(whole_x)\n\nax.plot(df.index, whole_poly, color='xkcd:sun yellow', lw=2)\n# ax.plot(df.index[window_x], fitted_y, color='0.8', lw=3)\nax.plot(df.index[window_x], fitted_y, color='xkcd:mustard', lw=2)\n\n\n\nax.set(ylim=[5, 17.5],\n       xlim=[start, end],\n       ylabel=\"Temperature (¬∞C)\",\n       title=\"Yatir Forest, 2022\",\n       yticks=[5,10,15])\ncenter_dates(ax)\n# fig.savefig(\"sliding_YF_temperature_2022.png\")\n\n\n\n\n\n\n\n\n\np_order = 3\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,5))\n\nclass Lines:\n    \"\"\"\n    empty class, later will be populated with graph objects.\n    this is useful to draw and erase lines on demand.\n    \"\"\"\n    pass\nlines = Lines()\n\n# set graph y limits\nylim = [3, 22]\n# choose here windown width in minutes\nwindow_width_min = 500.0\nwindow_width_min_integer = int(window_width_min)  # same but integer\nwindow_width_int = int(window_width_min // 10 + 1)  # window width in points\nN = len(df)  # df length\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\n# starting time\nt0 = t_swipe[0]\nind0 = df.index.get_loc(t0) + window_width_int//2 + 1\n# show sliding window on the top panel as a light blue shade\nlines.fill_bet = ax.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1)\n\nsg = savgol_filter(df['TD'], window_width_int, p_order)\ndf.loc[:, 'sg'] = sg \n# plot temperature\nax.plot(df.loc[start:end, 'TD'], color=\"black\")\n\n# define x,y data inside window to execute polyfit on\nwindow_x = np.arange(ind0 - window_width_int // 2, ind0 + window_width_int // 2)\nwindow_y = df['TD'][ind0 - window_width_int // 2:ind0 + window_width_int // 2].values\n# fit and plot polynomial inside the window\nfitted_y, coeffs = fit_polynomial(window_x, window_y, p_order)\n# get x,y data for the whole array\nwhole_x = np.arange(len(df))\nwhole_y = df['TD'].values\npoly = np.poly1d(coeffs)\nwhole_poly = poly(whole_x)\n\n# calculate the middle of the sliding window\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# plot a pink line showing the result of the moving average\n# from the beginning to the middle of the sliding window\nlines.pink_line, = ax.plot(df.loc[start:window_middle, 'sg'], color=\"xkcd:hot pink\", lw=3)\n\nlines.poly_all, = ax.plot(df.index, whole_poly, color='xkcd:sun yellow', lw=2)\nlines.poly_window, = ax.plot(df.index[window_x], fitted_y, color='xkcd:mustard', lw=2)\n\n# emphasize the location of the middle on the window with a circle\nlines.pink_circle, = ax.plot([window_middle], [df.loc[window_middle, 'sg']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\n# some explanation\nax.text(0.99, 0.97, f\"savitzky-golay\\nwidth = {window_width_int:.0f} pts\\npoly order = {p_order}\", transform=ax.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\n# axis tweaking\nax.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (¬∞C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\n# adjust dates on both panels as defined before\ncenter_dates(ax)\n\ndef update_swipe(k, lines):\n    \"\"\"\n    updates both panels, given the index k along which the window is sliding\n    \"\"\"\n    # left side of the sliding window\n    t0 = t_swipe[k]\n    # middle position\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    ind0 = df.index.get_loc(window_middle)\n    # erase previous blue shade on the top graph\n    lines.fill_bet.remove()\n    # fill again the blue shade in the updated window position\n    lines.fill_bet = ax.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                               y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n    # update pink curve\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'sg'].values)\n    # update pink circle\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'sg']])\n    # define x,y data inside window to execute polyfit on\n    \n    window_x = np.arange(ind0 - window_width_int // 2, ind0 + window_width_int // 2)\n    window_y = df['TD'][ind0 - window_width_int // 2:ind0 + window_width_int // 2]\n    # fit and plot polynomial inside the window\n    fitted_y, coeffs = fit_polynomial(window_x, window_y, p_order)\n    poly = np.poly1d(coeffs)\n    whole_poly = poly(whole_x)\n    lines.poly_all.set_data(df.index, whole_poly)\n    lines.poly_window.set_data(df.index[window_x], fitted_y)\n\nfig.savefig(f\"pngs/savgol{window_width_int}/savgol_zero.png\", dpi=600)\n\n# create a tqdm progress bar\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\n# loop over all sliding indices, update graph and then save it\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/savgol{window_width_int}/savgol_{window_width_int}_{fignum:03}.png\", dpi=600)\n    # update the progress bar\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 634/635 [13:07&lt;00:01,  1.24s/iteration]\n\n\n\n\n\nCombine all saved images into one mp4 video.\n\n# Define the path to your PNG images\npngs_path = f\"pngs/savgol51\"\npngs_name = f\"savgol_51_%03d.png\"\n\n# Define the output video file path\nvideo_output = f\"output_savgol51.mp4\"\n\n# Use ffmpeg to create a video from PNG images\n# desired framerate. choose 24 if you don't know what to do\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.1.1_2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58. 29.100 / 58. 29.100\n  libavcodec     60. 31.102 / 60. 31.102\n  libavformat    60. 16.100 / 60. 16.100\n  libavdevice    60.  3.100 / 60.  3.100\n  libavfilter     9. 12.100 /  9. 12.100\n  libswscale      7.  5.100 /  7.  5.100\n  libswresample   4. 12.100 /  4. 12.100\n  libpostproc    57.  3.100 / 57.  3.100\nInput #0, image2, from 'pngs/savgol51/savgol_51_%03d.png':\n  Duration: 00:00:52.83, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc, gbr/unknown/unknown), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -&gt; #0:0 (png (native) -&gt; h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7f9cb7906dc0] using SAR=1/1\n[libx264 @ 0x7f9cb7906dc0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7f9cb7906dc0] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7f9cb7906dc0] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output_savgol51.mp4':\n  Metadata:\n    encoder         : Lavf60.16.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.31.102 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n[out#0/mp4 @ 0x7f9cb7806000] video:3513kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.225481%\nframe=  634 fps=3.2 q=-1.0 Lsize=    3521kB time=00:00:52.58 bitrate= 548.5kbits/s speed=0.27x    \n[libx264 @ 0x7f9cb7906dc0] frame I:3     Avg QP:13.70  size:146882\n[libx264 @ 0x7f9cb7906dc0] frame P:232   Avg QP:19.02  size:  7856\n[libx264 @ 0x7f9cb7906dc0] frame B:399   Avg QP:24.04  size:  3341\n[libx264 @ 0x7f9cb7906dc0] consecutive B-frames: 11.4% 10.7% 10.4% 67.5%\n[libx264 @ 0x7f9cb7906dc0] mb I  I16..4: 32.8% 61.0%  6.2%\n[libx264 @ 0x7f9cb7906dc0] mb P  I16..4:  0.5%  0.7%  0.3%  P16..4:  0.4%  0.2%  0.1%  0.0%  0.0%    skip:97.7%\n[libx264 @ 0x7f9cb7906dc0] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  1.9%  0.3%  0.0%  direct: 0.0%  skip:97.6%  L0:50.4% L1:47.8% BI: 1.7%\n[libx264 @ 0x7f9cb7906dc0] 8x8 transform intra:51.3% inter:35.8%\n[libx264 @ 0x7f9cb7906dc0] coded y,u,v intra: 7.2% 6.5% 4.4% inter: 0.1% 0.1% 0.0%\n[libx264 @ 0x7f9cb7906dc0] i16 v,h,dc,p: 89% 10%  1%  0%\n[libx264 @ 0x7f9cb7906dc0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31%  3% 65%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7f9cb7906dc0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 54%  7% 23%  3%  1%  4%  1%  5%  1%\n[libx264 @ 0x7f9cb7906dc0] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7f9cb7906dc0] ref P L0: 58.7%  5.7% 25.2% 10.4%\n[libx264 @ 0x7f9cb7906dc0] ref B L0: 85.9% 11.8%  2.4%\n[libx264 @ 0x7f9cb7906dc0] ref B L1: 96.9%  3.1%\n[libx264 @ 0x7f9cb7906dc0] kb/s:544.58\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/savgol51/savgol_51_%03d.png -c:v libx264 -vf fps=12 output_savgol51.mp4', returncode=0)"
  },
  {
    "objectID": "behind-the-scenes/API-IMS.html",
    "href": "behind-the-scenes/API-IMS.html",
    "title": "50¬† API to download data from IMS",
    "section": "",
    "text": "# TOKEN = \"f058958a-d8bd-47cc-95d7-7ecf98610e47\"\n# STATION_NUM = 28  # 28 = \"SHANI\"\n# DATA = 10  # 10 = TDmax (max temperature)\n# start = \"2022/01/01\"\n# end = \"2022/02/01\"\n# url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}\"\n# url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}/data/?from={start}&to={end}\"\n# # url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}/data/{DATA}/data/11/?from={start}&to={end}\"\n# headers = {'Authorization': 'ApiToken f058958a-d8bd-47cc-95d7-7ecf98610e47'}\n# response = requests.request(\"GET\", url, headers=headers)\n# data= json.loads(response.text.encode('utf8'))\n\n# # Save the JSON data to a file\n# with open('shani_2022_january.json', 'w') as json_file:\n#     json.dump(data, json_file)\n\n# data\n\n\n# # https://ims.gov.il/he/ObservationDataAPI\n# # https://ims.gov.il/sites/default/files/2021-09/API%20explanation.pdf\n# # https://ims.gov.il/sites/default/files/2022-04/Python%20API%20example.pdf\n# TOKEN = \"f058958a-d8bd-47cc-95d7-7ecf98610e47\"\n# STATION_NUM = 23  # 23 = \"JERUSALEM CENTRE\"\n# DATA = 9  # 9 = TDmax (max temperature)\n# start = \"2022/01/01\"\n# end = \"2022/02/01\"\n# url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}/data/{DATA}/?from={start}&to={end}\"\n# headers = {'Authorization': 'ApiToken f058958a-d8bd-47cc-95d7-7ecf98610e47'}\n# response = requests.request(\"GET\", url, headers=headers)\n# data= json.loads(response.text.encode('utf8'))\n\n# print(url)\n\n\n# url = \"https://api.ims.gov.il/v1/envista/stations/28/data/10/data/11/?from=2022/01/01&to=2022/01/03\"\n# response = requests.request(\"GET\", url, headers=headers)\n# data = json.loads(response.text.encode('utf8'))\n\n\n# # RH = 8\n# # TDmax = 10, max temperature\n# # TDmin = 11, min temperature\n# url = \"https://api.ims.gov.il/v1/envista/stations/28/data/10/?from=2022/01/01&to=2022/01/31\"\n# response = requests.request(\"GET\", url, headers=headers)\n# data = json.loads(response.text.encode('utf8'))\n\n# df = pd.json_normalize(data['data'],record_path=['channels'], meta=['datetime'])\n# df['date'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)  # ignore time zone information\n# df = df.set_index('date')\n\n# df\n# data['data']"
  }
]