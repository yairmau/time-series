[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "about\nWelcome to Time Series Analysis for Environmental Sciences (71606) at the Hebrew University of Jerusalem. This is Yair Mau, your host for today. I am a senior lecturer at the Institute of Environmental Sciences, at the Faculty of Agriculture, Food and Environment, in Rehovot, Israel.\nThis website contains (almost) all the material you’ll need for the course. If you find any mistakes, or have any comments, please email me."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "disclaimer",
    "text": "disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#what-who-when-and-where",
    "href": "index.html#what-who-when-and-where",
    "title": "Time Series Analysis",
    "section": "what, who, when and where?",
    "text": "what, who, when and where?\n Course number 71606, 3 academic points\n Yair Mau (lecturer), Erez Feuer (TA)\n Tuesdays, from 14:15 to 17:00\n Computer classroom #18\n Office hours: upon request"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "syllabus",
    "text": "syllabus\n\ncourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\ncourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nlearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\ncourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nbooks and other sources\nClick here.\n\n\ncourse evaluation\nThere will be small projects during the semester (totaling 50% of the final grade), and one final project (50%)."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "weekly program",
    "text": "weekly program\n\nThis year’s course will be a bit different that planned due to the shortening of the academic semester. The information below is NOT up to date. Ask Yair what is relevant this year.\n\n\nweek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nweek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nweek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nweek 4\n\nLecture: Time series plotting: best practices. Dos and don’ts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nweek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nweek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nweek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nweek 8\nSame as lecture 7 above\n\n\nweek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nweek 10\n\nLecture: Fourier decomposition, filtering, Nyquist–Shannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nweek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "who-cares.html#why-time-series-analysis",
    "href": "who-cares.html#why-time-series-analysis",
    "title": "who cares?",
    "section": "why “Time Series Analysis?”",
    "text": "why “Time Series Analysis?”\n\nTime has two aspects. There is the arrow, the running river, without which there is no change, no progress, or direction, or creation. And there is the circle or the cycle, without which there is chaos, meaningless succession of instants, a world without clocks or seasons or promises.\nURSULA K. LE GUIN\n\nYou are here because you are interested in how things change, evolve. In this course I want to discuss with you how to make sense of data whose temporal nature is in its very essence. We will talk about randomness, cycles, frequencies, correlations, and more."
  },
  {
    "objectID": "who-cares.html#why-environmental-sciences",
    "href": "who-cares.html#why-environmental-sciences",
    "title": "who cares?",
    "section": "why “Environmental Sciences”",
    "text": "why “Environmental Sciences”\nThis same time series analysis (TSA) course could be called instead “TSA for finance”, “TSA for Biology”, or any other application. The emphasis in this course is not Environmental Sciences, but the concepts and tools of TSA. Because my research is in Environmental Science, and many of the graduate students at HUJI-Rehovot research this, I chose to use examples “close to home”. The same toolset should be useful for students of other disciplines."
  },
  {
    "objectID": "who-cares.html#what-is-it-good-for",
    "href": "who-cares.html#what-is-it-good-for",
    "title": "who cares?",
    "section": "what is it good for?",
    "text": "what is it good for?\nIn many fields of science we are flooded by data, and it’s hard to see the forest for the trees. I hope that the topics we’ll discuss in this course can help you find meaningful patterns in your data, formulate interesting hypotheses, and design better experiments."
  },
  {
    "objectID": "who-cares.html#do-i-need-it",
    "href": "who-cares.html#do-i-need-it",
    "title": "who cares?",
    "section": "do I need it?",
    "text": "do I need it?\nMaybe. If you are a grad student and you have temporal data to analyze, then probably yes. However, I have very fond memories of courses that I took as a grad student that were completely unrelated to my research. Sometimes “because it’s fun” is a perfectly good answer."
  },
  {
    "objectID": "who-cares.html#what-will-i-actually-gain-from-it",
    "href": "who-cares.html#what-will-i-actually-gain-from-it",
    "title": "who cares?",
    "section": "what will I actually gain from it?",
    "text": "what will I actually gain from it?\nBy the end of this course you will have gained:\n\na hands-on experience of fundamental time-series analysis tools\nan intuition regarding the basic concepts\ntechnical abilities\na springboard for learning more about the subject by yourself"
  },
  {
    "objectID": "basics/boring.html#anaconda",
    "href": "basics/boring.html#anaconda",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.1 Anaconda",
    "text": "1.1 Anaconda\nInstall Anaconda’s Python distribution. The Anaconda installation brings with it all the main python packages we will need to use. In order to install extra packages, refer to these two tutorials: tutorial 1, tutorial 2."
  },
  {
    "objectID": "basics/boring.html#vscode",
    "href": "basics/boring.html#vscode",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.2 VSCode",
    "text": "1.2 VSCode\nInstall VSCode. Visual Studio Code is a very nice IDE (Integrated Development Environment) made by Microsoft, available to all operating systems. Contrary to the title of this page, it is not absolutely necessary to use it, but I like VSCode, and as my student, so do you 😉."
  },
  {
    "objectID": "basics/boring.html#jupyter-notebooks",
    "href": "basics/boring.html#jupyter-notebooks",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.3 jupyter notebooks",
    "text": "1.3 jupyter notebooks\nWe will code exclusively in Jupyter Notebooks. Get acquainted with them. Make sure you can point VSCode to the Anaconda environment of your choice (“base” by default). Don’t worry, this is easier than it sounds.\nOne failproof way of making sure VSCode uses the Anaconda installation is the following:\n\nOpen Anaconda Navigator\nIf you are using HUJI’s computers, in “Environments”, choose “asgard”. If you are using your own computer, ignore this step.\nopen VSCode from inside Anaconda Navigator (see image below).\n\n\nSometimes you will need to manualy install the Jupyter extension on VSCode. In this case follow this tutorial."
  },
  {
    "objectID": "basics/boring.html#folder-structure",
    "href": "basics/boring.html#folder-structure",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.4 folder structure",
    "text": "1.4 folder structure\nYou NEED to be confortable with you computer’s folder (or directory) structure. Where are files located? How to navigate through different folders? How is my stuff organized? If you don’t feel absolutely comfortable with this, then read this, Windows, MacOS. If you use Linux then you surely know this stuff. Make yourself a “time-series” folder wherever you want, and have it backed up regularly (use Google Drive, Dropbox, do it manually, etc). “My dog deleted my files” is not an excuse."
  },
  {
    "objectID": "basics/numpy-pandas-matplotlib.html#pandas",
    "href": "basics/numpy-pandas-matplotlib.html#pandas",
    "title": "2  numpy, pandas, matplotlib",
    "section": "2.1 pandas",
    "text": "2.1 pandas\nWe will primarily use the Pandas package to deal with data. Pandas has become the standard Python tool to manipulate time series, and you should get acquainted with its basic usage. This course will provide you the opportunity to learn by example, but I’m sure we will only scratch the surface, and you’ll be left with lots of questions.\nI provide below a (non-comprehensive) list of useful tutorials, they are a good reference for the beginner and for the experienced user.\n\nPython Data Science Handbook, by Jake VanderPlas\nData Wrangling with pandas Cheat Sheet\nWorking with Dates and Times in Python\nCheat Sheet: The pandas DataFrame Object\nYouTube tutorials by Corey Schafer"
  },
  {
    "objectID": "basics/numpy-pandas-matplotlib.html#pyplot",
    "href": "basics/numpy-pandas-matplotlib.html#pyplot",
    "title": "2  numpy, pandas, matplotlib",
    "section": "2.2 pyplot",
    "text": "2.2 pyplot\nMatplotlib, and its submodule pyplot, are probably the most common Python plotting tool. Pyplot is both great and horrible:\n\nGreat: you’ll have absolutely full control of everything you want to plot. The sky is the limit.\nHorrible: you’ll cry as you do it, because there is so much to know, and it is not the most friendly plotting package.\n\nPyplot is object oriented, so you will usually manipulate the axes object like this.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 2, 0, 3]\n\n# Figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 6))\n# plot on the left\nax1.plot(x, y, color=\"tab:blue\")\nax1.plot(x, y[::-1], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n        ylabel=\"something\",\n        title=\"left panel\")\n# plot on the right\nax2.plot(x, y[::-1])\nax2.set(xlabel=\"date\",\n        ylabel=\"something else\",\n        title=\"right panel\")\n\n[Text(0.5, 0, 'date'),\n Text(0, 0.5, 'something else'),\n Text(0.5, 1.0, 'right panel')]\n\n\n\n\n\nFor the very beginners, you need to know that figure refers to the whole white canvas, and axes means the rectangle inside which something will be plotted:\n\nThe image above is good because it has 2 panels, and it’s easy to understand what going on. Sadly, they mixed the two terms, axis and axes.\n\naxes is where the whole plot will be drawn. In the figure above it is the same as each panel.\naxis is each of the vertical and horizontal lines, where you have ticks and numbers.\n\n\nIf you are new to all this, I recommend that you go to:\n\nEarth Lab’s Introduction to Plotting in Python Using Matplotlib\nJake VanderPlas’s Python Data Science Handbook"
  },
  {
    "objectID": "basics/example.html#open-a-new-jupyter-notebook",
    "href": "basics/example.html#open-a-new-jupyter-notebook",
    "title": "3  learn by example",
    "section": "3.1 open a new Jupyter Notebook",
    "text": "3.1 open a new Jupyter Notebook\n\nOn your computer, open the program Anaconda Navigator (it may take a while to load).\nFind the white box called VS Code and click Launch.\nNow go to File > Open Folder, and open the folder you created for this course. VS Code may ask you if you trust the authors, and the answer is “yes” (it’s your computer).\nFile > New File, and call it example.ipynb\nYou can start copying and pasting code from this website to your Jupyter Notebook. To run a cell, press Shift+Enter.\nYou may be asked to choose to Select Kernel. This is VS Code wanting to know which python installation to use. Click on “Python Environments”, and then choose the option with the word anaconda in it.\nThat’s all! Congratulations!"
  },
  {
    "objectID": "basics/example.html#import-packages",
    "href": "basics/example.html#import-packages",
    "title": "3  learn by example",
    "section": "3.2 import packages",
    "text": "3.2 import packages\nFirst, import packages to be used. They should all be already included in the Anaconda distribution you installed.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters"
  },
  {
    "objectID": "basics/example.html#load-data",
    "href": "basics/example.html#load-data",
    "title": "3  learn by example",
    "section": "3.3 load data",
    "text": "3.3 load data\nLoad CO2 data into a Pandas dataframe. You can load it directly from the URL (option 1), or first download the CSV to your computer and then load it (option 2). The link to download the data directly form NOAA is this. If for some reason this doesn’t work, download here.\n\n# option 1: load data directly from URL\n# url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url,\n#                  header=34,\n#                  na_values=[-999.99]\n#                  )\n\n# option 2: download first (use the URL above and save it to your computer), then load csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename,\n                comment='#',  # will ignore rows starting with #\n                 na_values=[-999.99]  # substitute -999.99 for NaN (Not a Number), data not available\n                 )\n# check how the dataframe (table) looks like\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      decimal\n      average\n      ndays\n      1 year ago\n      10 years ago\n      increase since 1800\n    \n  \n  \n    \n      0\n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5\n      NaN\n      NaN\n      50.39\n    \n    \n      1\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.05\n    \n    \n      2\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.59\n    \n    \n      3\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.64\n    \n    \n      4\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2566\n      2023\n      7\n      23\n      2023.5575\n      421.28\n      4\n      418.03\n      397.30\n      141.60\n    \n    \n      2567\n      2023\n      7\n      30\n      2023.5767\n      420.83\n      6\n      418.10\n      396.80\n      141.69\n    \n    \n      2568\n      2023\n      8\n      6\n      2023.5959\n      420.02\n      6\n      417.36\n      395.65\n      141.41\n    \n    \n      2569\n      2023\n      8\n      13\n      2023.6151\n      418.98\n      4\n      417.25\n      395.24\n      140.89\n    \n    \n      2570\n      2023\n      8\n      20\n      2023.6342\n      419.31\n      2\n      416.64\n      395.22\n      141.71\n    \n  \n\n2571 rows × 9 columns"
  },
  {
    "objectID": "basics/example.html#dealing-with-dates",
    "href": "basics/example.html#dealing-with-dates",
    "title": "3  learn by example",
    "section": "3.4 dealing with dates",
    "text": "3.4 dealing with dates\nCreate a new column called date, that combines the information from three separate columns: year, month, day.\n\n# function to_datetime translates the full date into a pandas datetime object,\n# that is, pandas knows this is a date, it's not just a string\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n# make 'date' column the dataframe index\ndf = df.set_index('date')\n# now see if everything is ok\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      decimal\n      average\n      ndays\n      1 year ago\n      10 years ago\n      increase since 1800\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1974-05-19\n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5\n      NaN\n      NaN\n      50.39\n    \n    \n      1974-05-26\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.05\n    \n    \n      1974-06-02\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.59\n    \n    \n      1974-06-09\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.64\n    \n    \n      1974-06-16\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-07-23\n      2023\n      7\n      23\n      2023.5575\n      421.28\n      4\n      418.03\n      397.30\n      141.60\n    \n    \n      2023-07-30\n      2023\n      7\n      30\n      2023.5767\n      420.83\n      6\n      418.10\n      396.80\n      141.69\n    \n    \n      2023-08-06\n      2023\n      8\n      6\n      2023.5959\n      420.02\n      6\n      417.36\n      395.65\n      141.41\n    \n    \n      2023-08-13\n      2023\n      8\n      13\n      2023.6151\n      418.98\n      4\n      417.25\n      395.24\n      140.89\n    \n    \n      2023-08-20\n      2023\n      8\n      20\n      2023.6342\n      419.31\n      2\n      416.64\n      395.22\n      141.71\n    \n  \n\n2571 rows × 9 columns"
  },
  {
    "objectID": "basics/example.html#first-plot",
    "href": "basics/example.html#first-plot",
    "title": "3  learn by example",
    "section": "3.5 first plot",
    "text": "3.5 first plot\nWe are now ready for our first plot! Let’s see the weekly CO2 average.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()"
  },
  {
    "objectID": "basics/example.html#first-plot-v2.0",
    "href": "basics/example.html#first-plot-v2.0",
    "title": "3  learn by example",
    "section": "3.6 first plot, v2.0",
    "text": "3.6 first plot, v2.0\nThe dates in the x-label are not great. Let’s try to make them prettier.\nWe need to import a few more packages first.\n\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()  # datetime converter for a matplotlib\n\nNow let’s replot.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2011-12-25', 389),  xycoords='data',\n    xytext=(-10, -80), textcoords='offset points',\n    arrowprops=dict(arrowstyle=\"->\",\n                    color=\"black\",\n                    connectionstyle=\"arc3,rad=0.2\"))\nfig.savefig(\"CO2-graph.png\", dpi=300)\n\n/var/folders/hc/jhnmlst937d27zzq9fhfks780000gn/T/ipykernel_10652/850389963.py:42: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.savefig(\"CO2-graph.png\", dpi=300)\n/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nThe dates on the horizontal axis are determined thus:\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nThis deremines the location of the ticks (between 3 and 5 ticks, whatever “works best”)\nax1.xaxis.set_major_locator(locator)\nThis actually puts the ticks in the positions determined above\nformatter = mdates.ConciseDateFormatter(locator)\nThis says that the labels will be placed at the locations determined in 1.\nax1.xaxis.set_major_formatter(formatter)\nFinally, labels are written down\n\nThe arrow is placed in the graph using annotate. It has a tricky syntax and a million options. Read Jake VanderPlas’s excellent examples to learn more."
  },
  {
    "objectID": "basics/example.html#modifications",
    "href": "basics/example.html#modifications",
    "title": "3  learn by example",
    "section": "3.7 modifications",
    "text": "3.7 modifications\nLet’s change a lot of plotting options to see how things could be different.\n\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,4)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"tab:blue\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax2.set(xlabel=\"date\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=5, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2010-12-25', 395),  xycoords='data',\n    xytext=(-100, 40), textcoords='offset points',\n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"white\"),\n    arrowprops=dict(arrowstyle=\"->\",\n                    color=\"black\",\n                    connectionstyle=\"angle,angleA=0,angleB=-90,rad=40\"))\n\nText(-100, 40, '2010/11')\n\n\n\n\n\nThe main changes were:\n\nUsing the Seaborn package, we changed the fontsize and the overall plot style. Read more.\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\nWe changed the colors of the lineplots. To know what colors exist, click here.\nThe arrow annotation has a different style. Read more."
  },
  {
    "objectID": "basics/example.html#playing-with-the-code",
    "href": "basics/example.html#playing-with-the-code",
    "title": "3  learn by example",
    "section": "3.8 playing with the code",
    "text": "3.8 playing with the code\nI encourage you to play with the code you just ran. An easy way of learning what each line does is to comment something out and see what changes in the output you see. If you feel brave, try to modify the code a little bit."
  },
  {
    "objectID": "basics/AI-policy.html",
    "href": "basics/AI-policy.html",
    "title": "4  AI policy",
    "section": "",
    "text": "The guidelines below are an adaptation of Ethan Mollick’s extremely useful ideas on AI as an assistant tool for teaching.\nI EXPECT YOU to use LLMs (large language models) such as ChatGPT, Bing AI, Google Bard, or whatever else springs up since the time of this writing. You should familiarize yourself with the AI’s capabilities and limitations.\nUse LLMs to help you learn, chat with them about what you want to accomplish and learn from them how to do it. Ask your LLM what each part of the code means, copy and pasting blindly is unacceptable. You are here to learn.\nConsider the following important points:\n\nUltimately, you, the student, are responsible for the assignment.\nAcknowledge the use of AI in your assignment. Be transparent about your use of the tool and the extent of assistance it provided."
  },
  {
    "objectID": "resampling/motivation.html#jerusalem-2019",
    "href": "resampling/motivation.html#jerusalem-2019",
    "title": "5  motivation",
    "section": "5.1 Jerusalem, 2019",
    "text": "5.1 Jerusalem, 2019\nData from the Israel Meteorological Service, IMS.\nSee the temperature at a weather station in Jerusalem, for the whole 2019 year. This is an interactive graph: to zoom in, play with the bottom panel.\n\n\n\n\n\n\n\n\n discussion\nThe temperature fluctuates on various time scales, from daily to yearly. Let’s think together a few questions we’d like to ask about the data above.\n\nNow let’s see precipitation data:\n\n\n\n\n\n\n\n\n discussion\nWhat would be interesting to know about precipitation?\n\nWe have not talked about what kind of data we have in our hands here. The csv file provided by the IMS looks like this:\n\n\n\n\n\n\n  \n    \n      \n      Station\n      Date & Time (Winter)\n      Diffused radiation (W/m^2)\n      Global radiation (W/m^2)\n      Direct radiation (W/m^2)\n      Relative humidity (%)\n      Temperature (°C)\n      Maximum temperature (°C)\n      Minimum temperature (°C)\n      Wind direction (°)\n      Gust wind direction (°)\n      Wind speed (m/s)\n      Maximum 1 minute wind speed (m/s)\n      Maximum 10 minutes wind speed (m/s)\n      Time ending maximum 10 minutes wind speed (hhmm)\n      Gust wind speed (m/s)\n      Standard deviation wind direction (°)\n      Rainfall (mm)\n    \n  \n  \n    \n      0\n      Jerusalem Givat Ram\n      01/01/2019 00:00\n      0.0\n      0.0\n      0.0\n      80.0\n      8.7\n      8.8\n      8.6\n      75.0\n      84.0\n      3.3\n      4.3\n      3.5\n      23:58\n      6.0\n      15.6\n      0.0\n    \n    \n      1\n      Jerusalem Givat Ram\n      01/01/2019 00:10\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      74.0\n      82.0\n      3.3\n      4.1\n      3.3\n      00:01\n      4.9\n      14.3\n      0.0\n    \n    \n      2\n      Jerusalem Givat Ram\n      01/01/2019 00:20\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      76.0\n      82.0\n      3.2\n      4.1\n      3.3\n      00:19\n      4.9\n      9.9\n      0.0\n    \n    \n      3\n      Jerusalem Givat Ram\n      01/01/2019 00:30\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.7\n      8.6\n      78.0\n      73.0\n      3.6\n      4.2\n      3.6\n      00:30\n      5.2\n      11.7\n      0.0\n    \n    \n      4\n      Jerusalem Givat Ram\n      01/01/2019 00:40\n      0.0\n      0.0\n      0.0\n      79.0\n      8.6\n      8.7\n      8.5\n      80.0\n      74.0\n      3.6\n      4.4\n      3.8\n      00:35\n      5.4\n      10.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      52549\n      Jerusalem Givat Ram\n      31/12/2019 22:20\n      0.0\n      0.0\n      1.0\n      81.0\n      7.4\n      7.6\n      7.3\n      222.0\n      255.0\n      0.5\n      0.9\n      1.0\n      22:11\n      1.0\n      47.9\n      0.0\n    \n    \n      52550\n      Jerusalem Givat Ram\n      31/12/2019 22:30\n      0.0\n      0.0\n      1.0\n      83.0\n      7.3\n      7.4\n      7.3\n      266.0\n      259.0\n      0.6\n      0.8\n      0.6\n      22:28\n      1.1\n      22.8\n      0.0\n    \n    \n      52551\n      Jerusalem Givat Ram\n      31/12/2019 22:40\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.3\n      331.0\n      317.0\n      0.5\n      0.8\n      0.6\n      22:35\n      1.0\n      31.6\n      0.0\n    \n    \n      52552\n      Jerusalem Givat Ram\n      31/12/2019 22:50\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.4\n      312.0\n      285.0\n      0.6\n      1.0\n      0.6\n      22:50\n      1.4\n      31.3\n      0.0\n    \n    \n      52553\n      Jerusalem Givat Ram\n      31/12/2019 23:00\n      0.0\n      0.0\n      1.0\n      83.0\n      7.6\n      7.7\n      7.4\n      315.0\n      321.0\n      0.7\n      1.0\n      0.8\n      22:54\n      1.3\n      23.5\n      0.0\n    \n  \n\n52554 rows × 18 columns\n\n\n\nWe see that we have data points spaced out evenly every 10 minutes."
  },
  {
    "objectID": "resampling/motivation.html#challenges",
    "href": "resampling/motivation.html#challenges",
    "title": "5  motivation",
    "section": "5.2 Challenges",
    "text": "5.2 Challenges\nLet’s try to answer the following questions:\n\n\n\n\n\n\n What is the mean temperature for each month?\n\n\n\n\n\nFirst we have to divide temperature data by month, and then take the average for each month.\n\n\na possible solution\n\ndf_month = df['temperature'].resample('M').mean()\n\n\n\n\n\n\n\n\n\n\n For each month, what is the mean of the daily maximum temperature? What about the minimun?\n\n\n\n\n\nThis is a bit trickier.\n\nWe need to find the maximum/minimum temperature for each day.\nOnly then we split the daily data by month and take the average.\n\n\n\na possible solution\n\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_month['max temp'] = df_day['max temp'].resample('MS').mean()\n\n\n\n\n\n\n\n\n\n\n What is the average night temperature for every season? What about the day temperature?\n\n\n\n\n\n\nWe need to filter our data to contain only night times.\nWe need to divide rain data by seasons (3 months), and then take the mean for each season.\n\n\n\na possible solution\n\n# filter only night data\ndf_night = df.loc[((df.index.hour < 6) | (df.index.hour >= 18))]\nseason_average_night_temp = df_night['temperature'].resample('Q').mean()\n\n\n\nanother possible solution\n\n# filter using between_time\ndf_night = df.between_time('18:00', '06:00', inclusive='left')\nseason_average_night_temp = df_night['temperature'].resample('Q').mean()\n\n\n\n\n\n\n\n\n\n\n What is the daily precipitation?\n\n\n\n\n\nFirst we have to divide rain data by day, and then take the sum for each day.\n\n\na possible solution\n\ndaily_precipitation = df['rain'].resample('D').sum()\n\n\n\n\n\n\n\n\n\n\n How much rain was there every month?\n\n\n\n\n\nWe have to divide rain data by month, and then sum the totals of each month.\n\n\na possible solution\n\nmonthly_precipitation = df['rain'].resample('M').sum()\n\n\n\n\n\n\n\n\n\n\n How many rainy days were there each month?\n\n\n\n\n\n\nWe need to sum rain by day.\nWe need to count how many days are there each month where rain > 0.\n\n\n\na possible solution\n\ndaily_precipitation = df['rain'].resample('D').sum()\nonly_rainy_days = daily_precipitation.loc[daily_precipitation > 0]\nrain_days_per_month = only_rainy_days.resample('M').count()\n\n\n\n\n\n\n\n\n\n\n How many days, hours, and minutes were between the last rain of the season (Malkosh) to the first (Yoreh)?\n\n\n\n\n\n\nWe need to divide our data into two: rainy_season_1 and rainy_season_2.\nWe need to find the time of the last rain in rainy_season_1.\nWe need to find the time of the first rain in rainy_season_2.\nWe need to compute the time difference between the two dates.\n\n\n\na possible solution\n\nsplit_date = '2019-08-01'\nrainy_season_1 = df[:split_date]  # everything before split date\nrainy_season_2 = df[split_date:]  # everything after split date\nmalkosh = rainy_season_1['rain'].loc[rainy_season_1['rain'] > 0].last_valid_index()\nyoreh = rainy_season_2['rain'].loc[rainy_season_2['rain'] > 0].first_valid_index()\ndry_period = yoreh - malkosh\n# extracting days, hours, and minutes\ndays = dry_period.days\nhours = dry_period.components.hours\nminutes = dry_period.components.minutes\nprint(f'The dry period of 2019 was {days} days, {hours} hours and {minutes} minutes.')\n\n\n\n\n\n\n\n\n\n\n What was the rainiest morning (6am-12pm) of the year? Bonus, what about the rainiest night (6pm-6am)?\n\n\n\n\n\n\nWe need to filter our data to contain only morning times.\nWe need to sum rain by day.\nWe need to find the day with the maximum value.\n\n\n\na possible solution\n\n# filter to only day data\nmorning_df = df.loc[((df.index.hour >= 6) & (df.index.hour < 18))]\nmorning_rain = morning_df['rain'].resample('D').sum()\nrainiest_morning = morning_rain.idxmax()\n# plot\nmorning_rain.plot()\nplt.axvline(rainiest_morning, c='r', alpha=0.5, linestyle='--')\n\n\n\nbonus solution\n\n# filter to only night data\ndf_night = df.loc[((df.index.hour < 6) | (df.index.hour >= 18))]\n# resampling night for each day is tricky because the date changes at 12:00. We can do this trick:\n# we shift the time back by 6 hours so all the data for the same night will have the same date.\ndf_shifted = df_night.tshift(-6, freq='H')\nnight_rain = df_shifted['rain'].resample('D').sum()\nrainiest_night = night_rain.idxmax()\n# plot\nnight_rain.plot()\nplt.axvline(rainiest_night, c='r', alpha=0.5, linestyle='--')\n\n\n\n\nNote: this whole webpage is actually a Jupyter Notebook rendered as html. If you want to know how to make interactive graphs, go to the top of the page and click on “ Code”\nUseful functions compatible with pandas.resample() can be found here. The full list of resampling frequencies can be found here."
  },
  {
    "objectID": "resampling/resampling.html",
    "href": "resampling/resampling.html",
    "title": "6  resampling",
    "section": "",
    "text": "We can only really understand how to calculate monthly means if we do it ourselves.\nFirst, let’s import a bunch of packages we need to use.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\nNow we load the csv file for Jerusalem (2019), provided by the IMS.\n\ndiscussion\nWe will go to the IMS website together and see what are the options available and how to download. If you just need the csv right away, download it here.\n\n\nWe substitute every occurence of - for NaN (not a number, that is, the data is missing).\nWe call the columns Temperature (°C) and Rainfall (mm) with more convenient names, since we will be using them a lot.\nWe interpret the column Date & Time (Winter) as a date, saying to python that day comes first.\nWe make date the index of the dataframe.\n\n\nfilename = \"../archive/data/jerusalem2019.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Temperature (°C)': 'temperature',\n                   'Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      Station\n      Date & Time (Winter)\n      Diffused radiation (W/m^2)\n      Global radiation (W/m^2)\n      Direct radiation (W/m^2)\n      Relative humidity (%)\n      temperature\n      Maximum temperature (°C)\n      Minimum temperature (°C)\n      Wind direction (°)\n      Gust wind direction (°)\n      Wind speed (m/s)\n      Maximum 1 minute wind speed (m/s)\n      Maximum 10 minutes wind speed (m/s)\n      Time ending maximum 10 minutes wind speed (hhmm)\n      Gust wind speed (m/s)\n      Standard deviation wind direction (°)\n      rain\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-01 00:00:00\n      Jerusalem Givat Ram\n      01/01/2019 00:00\n      0.0\n      0.0\n      0.0\n      80.0\n      8.7\n      8.8\n      8.6\n      75.0\n      84.0\n      3.3\n      4.3\n      3.5\n      23:58\n      6.0\n      15.6\n      0.0\n    \n    \n      2019-01-01 00:10:00\n      Jerusalem Givat Ram\n      01/01/2019 00:10\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      74.0\n      82.0\n      3.3\n      4.1\n      3.3\n      00:01\n      4.9\n      14.3\n      0.0\n    \n    \n      2019-01-01 00:20:00\n      Jerusalem Givat Ram\n      01/01/2019 00:20\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      76.0\n      82.0\n      3.2\n      4.1\n      3.3\n      00:19\n      4.9\n      9.9\n      0.0\n    \n    \n      2019-01-01 00:30:00\n      Jerusalem Givat Ram\n      01/01/2019 00:30\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.7\n      8.6\n      78.0\n      73.0\n      3.6\n      4.2\n      3.6\n      00:30\n      5.2\n      11.7\n      0.0\n    \n    \n      2019-01-01 00:40:00\n      Jerusalem Givat Ram\n      01/01/2019 00:40\n      0.0\n      0.0\n      0.0\n      79.0\n      8.6\n      8.7\n      8.5\n      80.0\n      74.0\n      3.6\n      4.4\n      3.8\n      00:35\n      5.4\n      10.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2019-12-31 22:20:00\n      Jerusalem Givat Ram\n      31/12/2019 22:20\n      0.0\n      0.0\n      1.0\n      81.0\n      7.4\n      7.6\n      7.3\n      222.0\n      255.0\n      0.5\n      0.9\n      1.0\n      22:11\n      1.0\n      47.9\n      0.0\n    \n    \n      2019-12-31 22:30:00\n      Jerusalem Givat Ram\n      31/12/2019 22:30\n      0.0\n      0.0\n      1.0\n      83.0\n      7.3\n      7.4\n      7.3\n      266.0\n      259.0\n      0.6\n      0.8\n      0.6\n      22:28\n      1.1\n      22.8\n      0.0\n    \n    \n      2019-12-31 22:40:00\n      Jerusalem Givat Ram\n      31/12/2019 22:40\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.3\n      331.0\n      317.0\n      0.5\n      0.8\n      0.6\n      22:35\n      1.0\n      31.6\n      0.0\n    \n    \n      2019-12-31 22:50:00\n      Jerusalem Givat Ram\n      31/12/2019 22:50\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.4\n      312.0\n      285.0\n      0.6\n      1.0\n      0.6\n      22:50\n      1.4\n      31.3\n      0.0\n    \n    \n      2019-12-31 23:00:00\n      Jerusalem Givat Ram\n      31/12/2019 23:00\n      0.0\n      0.0\n      1.0\n      83.0\n      7.6\n      7.7\n      7.4\n      315.0\n      321.0\n      0.7\n      1.0\n      0.8\n      22:54\n      1.3\n      23.5\n      0.0\n    \n  \n\n52554 rows × 18 columns\n\n\n\nWith resample it’s easy to compute monthly averages. Resample by itself only divides the data into buckets (in this case monthly buckets), and waits for a further instruction. Here, the next instruction is mean.\n\ndf_month = df['temperature'].resample('M').mean()\ndf_month\n\ndate\n2019-01-31     9.119937\n2019-02-28     9.629812\n2019-03-31    10.731571\n2019-04-30    14.514329\n2019-05-31    22.916894\n2019-06-30    23.587361\n2019-07-31    24.019403\n2019-08-31    24.050822\n2019-09-30    22.313287\n2019-10-31    20.641868\n2019-11-30    17.257153\n2019-12-31    11.224131\nFreq: M, Name: temperature, dtype: float64\n\n\nInstead of M for month, which other options do I have? The full list can be found here, but the most commonly used are:\nM         month end frequency\nMS        month start frequency\nA         year end frequency\nAS, YS    year start frequency\nD         calendar day frequency\nH         hourly frequency\nT, min    minutely frequency\nS         secondly frequency\nThe results we got for the monthly means were given as a pandas series, not dataframe. Let’s correct this:\n\ndf_month = (df['temperature'].resample('M')         # resample by month\n                             .mean()                # take the mean\n                             .to_frame('mean temp') # make output a dafaframe\n           )\ndf_month\n\n\n\n\n\n  \n    \n      \n      mean temp\n    \n    \n      date\n      \n    \n  \n  \n    \n      2019-01-31\n      9.119937\n    \n    \n      2019-02-28\n      9.629812\n    \n    \n      2019-03-31\n      10.731571\n    \n    \n      2019-04-30\n      14.514329\n    \n    \n      2019-05-31\n      22.916894\n    \n    \n      2019-06-30\n      23.587361\n    \n    \n      2019-07-31\n      24.019403\n    \n    \n      2019-08-31\n      24.050822\n    \n    \n      2019-09-30\n      22.313287\n    \n    \n      2019-10-31\n      20.641868\n    \n    \n      2019-11-30\n      17.257153\n    \n    \n      2019-12-31\n      11.224131\n    \n  \n\n\n\n\n\nhot tip\nSometimes, a line of code can get too long and messy. In the code above, we broke line for every step, which makes the process so much cleaner. We highly advise you to do the same. Attention: This trick works as long as all the elements are inside the same parenthesis.\n\nNow it’s time to plot!\n\nfig, ax = plt.subplots()\nax.plot(df_month['mean temp'], color='black')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\")\n\n[Text(0, 0.5, 'Temperature (°C)'),\n [<matplotlib.axis.YTick at 0x7faf784c6d60>,\n  <matplotlib.axis.YTick at 0x7faf7843a220>,\n  <matplotlib.axis.YTick at 0x7faf784c62b0>,\n  <matplotlib.axis.YTick at 0x7faf784f3400>,\n  <matplotlib.axis.YTick at 0x7faf784f3760>,\n  <matplotlib.axis.YTick at 0x7faf784fa5b0>],\n Text(0.5, 1.0, 'Jerusalem, 2019')]\n\n\n\n\n\nThe dates in the horizontal axis are not great. An easy fix is to use the month numbers instead of dates. \n\nfig, ax = plt.subplots()\nax.plot(df_month.index.month, df_month['mean temp'], color='black')\nax.set(xlabel=\"month\",\n       ylabel='Temperature (°C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\",);\n\n\n\n\n\ndiscussion\nWhen you have datetime as the dataframe index, you don’t need to give the function plot two arguments, date and values. You can just tell plot to use the column you want, the function will take the dates by itself.\n What does this line mean?\ndf_month['mean temp'].index.month\nPrint on the screen the following, and see yourself what each thing is:\n\ndf_month\ndf_month.index\ndf_month.index.month\ndf_month.index.day\n\n\nWe’re done! Congratulations :)\nNow we need to calculate the average minimum/maximum daily temperatures. We start by creating an empty dataframe.\n\ndf_day = pd.DataFrame()\n\nNow resample data by day (D), and take the min/max of each day.\n\ndf_day['min temp'] = df['temperature'].resample('D').min()\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_day\n\n\n\n\n\n  \n    \n      \n      min temp\n      max temp\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2019-01-01\n      7.5\n      14.1\n    \n    \n      2019-01-02\n      6.6\n      11.5\n    \n    \n      2019-01-03\n      6.3\n      10.7\n    \n    \n      2019-01-04\n      6.6\n      14.6\n    \n    \n      2019-01-05\n      7.0\n      11.4\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2019-12-27\n      4.4\n      7.4\n    \n    \n      2019-12-28\n      6.6\n      10.3\n    \n    \n      2019-12-29\n      8.1\n      12.5\n    \n    \n      2019-12-30\n      6.9\n      13.0\n    \n    \n      2019-12-31\n      5.2\n      13.3\n    \n  \n\n365 rows × 2 columns\n\n\n\nThe next step is to calculate the average minimum/maximum for each month. This is similar to what we did above.\n\ndf_month['min temp'] = df_day['min temp'].resample('M').mean()\ndf_month['max temp'] = df_day['max temp'].resample('M').mean()\ndf_month\n\n\n\n\n\n  \n    \n      \n      mean temp\n      min temp\n      max temp\n    \n    \n      date\n      \n      \n      \n    \n  \n  \n    \n      2019-01-31\n      9.119937\n      5.922581\n      12.470968\n    \n    \n      2019-02-28\n      9.629812\n      6.825000\n      13.089286\n    \n    \n      2019-03-31\n      10.731571\n      7.532258\n      14.661290\n    \n    \n      2019-04-30\n      14.514329\n      10.866667\n      19.113333\n    \n    \n      2019-05-31\n      22.916894\n      17.296774\n      29.038710\n    \n    \n      2019-06-30\n      23.587361\n      19.163333\n      28.860000\n    \n    \n      2019-07-31\n      24.019403\n      19.367742\n      29.564516\n    \n    \n      2019-08-31\n      24.050822\n      19.903226\n      29.767742\n    \n    \n      2019-09-30\n      22.313287\n      18.430000\n      28.456667\n    \n    \n      2019-10-31\n      20.641868\n      16.945161\n      26.190323\n    \n    \n      2019-11-30\n      17.257153\n      14.066667\n      21.436667\n    \n    \n      2019-12-31\n      11.224131\n      8.806452\n      14.448387\n    \n  \n\n\n\n\nLet’s plot…\n\nfig, ax = plt.subplots()\nax.plot(df_month['max temp'], color='tab:red', label='max')\nax.plot(df_month['mean temp'], color='black', label='mean')\nax.plot(df_month['min temp'], color='tab:blue', label='min')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(10,35,5),\n       title=\"Jerusalem, 2019\")\nax.xaxis.set_major_locator(mdates.MonthLocator(range(1, 13, 2), bymonthday=15))\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nax.legend(fontsize=12, frameon=False);\n\n\n\n\nVoilà! You made a beautiful graph!\n\ndiscussion\nThis time we did not put month numbers in the horizontal axis, we now have month names. How did we do this black magic, you ask? See lines 8–10 above. Matplotlib gives you absolute power over what to put in the axis, if you can only know how to tell it to… Wanna know more? Click here."
  },
  {
    "objectID": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "href": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "title": "7  upsampling",
    "section": "7.1 Potential Evapotranspiration using Penman’s equation",
    "text": "7.1 Potential Evapotranspiration using Penman’s equation\nWe want to calculate the daily potential evapotranspiration using Penman’s equation. Part of the calculation involves characterizing the energy budget on soil surface. When direct solar radiation measurements are not available, we can estimate the energy balance by knowing the “cloudless skies mean solar radiation”, \\(R_{so}\\). This is the amount of energy (MJ/m\\(^2\\)/d) that hits the surface, assuming no clouds. This radiation depends on the season and on the latitude you are. For Israel, located at latitude 32° N, we can use the following data for 30°:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\ndates = pd.date_range(start='2021-01-01', periods=13, freq='MS')\nvalues = [17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04, 17.46]\ndf = pd.DataFrame({'date': dates, 'radiation': values})\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      radiation\n    \n    \n      date\n      \n    \n  \n  \n    \n      2021-01-01\n      17.46\n    \n    \n      2021-02-01\n      21.65\n    \n    \n      2021-03-01\n      25.96\n    \n    \n      2021-04-01\n      29.85\n    \n    \n      2021-05-01\n      32.11\n    \n    \n      2021-06-01\n      33.20\n    \n    \n      2021-07-01\n      32.66\n    \n    \n      2021-08-01\n      30.44\n    \n    \n      2021-09-01\n      26.67\n    \n    \n      2021-10-01\n      22.48\n    \n    \n      2021-11-01\n      18.30\n    \n    \n      2021-12-01\n      16.04\n    \n    \n      2022-01-01\n      17.46\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None')\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nWe only have 12 values for the whole year, and we can’t use this dataframe to compute daily ET. We need to upsample!\nIn the example below, we resample the monthly data into daily data, and do nothing else. Pandas doesn’t know what to do with the new points, so it fills them with NaN.\n\ndf_nan = df['radiation'].resample('D').asfreq().to_frame()\ndf_nan.head(33)\n\n\n\n\n\n  \n    \n      \n      radiation\n    \n    \n      date\n      \n    \n  \n  \n    \n      2021-01-01\n      17.46\n    \n    \n      2021-01-02\n      NaN\n    \n    \n      2021-01-03\n      NaN\n    \n    \n      2021-01-04\n      NaN\n    \n    \n      2021-01-05\n      NaN\n    \n    \n      2021-01-06\n      NaN\n    \n    \n      2021-01-07\n      NaN\n    \n    \n      2021-01-08\n      NaN\n    \n    \n      2021-01-09\n      NaN\n    \n    \n      2021-01-10\n      NaN\n    \n    \n      2021-01-11\n      NaN\n    \n    \n      2021-01-12\n      NaN\n    \n    \n      2021-01-13\n      NaN\n    \n    \n      2021-01-14\n      NaN\n    \n    \n      2021-01-15\n      NaN\n    \n    \n      2021-01-16\n      NaN\n    \n    \n      2021-01-17\n      NaN\n    \n    \n      2021-01-18\n      NaN\n    \n    \n      2021-01-19\n      NaN\n    \n    \n      2021-01-20\n      NaN\n    \n    \n      2021-01-21\n      NaN\n    \n    \n      2021-01-22\n      NaN\n    \n    \n      2021-01-23\n      NaN\n    \n    \n      2021-01-24\n      NaN\n    \n    \n      2021-01-25\n      NaN\n    \n    \n      2021-01-26\n      NaN\n    \n    \n      2021-01-27\n      NaN\n    \n    \n      2021-01-28\n      NaN\n    \n    \n      2021-01-29\n      NaN\n    \n    \n      2021-01-30\n      NaN\n    \n    \n      2021-01-31\n      NaN\n    \n    \n      2021-02-01\n      21.65\n    \n    \n      2021-02-02\n      NaN"
  },
  {
    "objectID": "resampling/upsampling.html#forwardbackward-fill",
    "href": "resampling/upsampling.html#forwardbackward-fill",
    "title": "7  upsampling",
    "section": "7.2 Forward/Backward fill",
    "text": "7.2 Forward/Backward fill\nWe can forward/backward fill these NaNs:\n\ndf_forw = df['radiation'].resample('D').ffill().to_frame()\ndf_back = df['radiation'].resample('D').bfill().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_forw['radiation'], color='tab:blue', label=\"forward fill\")\nax.plot(df_back['radiation'], color='tab:orange', label=\"backward fill\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThis does the job, but I want something better, not step functions. The radiation should vary smoothly from day to day. Let’s use interpolation."
  },
  {
    "objectID": "resampling/upsampling.html#interpolation",
    "href": "resampling/upsampling.html#interpolation",
    "title": "7  upsampling",
    "section": "7.3 Interpolation",
    "text": "7.3 Interpolation\n\ndf_linear = df['radiation'].resample('D').interpolate(method='time').to_frame()\ndf_cubic = df['radiation'].resample('D').interpolate(method='cubic').to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_linear['radiation'], color='tab:blue', label=\"linear interpolation\")\nax.plot(df_cubic['radiation'], color='tab:orange', label=\"cubic interpolation\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThere are many ways to fill NaNs and to interpolate. A nice detailed guide can be found here."
  },
  {
    "objectID": "resampling/interpolation.html",
    "href": "resampling/interpolation.html",
    "title": "8  interpolation",
    "section": "",
    "text": "Interpolation is the act of getting data you don’t have from data you alreay have. We used some interpolation when upsampling, and now it is time to talk about it a little bit more in depth.\nThere is no one correct way of interpolating, the method you use depends in the end on what you want to accomplish, what are your (hidden or explicit) assumptions, etc. Let’s see a few examples."
  },
  {
    "objectID": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "href": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "title": "9  FAQ",
    "section": "9.1 How to resample by year, but have it end in September?",
    "text": "9.1 How to resample by year, but have it end in September?\nThis is called anchored offset. One possible use to it is to calculate statistics according to the hydrological year that, for example, ends in September.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\nfilename = \"../archive/data/Kinneret_Kvuza_daily_rainfall.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Date': 'date',\n                   'Daily Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace\ndf\n\n\n\n\n\n  \n    \n      \n      Station\n      rain\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      1980-01-02\n      Kinneret Kvuza 09/1977-08/2023\n      0.0\n    \n    \n      1980-01-03\n      0\n      0.0\n    \n    \n      1980-01-04\n      0\n      0.0\n    \n    \n      1980-01-05\n      Kinneret Kvuza 09/1977-08/2023\n      35.5\n    \n    \n      1980-01-06\n      Kinneret Kvuza 09/1977-08/2023\n      2.2\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2019-12-26\n      Kinneret Kvuza 09/1977-08/2023\n      39.4\n    \n    \n      2019-12-27\n      Kinneret Kvuza 09/1977-08/2023\n      5.2\n    \n    \n      2019-12-28\n      Kinneret Kvuza 09/1977-08/2023\n      1.6\n    \n    \n      2019-12-29\n      0\n      0.0\n    \n    \n      2019-12-30\n      Kinneret Kvuza 09/1977-08/2023\n      0.1\n    \n  \n\n14608 rows × 2 columns\n\n\n\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(df['rain'], color='black')\nax[1].plot(df.loc['1998':'2000', 'rain'], color='black')\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\nfig.text(0.02, 0.5, 'daily precipitation (mm)', va='center', rotation='vertical')\nax[0].set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')\n\n\n\n\n\nWe see a marked dry season during the summer, so let’s assume the Hydrological Year ends in September.\n\ndf_year = df.resample('A-SEP').sum()\ndf_year = df_year.loc['1980':'2003']\ndf_year\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_94063/2047090134.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_year = df.resample('A-SEP').sum()\n\n\n\n\n\n\n  \n    \n      \n      rain\n    \n    \n      date\n      \n    \n  \n  \n    \n      1980-09-30\n      355.5\n    \n    \n      1981-09-30\n      463.1\n    \n    \n      1982-09-30\n      221.7\n    \n    \n      1983-09-30\n      557.1\n    \n    \n      1984-09-30\n      335.3\n    \n    \n      1985-09-30\n      379.8\n    \n    \n      1986-09-30\n      300.7\n    \n    \n      1987-09-30\n      424.7\n    \n    \n      1988-09-30\n      421.6\n    \n    \n      1989-09-30\n      251.6\n    \n    \n      1990-09-30\n      432.5\n    \n    \n      1991-09-30\n      328.3\n    \n    \n      1992-09-30\n      738.4\n    \n    \n      1993-09-30\n      434.9\n    \n    \n      1994-09-30\n      255.4\n    \n    \n      1995-09-30\n      408.6\n    \n    \n      1996-09-30\n      373.0\n    \n    \n      1997-09-30\n      416.2\n    \n    \n      1998-09-30\n      451.9\n    \n    \n      1999-09-30\n      227.8\n    \n    \n      2000-09-30\n      378.9\n    \n    \n      2001-09-30\n      273.9\n    \n    \n      2002-09-30\n      445.2\n    \n    \n      2003-09-30\n      602.4\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(df_year.index, df_year['rain'], color='black',\n       width=365)\nax.set_ylabel(\"yearly precipitation (mm)\")\nax.set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')"
  },
  {
    "objectID": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "href": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "title": "9  FAQ",
    "section": "9.2 When upsampling, how to fill missing values with zero?",
    "text": "9.2 When upsampling, how to fill missing values with zero?\nWe did that in the example above, like this:\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace"
  },
  {
    "objectID": "smoothing/motivation.html#tumbling-vs-sliding",
    "href": "smoothing/motivation.html#tumbling-vs-sliding",
    "title": "10  motivation",
    "section": "10.1 Tumbling vs Sliding",
    "text": "10.1 Tumbling vs Sliding"
  },
  {
    "objectID": "smoothing/sliding.html#convolution",
    "href": "smoothing/sliding.html#convolution",
    "title": "11  sliding window",
    "section": "11.1 convolution",
    "text": "11.1 convolution\nConvolution is a fancy word for averaging a time series using a sliding window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/sliding.html#kernels",
    "href": "smoothing/sliding.html#kernels",
    "title": "11  sliding window",
    "section": "11.2 kernels",
    "text": "11.2 kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation (the window in the animation is 4 standard deviations wide)."
  },
  {
    "objectID": "smoothing/sliding.html#math",
    "href": "smoothing/sliding.html#math",
    "title": "11  sliding window",
    "section": "11.3 math",
    "text": "11.3 math\nThe definition of a convolution between signal \\(f(t)\\) and kernel \\(k(t)\\) is\n\\[\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\\]\nThe expression \\(f*k\\) denotes the convolution of these two functions. The argument of \\(k\\) is \\(t-\\tau\\), meaning that the kernel runs from left to right (as \\(t\\) does), and at every point the two signals (\\(f\\) and \\(k\\)) are multiplied together. It is the product of the signal with the weight function \\(k\\) that gives us an average. Because of \\(-\\tau\\), the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\\[\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\\]\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/sliding.html#numerics",
    "href": "smoothing/sliding.html#numerics",
    "title": "11  sliding window",
    "section": "11.4 numerics",
    "text": "11.4 numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes\ndf['temperature'].rolling(window='20', center=True).mean()\n\nwindow=20 means that the width of the window is 20 points. Pandas lets us define a window width in time units, for example, window='120min'.\ncenter=True is needed in order to assign the result of averaging to the center of the window. Make it False and see what happens.\nmean() is the actual calculation, the average of temperature over the window. The rolling part does not compute anything, it just creates a moving window, and we are free to calculate whatever we want. Try to calculate the standard deviation or the maximum, for example.\n\nIt is implicit in the command above a “rectangular” kernel. What if we want other shapes?\n\n11.4.1 gaussian\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere\n\nwindow_width is an integer, number of points in your window\nstd_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\n\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package. The graph above was achieved by running:\ng = scipy.signal.gaussian(window_width, std)\nplt.plot(g)\n\n\n11.4.2 triangular\nSame idea as gaussian, but simpler, because we don’t need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/sliding.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/sliding.html#which-window-shape-and-width-to-choose",
    "title": "11  sliding window",
    "section": "11.5 which window shape and width to choose?",
    "text": "11.5 which window shape and width to choose?\n🤷‍♂️\nSorry, there is not definite answer here… It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above.\n\nOne important question you need to ask is: what are the time scales associated with the processes I’m interested in? For example, if I’m interested in the daily temperature pattern, getting rid of 1-minute-long fluctuations would probably be ok. On the other hand, if we were to smooth the signal so much that all that can be seen are the temperature changes between summer and winter, then my smoothing got out of hand, and I threw away the very process I wanted to study.\nAll this is to say that you need to know in advance a few things about the system you are studying, otherwise you can’t know what is “noise” that can be smoothed away."
  },
  {
    "objectID": "smoothing/perfect-smoother.html",
    "href": "smoothing/perfect-smoother.html",
    "title": "13  a perfect smoother",
    "section": "",
    "text": "Source: Eilers (2003)\nGitHub repository\nNoisy series \\(y\\) of length \\(m\\).\nThe smoothed series is called \\(z\\).\nWe have conflicting interests:\n\nwe want a \\(z\\) series “as smooth as possible”.\nhowever, the smoother \\(z\\) is, the farthest from \\(y\\) it will be (low fidelity).\n\nRoughness:\n\\[\nR = \\displaystyle\\sum_i (z_i - z_{i-1})^2\n\\]\nFit to data:\n\\[\nS = \\displaystyle\\sum_i (y_i - z_i)^2\n\\]\nCost functional to be minimized:\n\\[\nQ = S + \\lambda R\n\\]\n\n\n\n\nEilers, Paul HC. 2003. “A Perfect Smoother.” Analytical Chemistry 75 (14): 3631–36. https://doi.org/10.1021/ac034173t."
  },
  {
    "objectID": "outliers/motivation.html",
    "href": "outliers/motivation.html",
    "title": "14  motivation",
    "section": "",
    "text": "Outliers are observations significantly different from all other observations. Consider, for example, this temperature graph:\n\nWhile most measured points are between 20 and 30 °C, there is obviously something very wrong with the one data point above 80 °C.\nHow could such a thing come about? This could be the result of non-natural causes, such as measurement errors, wrong data collection, or wrong data entry. On the other hand, this point could have natural sources, such as a very hot spark flying next to the temperature sensor.\nIdentifying outliers is important, because they might greatly impact measures like mean and standard deviation. When left untouched, outliers might make us reach wrong conclusions about our data. See what happens to the slope of this linear regression with and without the outliers.\n\n\n\nSource: Zhang (2020)\n\n\n\n\n\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.” ouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "outliers/zscore.html",
    "href": "outliers/zscore.html",
    "title": "15  Z-score",
    "section": "",
    "text": "\\[\nz  = \\frac{x-\\mu}{\\sigma},\n\\]\n\n\nwhere\n\n\\(x=\\) data point,\n\n\\(\\mu=\\) time series mean\n\n\\(\\sigma=\\) time series standard deviation.\n\nLet’s write a function that identifies outliers according to the Z-score.\ndef zscore(df, degree=3):\n    data = df.copy()\n    data['zscore'] = (data - data.mean())/data.std()\n    outliers = data[(data['zscore'] <= -degree) | (data['zscore'] >= degree)]\n    return outliers['value'], data\nNow we can simply use this function:\nthreshold = 2.5\noutliers, transformed = zscore(tx, threshold)\nSource: Atwan (2022)\n\n\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook: Practical Recipes for Exploratory Data Analysis, Data Preparation, Forecasting, and Model Evaluation. Packt."
  },
  {
    "objectID": "outliers/IQR.html",
    "href": "outliers/IQR.html",
    "title": "16  IQR (Inter Quartile Range)",
    "section": "",
    "text": "The IQR (Inter Quartile Range) is the distance between the 25th percentile (Q1) and the 75th percentile (Q3). In a box plot, the whiskers usually extend \\(1.5\\times\\)IQR beyond Q1 and Q3, see below.\n\n\n\nSource: McDonald (2022)\n\n\nA common outlier detection method is to consider whatever points outside the whisker range as outliers.\n\n\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python Library.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57."
  },
  {
    "objectID": "best-practices/date-formatting.html",
    "href": "best-practices/date-formatting.html",
    "title": "18  date formatting",
    "section": "",
    "text": "Here you will find several examples of how to format dates in your plots. Not many explanations are provided.\nHow to use this page? Find first an example of a plot you like, only then go to the code and see how it’s done.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\n\n\nimport pandas as pd\n\nstart_date = '2018-01-01'\nend_date = '2018-04-30'\n\n# create date range with 1-hour intervals\ndates = pd.date_range(start_date, end_date, freq='1H')\n# create a random variable to plot\nvar = np.random.rand(len(dates)) - 0.51\nvar = var.cumsum()\nvar = var - var.min()\n# create dataframe, make \"date\" the index\ndf = pd.DataFrame({'date': dates, 'variable': var})\ndf.set_index(df['date'], inplace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2018-01-01 00:00:00\n      2018-01-01 00:00:00\n      28.317035\n    \n    \n      2018-01-01 01:00:00\n      2018-01-01 01:00:00\n      28.120523\n    \n    \n      2018-01-01 02:00:00\n      2018-01-01 02:00:00\n      28.596894\n    \n    \n      2018-01-01 03:00:00\n      2018-01-01 03:00:00\n      28.931941\n    \n    \n      2018-01-01 04:00:00\n      2018-01-01 04:00:00\n      28.561778\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2018-04-29 20:00:00\n      2018-04-29 20:00:00\n      1.914343\n    \n    \n      2018-04-29 21:00:00\n      2018-04-29 21:00:00\n      1.648757\n    \n    \n      2018-04-29 22:00:00\n      2018-04-29 22:00:00\n      1.992956\n    \n    \n      2018-04-29 23:00:00\n      2018-04-29 23:00:00\n      1.500860\n    \n    \n      2018-04-30 00:00:00\n      2018-04-30 00:00:00\n      1.650439\n    \n  \n\n2857 rows × 2 columns\n\n\n\ndefine a useful function to plot the graphs below\n\ndef explanation(ax, text, letter):\n    ax.text(0.99, 0.97, text,\n            transform=ax.transAxes,\n            horizontalalignment='right', verticalalignment='top',\n            fontweight=\"bold\")\n    ax.text(0.01, 0.01, letter,\n            transform=ax.transAxes,\n            horizontalalignment='left', verticalalignment='bottom',\n            fontweight=\"bold\")\n    ax.set(ylabel=\"variable (units)\")\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nax.plot(df['variable'])\nplt.gcf().autofmt_xdate()  # makes slated dates\nexplanation(ax, \"slanted dates\", \"\")\nfig.savefig(\"dates1.png\")\n\n\n\n\n\nfig, ax = plt.subplots(4, 1, figsize=(10, 16),\n                       gridspec_kw={'hspace': 0.3})\n\n### plot a ###\nax[0].plot(df['variable'])\ndate_form = DateFormatter(\"%b\")\nax[0].xaxis.set_major_locator(mdates.MonthLocator(interval=2))\nax[0].xaxis.set_major_formatter(date_form)\n\n### plot b ###\nax[1].plot(df['variable'])\ndate_form = DateFormatter(\"%B\")\nax[1].xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax[1].xaxis.set_major_formatter(date_form)\n\n### plot c ###\nax[2].plot(df['variable'])\nax[2].xaxis.set_major_locator(mdates.MonthLocator())\n# 16 is a slight approximation for the center, since months differ in number of days.\nax[2].xaxis.set_minor_locator(mdates.MonthLocator(bymonthday=16))\nax[2].xaxis.set_major_formatter(ticker.NullFormatter())\nax[2].xaxis.set_minor_formatter(DateFormatter('%B'))\nfor tick in ax[2].xaxis.get_minor_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n\n### plot d ###\nax[3].plot(df['variable'])\ndate_form = DateFormatter(\"%d %b\")\nax[3].xaxis.set_major_locator(mdates.DayLocator(interval=15))\nax[3].xaxis.set_major_formatter(date_form)\n\nexplanation(ax[0], \"month abbreviations, every 2 months\", \"a\")\nexplanation(ax[1], \"full month names\", \"b\")\nexplanation(ax[2], \"full month names centered between the 1st of the month\", \"c\")\nexplanation(ax[3], \"day + month abbr. --- every 15 days\", \"d\")\n\nfig.savefig(\"dates2.png\")\n\n\n\n\n\nfig, ax = plt.subplots(4, 1, figsize=(10, 16),\n                       gridspec_kw={'hspace': 0.3})\n\n### plot e ###\nax[0].plot(df['variable'])\ndate_form = DateFormatter(\"%d/%m\")\nax[0].xaxis.set_major_locator(mdates.DayLocator(bymonthday=[5, 20]))\nax[0].xaxis.set_major_formatter(date_form)\n\n### plot f ###\nax[1].plot(df['variable'])\nlocator = mdates.AutoDateLocator(minticks=11, maxticks=17)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\n\n### plot g ###\nax[2].plot(df.loc['2018-01-01':'2018-03-01', 'variable'])\nlocator = mdates.AutoDateLocator(minticks=6, maxticks=14)\nformatter = mdates.ConciseDateFormatter(locator)\nax[2].xaxis.set_major_locator(locator)\nax[2].xaxis.set_major_formatter(formatter)\n\n### plot h ###\nax[3].plot(df.loc['2018-01-01':'2018-01-02', 'variable'])\nlocator = mdates.AutoDateLocator(minticks=6, maxticks=10)\nformatter = mdates.ConciseDateFormatter(locator)\nax[3].xaxis.set_major_locator(locator)\nax[3].xaxis.set_major_formatter(formatter)\n\nexplanation(ax[0], \"exactly on days 05 and 20 of each month\", \"e\")\nexplanation(ax[1], \"ConciseDateFormatter\", \"f\")\nexplanation(ax[2], \"ConciseDateFormatter\", \"g\")\nexplanation(ax[3], \"ConciseDateFormatter\", \"h\")\n\nfig.savefig(\"dates3.png\")\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4),\n                       gridspec_kw={'hspace': 0.3})\n\n# import constants for the days of the week\nfrom matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\nax.plot(df['variable'])\n# tick on sundays every third week\nloc = mdates.WeekdayLocator(byweekday=SU, interval=3)\nax.xaxis.set_major_locator(loc)\ndate_form = DateFormatter(\"%a, %b %d\")\nax.xaxis.set_major_formatter(date_form)\nfig.autofmt_xdate(bottom=0.2, rotation=30, ha='right')\nexplanation(ax, \"every 3 Sundays, rotate labels\", \"\")\n\n\n\n\n\n\n\nCode\nExplanation\n\n\n\n\n%Y\n4-digit year (e.g., 2022)\n\n\n%y\n2-digit year (e.g., 22)\n\n\n%m\n2-digit month (e.g., 12)\n\n\n%B\nFull month name (e.g., December)\n\n\n%b\nAbbreviated month name (e.g., Dec)\n\n\n%d\n2-digit day of the month (e.g., 09)\n\n\n%A\nFull weekday name (e.g., Tuesday)\n\n\n%a\nAbbreviated weekday name (e.g., Tue)\n\n\n%H\n24-hour clock hour (e.g., 23)\n\n\n%I\n12-hour clock hour (e.g., 11)\n\n\n%M\n2-digit minute (e.g., 59)\n\n\n%S\n2-digit second (e.g., 59)\n\n\n%p\n“AM” or “PM”\n\n\n%Z\nTime zone name\n\n\n%z\nTime zone offset from UTC (e.g., -0500)"
  },
  {
    "objectID": "stationarity/autocorrelation.html#question",
    "href": "stationarity/autocorrelation.html#question",
    "title": "21  autocorrelation",
    "section": "21.1 question",
    "text": "21.1 question\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nTo answer this, we need to talk about autocorrelation. Let’s start by introducing the necessary concepts."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "21  autocorrelation",
    "section": "21.2 mean and standard deviation",
    "text": "21.2 mean and standard deviation\nLet’s call our time series from above \\(X\\), and its length \\(N\\). Then:\n\\[\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\\]\nThe mean and standard deviation can be visualized thus:\n\nOne last basic concept we need is the expected value: \\[\nE[X] = \\sum_{i=1}^N X_i p_i\n\\]\nFor our time series, the probability \\(p_i\\) that a given point \\(X_i\\) is in the dataset is simply \\(1/N\\), therefore the expectation becomes\n\\[\nE[X] = \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N}\n\\]"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "21  autocorrelation",
    "section": "21.3 autocorrelation",
    "text": "21.3 autocorrelation\nThe autocorrelation of a time series \\(X\\) is the answer to the following question:\n\nif we shift \\(X\\) by \\(\\tau\\) units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are \\(X(t)\\) and \\(X(t+\\tau)\\)?\n\nUsing the Pearson correlation coefficient\n\n\nPearson correlation coefficient between \\(X\\) and \\(Y\\): \\[\n\\rho_{X,Y} = \\frac{E\\left[ (X - \\mu_X)(X_Y - \\mu_Y) \\right]}{\\sigma_X\\sigma_Y}\n\\]\nwe get\n\\[\n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\\]\nA video is worth a billion words, so let’s see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\(\\tau=0\\) (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "23  cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "25  LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "frequency/fourier.html#basic-wave-concepts",
    "href": "frequency/fourier.html#basic-wave-concepts",
    "title": "27  Fourier transform",
    "section": "27.1 basic wave concepts",
    "text": "27.1 basic wave concepts\nThe function\n\\[\nf(t) = B\\sin(2\\pi f t)\n\\tag{27.1}\\]\nhas two basic characteristics, its amplitude \\(B\\) and frequency \\(f\\).\n\nIn the figure above, the amplitude \\(B=0.6\\) and we see that the distance between two peaks is called period, \\(T=2\\) s. The frequency is defined as the inverse of the period:\n\\[\nf = \\frac{1}{T}.\n\\tag{27.2}\\]\nWhen time is in seconds, then the frequency is measured in Hertz (Hz). For the graph above, therefore, we see a wave whose frequency is \\(f = 1/(2 \\text{ s}) = 0.5\\) Hz.\nIn the figure below, we see what happens when we vary the values of the frequency and amplitude.\n\nThe graph above introduces two new characteristics of a wave, its phase \\(\\phi\\), and its offset \\(B\\). A more general description of a sine wave is\n\\[\nf(t) = B\\sin(2\\pi f t + \\phi) + B_0.\n\\tag{27.3}\\]\nThe offset \\(B_0\\) moves the wave up and down, while changing the value of \\(\\phi\\) makes the sine wave move left and right. When the phase \\(\\phi=2\\pi\\), the sine wave will have shifted a full period, and the resulting wave is identical to the original:\n\\[\nB\\sin(2\\pi f t) = B\\sin(2\\pi f t + 2\\pi).\n\\tag{27.4}\\]\nAll the above can also be said about a cosine, whose general for can be given as\n\\[\nA\\cos(2\\pi f t + \\phi) + A_0\n\\tag{27.5}\\]\nOne final point before we jump into the deep waters is that the sine and cosine functions are related through a simple phase shift:\n\\[\n\\cos\\left(2\\pi f t + \\frac{\\pi}{2}\\right) = \\sin\\left(2\\pi f t\\right)\n\\]"
  },
  {
    "objectID": "frequency/fourier.html#fouriers-theorem",
    "href": "frequency/fourier.html#fouriers-theorem",
    "title": "27  Fourier transform",
    "section": "27.2 Fourier’s theorem",
    "text": "27.2 Fourier’s theorem\nFourier’s theorem states that\n\nAny periodic signal is composed of a superposition of pure sine waves, with suitably chosen amplitudes and phases, whose frequencies are harmonics of the fundamental frequency of the signal.\n\nSee the following animations to visualize the theorem in action.\n\nSource: https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Fourier_synthesis_square_wave_animated.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Sawtooth_Fourier_Animation.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Continuous_Fourier_transform_of_rect_and_sinc_functions.gif"
  },
  {
    "objectID": "frequency/fourier.html#fourier-series",
    "href": "frequency/fourier.html#fourier-series",
    "title": "27  Fourier transform",
    "section": "27.3 Fourier series",
    "text": "27.3 Fourier series\n\na periodic function can be described as a sum of sines and cosines.\n\n\n\nNot any function, but certainly most functions we will deal with in this course. The function has to fullful the Dirichlet conditions\nThe classic examples are usually the square function and the sawtooth function:\n\n\n[Source: https://www.geogebra.org/m/tkajbzmg]\nhttps://www.geogebra.org/m/k4eq4fkr\n\n\n\\[\nF[x(t)] = F(f) = \\int_{-\\infty}^{\\infty}x(t)e^{-2\\pi i f t}dt\n\\]\n\\[\nf(t) = \\int_{-\\infty}^{\\infty}F(f)e^{2\\pi i f t}df\n\\]\nhttps://dibsmethodsmeetings.github.io/fourier-transforms/\nhttps://www.jezzamon.com/fourier/index.html"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "href": "seasonality/seasonal-decomposition.html#trends-in-atmospheric-carbon-dioxide",
    "title": "31  seasonal decomposition",
    "section": "31.1 trends in atmospheric carbon dioxide",
    "text": "31.1 trends in atmospheric carbon dioxide\nMauna Loa CO2 concentration.\ndata from NOAA\n\nurl = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url, header=47, na_values=[-999.99])\n\n# you can first download, and then read the csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename, header=35, na_values=[-999.99])\n\ndf\n\n\n\n\n\n  \n    \n      \n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5.1\n      -999.99\n      -999.99.1\n      50.39\n    \n  \n  \n    \n      0\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.05\n    \n    \n      1\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.59\n    \n    \n      2\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.64\n    \n    \n      3\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n    \n    \n      4\n      1974\n      6\n      23\n      1974.4753\n      331.73\n      5\n      NaN\n      NaN\n      49.72\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2565\n      2023\n      7\n      23\n      2023.5575\n      421.28\n      4\n      418.03\n      397.30\n      141.60\n    \n    \n      2566\n      2023\n      7\n      30\n      2023.5767\n      420.83\n      6\n      418.10\n      396.80\n      141.69\n    \n    \n      2567\n      2023\n      8\n      6\n      2023.5959\n      420.02\n      6\n      417.36\n      395.65\n      141.41\n    \n    \n      2568\n      2023\n      8\n      13\n      2023.6151\n      418.98\n      4\n      417.25\n      395.24\n      140.89\n    \n    \n      2569\n      2023\n      8\n      20\n      2023.6342\n      419.31\n      2\n      416.64\n      395.22\n      141.71\n    \n  \n\n2570 rows × 9 columns\n\n\n\n\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      decimal\n      average\n      ndays\n      1 year ago\n      10 years ago\n      increase since 1800\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1974-05-19\n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5\n      NaN\n      NaN\n      50.40\n    \n    \n      1974-05-26\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.06\n    \n    \n      1974-06-02\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.60\n    \n    \n      1974-06-09\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.65\n    \n    \n      1974-06-16\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-06-26\n      2022\n      6\n      26\n      2022.4836\n      420.31\n      7\n      418.14\n      395.36\n      138.71\n    \n    \n      2022-07-03\n      2022\n      7\n      3\n      2022.5027\n      419.73\n      6\n      417.49\n      395.15\n      138.64\n    \n    \n      2022-07-10\n      2022\n      7\n      10\n      2022.5219\n      419.08\n      6\n      417.25\n      394.59\n      138.52\n    \n    \n      2022-07-17\n      2022\n      7\n      17\n      2022.5411\n      418.43\n      6\n      417.14\n      394.64\n      138.41\n    \n    \n      2022-07-24\n      2022\n      7\n      24\n      2022.5603\n      417.84\n      6\n      415.68\n      394.11\n      138.36\n    \n  \n\n2515 rows × 9 columns\n\n\n\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(1, figsize=(8,6))\nax.plot(df['average'])\nax.set(xlabel=\"date\",\n       ylabel=\"CO2 concentration (ppm)\",\n       # ylim=[0, 430],\n       title=\"Mauna Loa CO2 concentration\");\n\nKeyError: 'average'\n\n\n\n\n\nfill missing data. interpolate method: ‘time’\ninterpolation methods visualized\n\ndf['co2'] = (df['average'].resample(\"D\") #resample daily\n                          .interpolate(method='time') #interpolate by time\n            )\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      decimal\n      average\n      ndays\n      1 year ago\n      10 years ago\n      increase since 1800\n      co2\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1974-05-19\n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5\n      NaN\n      NaN\n      50.40\n      333.37\n    \n    \n      1974-05-26\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.06\n      332.95\n    \n    \n      1974-06-02\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.60\n      332.35\n    \n    \n      1974-06-09\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.65\n      332.20\n    \n    \n      1974-06-16\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n      332.37\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-06-26\n      2022\n      6\n      26\n      2022.4836\n      420.31\n      7\n      418.14\n      395.36\n      138.71\n      420.31\n    \n    \n      2022-07-03\n      2022\n      7\n      3\n      2022.5027\n      419.73\n      6\n      417.49\n      395.15\n      138.64\n      419.73\n    \n    \n      2022-07-10\n      2022\n      7\n      10\n      2022.5219\n      419.08\n      6\n      417.25\n      394.59\n      138.52\n      419.08\n    \n    \n      2022-07-17\n      2022\n      7\n      17\n      2022.5411\n      418.43\n      6\n      417.14\n      394.64\n      138.41\n      418.43\n    \n    \n      2022-07-24\n      2022\n      7\n      24\n      2022.5603\n      417.84\n      6\n      415.68\n      394.11\n      138.36\n      417.84\n    \n  \n\n2515 rows × 10 columns"
  },
  {
    "objectID": "seasonality/seasonal-decomposition.html#decompose-data",
    "href": "seasonality/seasonal-decomposition.html#decompose-data",
    "title": "31  seasonal decomposition",
    "section": "31.2 decompose data",
    "text": "31.2 decompose data\nseasonal_decompose returns an object with four components:\n\nobserved: \\(Y(t)\\)\ntrend: \\(T(t)\\)\nseasonal: \\(S(t)\\)\nresid: \\(e(t)\\)\n\nAdditive model: \\[\nY(t) = T(t) + S(t) + e(t)\n\\]\nMultiplicative model: \\[\nY(t) = T(t) \\times S(t) \\times e(t)\n\\]\n\n31.2.0.1 Interlude\nlearn how to use zip in a loop\n\nletters = ['a', 'b', 'c', 'd', 'e']\nnumbers = [1, 2, 3, 4, 5]\n# zip let's us iterate over to lists at the same time\nfor l, n in zip(letters, numbers):\n    print(f\"{l} = {n}\")\n\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n\nPlot each component separately.\n\n# %matplotlib widget\n\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\ndecomposed_m = seasonal_decompose(df['co2'], model='multiplicative')\ndecomposed_a = seasonal_decompose(df['co2'], model='additive')\ndecomposed = decomposed_m\npos = (0.5, 0.9)\ncomponents =[\"observed\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(decomposed, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\n\n\n\n\n\n# %matplotlib widget\n\ndecomposed = decomposed_m\n\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df['co2'], color=\"tab:blue\", label=\"observed\")\nax[0].plot(decomposed.trend * decomposed.resid, color=\"tab:orange\", label=\"trend*resid\")\nax[0].plot(decomposed.trend * decomposed.seasonal, color=\"tab:red\", label=\"trend*seasonal\")\nax[0].plot(decomposed.trend, color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"CO$_2$ concentration (ppm)\",\n          title=\"Mauna Loa CO$_2$ concentration\")\nax[0].legend(frameon=False)\n\nstart = \"2000-01-01\"\nend = \"2003-01-01\"\nzoom = slice(start, end)\nax[1].plot(df.loc[zoom, 'co2'], color=\"tab:blue\", label=\"observed\")\nax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=\"tab:orange\", label=\"trend*resid\")\nax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=\"tab:red\", label=\"trend*seasonal\")\nax[1].plot(decomposed.trend[zoom], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"Components, 2000--2003\");"
  },
  {
    "objectID": "rates-of-change/motivation.html",
    "href": "rates-of-change/motivation.html",
    "title": "33  motivation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n%matplotlib widget\n\n\nfilename = \"../archive/data/kinneret_cleaned.csv\"\ndf = pd.read_csv(filename)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      level\n    \n    \n      date\n      \n    \n  \n  \n    \n      2023-09-12\n      -211.115\n    \n    \n      2023-09-11\n      -211.105\n    \n    \n      2023-09-10\n      -211.095\n    \n    \n      2023-09-09\n      -211.085\n    \n    \n      2023-09-08\n      -211.070\n    \n    \n      ...\n      ...\n    \n    \n      1966-11-01\n      -210.390\n    \n    \n      1966-10-15\n      -210.320\n    \n    \n      1966-10-01\n      -210.270\n    \n    \n      1966-09-15\n      -210.130\n    \n    \n      1966-09-01\n      -210.020\n    \n  \n\n10286 rows × 1 columns\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['level'], color=\"tab:blue\")\nax.set(title=\"Kinneret Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThe data seems ok, until we take a closer look. Data points are not evenly spaced in time.\n\nfig, ax = plt.subplots()\nax.plot(df.loc[\"1993\":\"1995\", 'level'], color=\"tab:blue\", marker=\"o\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_3777/934261896.py:2: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n  ax.plot(df.loc[\"1993\":\"1995\", 'level'], color=\"tab:blue\", marker=\"o\")\n\n\n\n\n\nWe can resample by day (a much higher rate than the original), and linearly interpolate:\n\ndf2 = df['level'].resample('D').interpolate('time').to_frame()\ndf2['level_sm'] = df2['level'].rolling('30D', center=True).mean()\ndf3 = df2['level'].resample('W').mean().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df2.loc[\"1993\":\"1995\", 'level_sm'],\n        color=\"tab:red\",\n        label=\"daily resapled\")\nax.plot(df3.loc[\"1993\":\"1995\", 'level'],\n        color=\"black\",\n        label=\"daily resapled\")\nax.plot(df2.loc[\"1993\":\"1995\", 'level'],\n        color=\"tab:orange\",\n        label=\"daily resapled\")\nax.plot(df.loc[\"1993\":\"1995\", 'level'],\n        color=\"tab:blue\",\n        marker=\"o\",\n        linestyle=\"None\",\n        label=\"original\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\nax.legend(frameon=False)\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_3777/2583247388.py:11: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n  ax.plot(df.loc[\"1993\":\"1995\", 'level'],\n\n\n<matplotlib.legend.Legend at 0x7fa71e8b0bb0>\n\n\n\n\n\n\ndf2['naive'] = df2['level'].diff()\ndf2['gradient'] = np.gradient(df2['level'])\n\ndf3['naive'] = df3['level'].diff()\ndf3['gradient'] = np.gradient(df3['level'])\n\n\nfig, ax = plt.subplots()\nax.plot(df3.loc[\"1980\":\"2020\", 'naive'], color=\"tab:blue\")\nax.plot(df3.loc[\"1980\":\"2020\", 'gradient'], color=\"tab:red\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\n\n[Text(0.5, 1.0, 'Dead Sea Level'), Text(0, 0.5, 'level (m)')]\n\n\n\n\n\n\ndf3 = df2[\"level\"].rolling('365.24D', center=True).mean().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df3.loc[\"1980\":\"2020\", 'level'], color=\"tab:blue\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\n\n[Text(0.5, 1.0, 'Dead Sea Level'), Text(0, 0.5, 'level (m)')]"
  },
  {
    "objectID": "rates-of-change/finite-differences.html",
    "href": "rates-of-change/finite-differences.html",
    "title": "35  finite differences",
    "section": "",
    "text": "Definition of a derivative:\n\\[\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n\\]\nNumerically, we can approximate the derivative \\(f'(t)\\) of a time series \\(f(t)\\) as\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{35.1}\\]\n\n\nThe expression \\(\\mathcal{O}(\\Delta t)\\) means that the error associated with the approximation is proportional to \\(\\Delta t\\). This is called “Big O notation”.\nThe expression above is called the two-point forward difference formula. Likewise, we can define the two-point backward difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{35.2}\\]\nIf we sum together Equation 35.1 and Equation 35.2 we get:\n\n\\[\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n&= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n\\tag{35.3}\\]\nDividing both sides by 2 gives the two-point central difference formula:\n\\[\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n\\tag{35.4}\\]\nTwo things are worth mentioning about the approximation above:\n\nit is balanced, that is, there is no preference of the future over the past.\nits error is proportional to \\(\\Delta t^2\\), it is a lot more precise than the unbalanced approximations :)\n\n\n\nTo understand why the error is proportional to \\(\\Delta t^2\\), one can subtract the Taylor expansion of \\(f(t-\\Delta t)\\) from the Taylor expansion of \\(f(t+\\Delta t)\\). See this, pages 3 and 4.\n\nThe function np.gradient calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n\nThe “gradient” usually refers to a first derivative with respect to space, and it is denoted as \\(\\nabla f(x)=\\frac{df(x)}{dx}\\). However, it doesn’t really matter if we call the independent variable \\(x\\) or \\(t\\), the derivative operator is exactly the same.\nCheck out this nice example."
  },
  {
    "objectID": "rates-of-change/fourier-based-derivatives.html",
    "href": "rates-of-change/fourier-based-derivatives.html",
    "title": "36  Fourier-based derivatives",
    "section": "",
    "text": "This tutorial is based on Pelliccia (2019).\nnice trick: https://math.stackexchange.com/questions/430858/fourier-transform-of-derivative\n\n\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing Method.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  },
  {
    "objectID": "assignments/assignment1.html#task",
    "href": "assignments/assignment1.html#task",
    "title": "40  assignment 1",
    "section": "40.1 Task",
    "text": "40.1 Task\nGo to the IMS website, and choose another weather station we have not worked with yet. Download 10-minute data for a full year, any year.\nMake 3 graphs:\n\nDaily maximum humidity. Bonus: add another line to the graph, the daily minimum humidity.\nThe number of rainy dais for each month\nFor each day of the year, show the number of hours when global solar radiation was above, on average, the threshold 10 W/m\\(^2\\). Now add another line, for the threshold 500 W/m\\(^2\\).\n\nMake 5 more graphs (total of 8 graphs) of whatever you find interesting. You have the liberty to explore various facets of your dataset that capture your interest. It’s essential, however, to maintain a focus on resampling. Each of your plots should effectively showcase and emphasize different aspects or techniques of resampling in your data analysis. To ensure diversity in your visualizations, avoid repetitive themes; for instance, if your first plot illustrates daily wind speed, then your second plot should not simply be a monthly resampling of wind speed. Aim for variety and innovation in each plot to fully explore the potential of resampling in data visualization.\nYou must download this Jupyter Notebook template. Create a zip file with your Jupyter notebook and with the .csv you used. Upload this zip file to the moodle task we created."
  },
  {
    "objectID": "assignments/assignment1.html#guidelines",
    "href": "assignments/assignment1.html#guidelines",
    "title": "40  assignment 1",
    "section": "40.2 Guidelines",
    "text": "40.2 Guidelines\n\nAlways name the axes and add units when relevant.\nAlways give a title to the plot.\nMake sure that all axis tick labels (the numbers/dates on the axes) are readable.\nInclude a legend if you have multiple lines, colors, or groups.\nUse appropriate scales for the axes (linear, logarithmic, etc.) depending on the data’s nature.\nEnsure that the plot is adequately sized for all elements to be clear and visible.\nChoose colors and markers that are distinguishable, especially for plots with multiple elements.\nIf applicable, include error bars to indicate the variability or uncertainty in the data.\nUse grid lines sparingly; they should not overshadow the data."
  },
  {
    "objectID": "assignments/assignment1.html#evaluation",
    "href": "assignments/assignment1.html#evaluation",
    "title": "40  assignment 1",
    "section": "40.3 Evaluation",
    "text": "40.3 Evaluation\nAll your assignments will be evaluated according to the following criteria:\n\n40% Presentation. How the graphs look, labels, general organization, markdown, clean code.\n30% Discussion. This is where you explain what you did, what you found out, etc.\n15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration.\n10% Replicability: Your code runs flawlessly.\n5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself!\nBonus: 10% for originality, creative problem solving, or notable analysis."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "43  technical stuff",
    "section": "43.1 operating systems",
    "text": "43.1 operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "43  technical stuff",
    "section": "43.2 software",
    "text": "43.2 software\nAnaconda’s Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "43  technical stuff",
    "section": "43.3 python packages",
    "text": "43.3 python packages\nKats — a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "references.html#books",
    "href": "references.html#books",
    "title": "sources",
    "section": "books",
    "text": "books\nfrom Data to Viz\nFundamentals of Data Visualization, by Claus O. Wilke\nPyNotes in Agriscience\nForecasting: Principles and Practice (3rd ed), by Rob J Hyndman and George Athanasopoulos\nPython for Finance Cookbook 2nd Edition - Code Repository\nPractical time series analysis,: prediction with statistics and machine learning, by Aileen Nielsen\nThe online edition of this book is available for Hebrew University staff and students.\nTime series analysis with Python cookbook : practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation, by Tarek A. Atwan\nThe online edition of this book is available for Hebrew University staff and students.\nHands-on Time Series Analysis with Python: From Basics to Bleeding Edge Techniques, by B V Vishwas, Ashish Patel\nThe online edition of this book is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#videos",
    "href": "references.html#videos",
    "title": "sources",
    "section": "videos",
    "text": "videos\nTimes Series Analysis for Everyone, by Bruno Goncalves\nThis series is available for Hebrew University staff and students.\nTime Series Analysis with Pandas, by Joshua Malina This video is available for Hebrew University staff and students."
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "sources",
    "section": "references",
    "text": "references\n\n\nAtwan, Tarek A. 2022. Time Series Analysis with Python Cookbook:\nPractical Recipes for Exploratory Data Analysis, Data Preparation,\nForecasting, and Model Evaluation. Packt.\n\n\nEilers, Paul HC. 2003. “A Perfect Smoother.” Analytical\nChemistry 75 (14): 3631–36. https://doi.org/10.1021/ac034173t.\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python\nLibrary.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57.\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing\nMethod.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/.\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.”\nouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "things-to-remember.html",
    "href": "things-to-remember.html",
    "title": "stuff",
    "section": "",
    "text": "Hey! What are you doing here? This page is not meant for you 😜\n\nThis is not a coding course. Neither a programming course. Our goal is to learn how to think about time series. Of course, we want to tell the computer how to do stuff, and that requires programming.\n“How do you do?” How do you do what?!\nWelche Sprachen sprechen Sie?\nFirst do, then get use to it.\nOnce you got used to doing, go back and understand better."
  }
]