[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "about\nWelcome to Time Series Analysis for Environmental Sciences (71606) at the Hebrew University of Jerusalem. This is Yair Mau, your host for today. I am a senior lecturer at the Institute of Environmental Sciences, at the Faculty of Agriculture, Food and Environment, in Rehovot, Israel.\nThis website contains (almost) all the material you’ll need for the course. If you find any mistakes, or have any comments, please email me."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Time Series Analysis",
    "section": "disclaimer",
    "text": "disclaimer\n\nThe material here is not comprehensive and does not constitute a stand alone course in Time Series Analysis. This is only the support material for the actual presential course I give."
  },
  {
    "objectID": "index.html#what-who-when-and-where",
    "href": "index.html#what-who-when-and-where",
    "title": "Time Series Analysis",
    "section": "what, who, when and where?",
    "text": "what, who, when and where?\n Course number 71606, 3 academic points\n Yair Mau (lecturer), Erez Feuer (TA)\n Tuesdays, from 14:15 to 17:00\n Computer classroom #18\n Office hours: upon request"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Time Series Analysis",
    "section": "syllabus",
    "text": "syllabus\n\ncourse description\nData analysis of time series, with practical examples from environmental sciences.\n\n\ncourse aims\nThis course aims at giving the students a broad overview of the main steps involved in the analysis of time series: data management, data wrangling, visualization, analysis, and forecast. The course will provide a hands-on approach, where students will actively engage with real-life datasets from the field of environmental science.\n\n\nlearning outcomes\nOn successful completion of this module,students should be able to:\n\nExplore a time-series dataset, while formulating interesting questions.\nChoose the appropriate tools to attack the problem and answer the questions.\nCommunicate their findings and the methods they used to achieve them, using graphs, statistics, text, and a well-documented code.\n\n\n\ncourse content\n\nData wrangling: organization, cleaning, merging, filling gaps, excluding outliers, smoothing, resampling.\nVisualization: best practices for graph making using leading python libraries.\nAnalysis: stationarity, seasonality, (auto)correlations, lags, derivatives, spectral analysis.\nForecast: ARIMA\nData management: how to plan ahead and best organize large quantities of data. If there is enough time, we will build a simple time-series database.\n\n\n\nbooks and other sources\nClick here.\n\n\ncourse evaluation\nThere will be assignments during the semester (totaling 50% of the final grade), and one final project (50%).\n\n\nEvaluation policy\n\nIndividual Work: While we support helping your peers, it’s important to remember that all assignments must be completed individually. This means that your submissions should be your own unique work and not contain code or text that is identical to someone else’s.\nZero Plagiarism: Do not copy text verbatim from any source. Always express ideas in your own words.\nOn-Time Submission: Assignments must be turned in by the specified deadline. Late submissions will receive a grade of 0. If you require an extension, requests will only be considered if made at least 24 hours before the due date.\nNon-Compliance Consequence: Assignments that do not adhere to these guidelines will automatically receive a grade of 0."
  },
  {
    "objectID": "index.html#weekly-program",
    "href": "index.html#weekly-program",
    "title": "Time Series Analysis",
    "section": "weekly program",
    "text": "weekly program\n\nThis year’s course will be a bit different that planned due to the shortening of the academic semester. The information below is NOT up to date. Ask Yair what is relevant this year.\n\n\nweek 1\n\nLecture: Course overview, setting of expectations. Introduction, basic concepts, continuous vs discrete time series, sampling, aliasing\nExercise: Loading csv file into python, basic time series manipulation with pandas and plotting\n\n\n\nweek 2\n\nLecture: Filling gaps, removing outliers\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity\n\n\n\nweek 3\n\nLecture: Interpolation, resampling, binning statistics\nExercise: Practice the same topics learned during the lecture. Data: air temperature and relative humidity, precipitation\n\n\n\nweek 4\n\nLecture: Time series plotting: best practices. Dos and don’ts and maybes\nExercise: Practice with Seaborn, Plotly, Pandas, Matplotlib\n\n\nProject 1\nBasic data wrangling, using real data (temperature, relative humidity, precipitation) downloaded from USGS. 25% of the final grade\n\n\n\nweek 5\n\nLecture: Smoothing, running averages, convolution\nExercise: Practice the same topics learned during the lecture. Data: sap flow, evapotranspiration\n\n\n\nweek 6\n\nLecture: Strong and weak stationarity, stochastic processes, auto-correlation\nExercise: Practice the same topics learned during the lecture. Data: temperature and wind speed\n\n\n\nweek 7\n\nLecture: Correlation between signals. Pearson correlation, time-lagged cross-correlations, dynamic time warping\nExercise: Practice the same topics learned during the lecture. Data: temperature, solar radiation, relative humidity, soil moisture, evapotranspiration\n\n\n\nweek 8\nSame as lecture 7 above\n\n\nweek 9\n\nLecture: Download data from repositories, using API, merging, documentation\nExercise: Download data from USGS, NOAA, Fluxnet, Israel Meteorological Service\n\n\nProject 2\nStudents will study a Fluxnet site of their choosing. How do gas fluxes (CO2, H2O) depend on environmental conditions? 25% of the final grade\n\n\n\nweek 10\n\nLecture: Fourier decomposition, filtering, Nyquist–Shannon sampling theorem\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 11\n\nLecture: Seasonality, seasonal decomposition (trend, seasonal, residue), Hilbert transform\nExercise: Practice the same topics learned during the lecture. Data: monthly atmospheric CO2 concentration, hourly air temperature\n\n\n\nweek 12\n\nLecture: Derivatives, differencing\nExercise: Practice the same topics learned during the lecture. Data: dendrometer data\n\n\n\nweek 13\n\nLecture: Forecasting. ARIMA\nExercise: Practice the same topics learned during the lecture. Data: vegetation variables (sap flow, ET, DBH, etc)\n\n\nFinal Project\nIn consultation with the lecturer, students will ask a specific scientific question about a site of their choosing (from NOAA, USGS, Fluxnet), and answer it using the tools learned during the semester. The report will be written in Jupyter Notebook, combining in one document all the calculations, documentation, figures, analysis, and discussion. 50% of the final grade."
  },
  {
    "objectID": "who-cares.html#why-time-series-analysis",
    "href": "who-cares.html#why-time-series-analysis",
    "title": "who cares?",
    "section": "why “Time Series Analysis?”",
    "text": "why “Time Series Analysis?”\n\nTime has two aspects. There is the arrow, the running river, without which there is no change, no progress, or direction, or creation. And there is the circle or the cycle, without which there is chaos, meaningless succession of instants, a world without clocks or seasons or promises.\nURSULA K. LE GUIN\n\nYou are here because you are interested in how things change, evolve. In this course I want to discuss with you how to make sense of data whose temporal nature is in its very essence. We will talk about randomness, cycles, frequencies, correlations, and more."
  },
  {
    "objectID": "who-cares.html#why-environmental-sciences",
    "href": "who-cares.html#why-environmental-sciences",
    "title": "who cares?",
    "section": "why “Environmental Sciences”",
    "text": "why “Environmental Sciences”\nThis same time series analysis (TSA) course could be called instead “TSA for finance”, “TSA for Biology”, or any other application. The emphasis in this course is not Environmental Sciences, but the concepts and tools of TSA. Because my research is in Environmental Science, and many of the graduate students at HUJI-Rehovot research this, I chose to use examples “close to home”. The same toolset should be useful for students of other disciplines."
  },
  {
    "objectID": "who-cares.html#what-is-it-good-for",
    "href": "who-cares.html#what-is-it-good-for",
    "title": "who cares?",
    "section": "what is it good for?",
    "text": "what is it good for?\nIn many fields of science we are flooded by data, and it’s hard to see the forest for the trees. I hope that the topics we’ll discuss in this course can help you find meaningful patterns in your data, formulate interesting hypotheses, and design better experiments."
  },
  {
    "objectID": "who-cares.html#do-i-need-it",
    "href": "who-cares.html#do-i-need-it",
    "title": "who cares?",
    "section": "do I need it?",
    "text": "do I need it?\nMaybe. If you are a grad student and you have temporal data to analyze, then probably yes. However, I have very fond memories of courses that I took as a grad student that were completely unrelated to my research. Sometimes “because it’s fun” is a perfectly good answer."
  },
  {
    "objectID": "who-cares.html#what-will-i-actually-gain-from-it",
    "href": "who-cares.html#what-will-i-actually-gain-from-it",
    "title": "who cares?",
    "section": "what will I actually gain from it?",
    "text": "what will I actually gain from it?\nBy the end of this course you will have gained:\n\na hands-on experience of fundamental time-series analysis tools\nan intuition regarding the basic concepts\ntechnical abilities\na springboard for learning more about the subject by yourself"
  },
  {
    "objectID": "basics/boring.html#anaconda",
    "href": "basics/boring.html#anaconda",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.1 Anaconda",
    "text": "1.1 Anaconda\nInstall Anaconda’s Python distribution. The Anaconda installation brings with it all the main python packages we will need to use. In order to install extra packages, refer to these two tutorials: tutorial 1, tutorial 2."
  },
  {
    "objectID": "basics/boring.html#vscode",
    "href": "basics/boring.html#vscode",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.2 VSCode",
    "text": "1.2 VSCode\nInstall VSCode. Visual Studio Code is a very nice IDE (Integrated Development Environment) made by Microsoft, available to all operating systems. Contrary to the title of this page, it is not absolutely necessary to use it, but I like VSCode, and as my student, so do you 😉."
  },
  {
    "objectID": "basics/boring.html#jupyter-notebooks",
    "href": "basics/boring.html#jupyter-notebooks",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.3 jupyter notebooks",
    "text": "1.3 jupyter notebooks\nWe will code exclusively in Jupyter Notebooks. Get acquainted with them. Make sure you can point VSCode to the Anaconda environment of your choice (“base” by default). Don’t worry, this is easier than it sounds.\nOne failproof way of making sure VSCode uses the Anaconda installation is the following:\n\nOpen Anaconda Navigator\nIf you are using HUJI’s computers, in “Environments”, choose “asgard”. If you are using your own computer, ignore this step.\nopen VSCode from inside Anaconda Navigator (see image below).\n\n\nSometimes you will need to manualy install the Jupyter extension on VSCode. In this case follow this tutorial."
  },
  {
    "objectID": "basics/boring.html#folder-structure",
    "href": "basics/boring.html#folder-structure",
    "title": "1  the boring stuff you absolutely need to do",
    "section": "1.4 folder structure",
    "text": "1.4 folder structure\nYou NEED to be confortable with you computer’s folder (or directory) structure. Where are files located? How to navigate through different folders? How is my stuff organized? If you don’t feel absolutely comfortable with this, then read this, Windows, MacOS. If you use Linux then you surely know this stuff. Make yourself a “time-series” folder wherever you want, and have it backed up regularly (use Google Drive, Dropbox, do it manually, etc). “My dog deleted my files” is not an excuse."
  },
  {
    "objectID": "basics/numpy-pandas-matplotlib.html#pandas",
    "href": "basics/numpy-pandas-matplotlib.html#pandas",
    "title": "2  numpy, pandas, matplotlib",
    "section": "2.1 pandas",
    "text": "2.1 pandas\nWe will primarily use the Pandas package to deal with data. Pandas has become the standard Python tool to manipulate time series, and you should get acquainted with its basic usage. This course will provide you the opportunity to learn by example, but I’m sure we will only scratch the surface, and you’ll be left with lots of questions.\nI provide below a (non-comprehensive) list of useful tutorials, they are a good reference for the beginner and for the experienced user.\n\nPython Data Science Handbook, by Jake VanderPlas\nData Wrangling with pandas Cheat Sheet\nWorking with Dates and Times in Python\nCheat Sheet: The pandas DataFrame Object\nYouTube tutorials by Corey Schafer"
  },
  {
    "objectID": "basics/numpy-pandas-matplotlib.html#pyplot",
    "href": "basics/numpy-pandas-matplotlib.html#pyplot",
    "title": "2  numpy, pandas, matplotlib",
    "section": "2.2 pyplot",
    "text": "2.2 pyplot\nMatplotlib, and its submodule pyplot, are probably the most common Python plotting tool. Pyplot is both great and horrible:\n\nGreat: you’ll have absolutely full control of everything you want to plot. The sky is the limit.\nHorrible: you’ll cry as you do it, because there is so much to know, and it is not the most friendly plotting package.\n\nPyplot is object oriented, so you will usually manipulate the axes object like this.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 2, 0, 3]\n\n# Figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 6))\n# plot on the left\nax1.plot(x, y, color=\"tab:blue\")\nax1.plot(x, y[::-1], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n        ylabel=\"something\",\n        title=\"left panel\")\n# plot on the right\nax2.plot(x, y[::-1])\nax2.set(xlabel=\"date\",\n        ylabel=\"something else\",\n        title=\"right panel\")\n\n[Text(0.5, 0, 'date'),\n Text(0, 0.5, 'something else'),\n Text(0.5, 1.0, 'right panel')]\n\n\n\n\n\nFor the very beginners, you need to know that figure refers to the whole white canvas, and axes means the rectangle inside which something will be plotted:\n\nThe image above is good because it has 2 panels, and it’s easy to understand what going on. Sadly, they mixed the two terms, axis and axes.\n\naxes is where the whole plot will be drawn. In the figure above it is the same as each panel.\naxis is each of the vertical and horizontal lines, where you have ticks and numbers.\n\n\nIf you are new to all this, I recommend that you go to:\n\nEarth Lab’s Introduction to Plotting in Python Using Matplotlib\nJake VanderPlas’s Python Data Science Handbook"
  },
  {
    "objectID": "basics/example.html#open-a-new-jupyter-notebook",
    "href": "basics/example.html#open-a-new-jupyter-notebook",
    "title": "3  learn by example",
    "section": "3.1 open a new Jupyter Notebook",
    "text": "3.1 open a new Jupyter Notebook\n\nOn your computer, open the program Anaconda Navigator (it may take a while to load).\nFind the white box called VS Code and click Launch.\nNow go to File > Open Folder, and open the folder you created for this course. VS Code may ask you if you trust the authors, and the answer is “yes” (it’s your computer).\nFile > New File, and call it example.ipynb\nYou can start copying and pasting code from this website to your Jupyter Notebook. To run a cell, press Shift+Enter.\nYou may be asked to choose to Select Kernel. This is VS Code wanting to know which python installation to use. Click on “Python Environments”, and then choose the option with the word anaconda in it.\nThat’s all! Congratulations!"
  },
  {
    "objectID": "basics/example.html#import-packages",
    "href": "basics/example.html#import-packages",
    "title": "3  learn by example",
    "section": "3.2 import packages",
    "text": "3.2 import packages\nFirst, import packages to be used. They should all be already included in the Anaconda distribution you installed.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters"
  },
  {
    "objectID": "basics/example.html#load-data",
    "href": "basics/example.html#load-data",
    "title": "3  learn by example",
    "section": "3.3 load data",
    "text": "3.3 load data\nLoad CO2 data into a Pandas dataframe. You can load it directly from the URL (option 1), or first download the CSV to your computer and then load it (option 2). The link to download the data directly form NOAA is this. If for some reason this doesn’t work, download here.\n\n# option 1: load data directly from URL\n# url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n# df = pd.read_csv(url,\n#                  header=34,\n#                  na_values=[-999.99]\n#                  )\n\n# option 2: download first (use the URL above and save it to your computer), then load csv\nfilename = \"co2_weekly_mlo.csv\"\ndf = pd.read_csv(filename,\n                comment='#',  # will ignore rows starting with #\n                 na_values=[-999.99]  # substitute -999.99 for NaN (Not a Number), data not available\n                 )\n# check how the dataframe (table) looks like\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      decimal\n      average\n      ndays\n      1 year ago\n      10 years ago\n      increase since 1800\n    \n  \n  \n    \n      0\n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5\n      NaN\n      NaN\n      50.39\n    \n    \n      1\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.05\n    \n    \n      2\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.59\n    \n    \n      3\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.64\n    \n    \n      4\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2566\n      2023\n      7\n      23\n      2023.5575\n      421.28\n      4\n      418.03\n      397.30\n      141.60\n    \n    \n      2567\n      2023\n      7\n      30\n      2023.5767\n      420.83\n      6\n      418.10\n      396.80\n      141.69\n    \n    \n      2568\n      2023\n      8\n      6\n      2023.5959\n      420.02\n      6\n      417.36\n      395.65\n      141.41\n    \n    \n      2569\n      2023\n      8\n      13\n      2023.6151\n      418.98\n      4\n      417.25\n      395.24\n      140.89\n    \n    \n      2570\n      2023\n      8\n      20\n      2023.6342\n      419.31\n      2\n      416.64\n      395.22\n      141.71\n    \n  \n\n2571 rows × 9 columns"
  },
  {
    "objectID": "basics/example.html#dealing-with-dates",
    "href": "basics/example.html#dealing-with-dates",
    "title": "3  learn by example",
    "section": "3.4 dealing with dates",
    "text": "3.4 dealing with dates\nCreate a new column called date, that combines the information from three separate columns: year, month, day.\n\n# function to_datetime translates the full date into a pandas datetime object,\n# that is, pandas knows this is a date, it's not just a string\ndf['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n# make 'date' column the dataframe index\ndf = df.set_index('date')\n# now see if everything is ok\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      decimal\n      average\n      ndays\n      1 year ago\n      10 years ago\n      increase since 1800\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1974-05-19\n      1974\n      5\n      19\n      1974.3795\n      333.37\n      5\n      NaN\n      NaN\n      50.39\n    \n    \n      1974-05-26\n      1974\n      5\n      26\n      1974.3986\n      332.95\n      6\n      NaN\n      NaN\n      50.05\n    \n    \n      1974-06-02\n      1974\n      6\n      2\n      1974.4178\n      332.35\n      5\n      NaN\n      NaN\n      49.59\n    \n    \n      1974-06-09\n      1974\n      6\n      9\n      1974.4370\n      332.20\n      7\n      NaN\n      NaN\n      49.64\n    \n    \n      1974-06-16\n      1974\n      6\n      16\n      1974.4562\n      332.37\n      7\n      NaN\n      NaN\n      50.06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-07-23\n      2023\n      7\n      23\n      2023.5575\n      421.28\n      4\n      418.03\n      397.30\n      141.60\n    \n    \n      2023-07-30\n      2023\n      7\n      30\n      2023.5767\n      420.83\n      6\n      418.10\n      396.80\n      141.69\n    \n    \n      2023-08-06\n      2023\n      8\n      6\n      2023.5959\n      420.02\n      6\n      417.36\n      395.65\n      141.41\n    \n    \n      2023-08-13\n      2023\n      8\n      13\n      2023.6151\n      418.98\n      4\n      417.25\n      395.24\n      140.89\n    \n    \n      2023-08-20\n      2023\n      8\n      20\n      2023.6342\n      419.31\n      2\n      416.64\n      395.22\n      141.71\n    \n  \n\n2571 rows × 9 columns"
  },
  {
    "objectID": "basics/example.html#first-plot",
    "href": "basics/example.html#first-plot",
    "title": "3  learn by example",
    "section": "3.5 first plot",
    "text": "3.5 first plot\nWe are now ready for our first plot! Let’s see the weekly CO2 average.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\")\n# makes slanted dates\nplt.gcf().autofmt_xdate()"
  },
  {
    "objectID": "basics/example.html#first-plot-v2.0",
    "href": "basics/example.html#first-plot-v2.0",
    "title": "3  learn by example",
    "section": "3.6 first plot, v2.0",
    "text": "3.6 first plot, v2.0\nThe dates in the x-label are not great. Let’s try to make them prettier.\nWe need to import a few more packages first.\n\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()  # datetime converter for a matplotlib\n\nNow let’s replot.\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,5)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"black\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"magenta\")\nax2.set(xlabel=\"date\",\n        ylabel=r\"CO$_2$ concentration (ppm)\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# put ticks and label on the right for ax2\nax2.yaxis.tick_right()\nax2.yaxis.set_label_position(\"right\")\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2011-12-25', 389),  xycoords='data',\n    xytext=(-10, -80), textcoords='offset points',\n    arrowprops=dict(arrowstyle=\"->\",\n                    color=\"black\",\n                    connectionstyle=\"arc3,rad=0.2\"))\nfig.savefig(\"CO2-graph.png\", dpi=300)\n\n/var/folders/hc/jhnmlst937d27zzq9fhfks780000gn/T/ipykernel_10652/850389963.py:42: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.savefig(\"CO2-graph.png\", dpi=300)\n/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: AutoDateLocator was unable to pick an appropriate interval for this date range. It may be necessary to add an interval value to the AutoDateLocator's intervald dictionary. Defaulting to 6.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nThe dates on the horizontal axis are determined thus:\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nThis deremines the location of the ticks (between 3 and 5 ticks, whatever “works best”)\nax1.xaxis.set_major_locator(locator)\nThis actually puts the ticks in the positions determined above\nformatter = mdates.ConciseDateFormatter(locator)\nThis says that the labels will be placed at the locations determined in 1.\nax1.xaxis.set_major_formatter(formatter)\nFinally, labels are written down\n\nThe arrow is placed in the graph using annotate. It has a tricky syntax and a million options. Read Jake VanderPlas’s excellent examples to learn more."
  },
  {
    "objectID": "basics/example.html#modifications",
    "href": "basics/example.html#modifications",
    "title": "3  learn by example",
    "section": "3.7 modifications",
    "text": "3.7 modifications\nLet’s change a lot of plotting options to see how things could be different.\n\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\n\n# %matplotlib widget\n# uncomment the above line if you want dynamic control of the figure when using VSCode\nfig, (ax1, ax2) = plt.subplots(1, 2,  # 1 row, 2 columns\n                               figsize=(8,4)  # width, height, in inches\n                               )\n# left panel\nax1.plot(df['average'], color=\"tab:blue\")\nax1.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax1.set(xlabel=\"date\",\n       ylabel=r\"CO$_2$ concentration (ppm)\",\n       title=\"long term\");\n# right panel\nax2.plot(df.loc['2010-01-01':'2011-12-31','average'], color=\"tab:orange\")\nax2.set(xlabel=\"date\",\n        ylim=[385, 400],  # choose y limits\n        yticks=np.arange(385, 401, 5),  # choose ticks\n        title=\"years 2010--2011\");\n# title above both panels\nfig.suptitle(\"Mauna Loa Observatory\", y=1.00)\n\nlocator = mdates.AutoDateLocator(minticks=3, maxticks=5)\nformatter = mdates.ConciseDateFormatter(locator)\nax1.xaxis.set_major_locator(locator)\nax1.xaxis.set_major_formatter(formatter)\n\nlocator = mdates.AutoDateLocator(minticks=5, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax2.xaxis.set_major_locator(locator)\nax2.xaxis.set_major_formatter(formatter)\n\nax1.annotate(\n    \"2010/11\",\n    xy=('2010-12-25', 395),  xycoords='data',\n    xytext=(-100, 40), textcoords='offset points',\n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"white\"),\n    arrowprops=dict(arrowstyle=\"->\",\n                    color=\"black\",\n                    connectionstyle=\"angle,angleA=0,angleB=-90,rad=40\"))\n\nText(-100, 40, '2010/11')\n\n\n\n\n\nThe main changes were:\n\nUsing the Seaborn package, we changed the fontsize and the overall plot style. Read more.\nsns.set(style=\"darkgrid\")\nsns.set_context(\"notebook\")\nWe changed the colors of the lineplots. To know what colors exist, click here.\nThe arrow annotation has a different style. Read more."
  },
  {
    "objectID": "basics/example.html#playing-with-the-code",
    "href": "basics/example.html#playing-with-the-code",
    "title": "3  learn by example",
    "section": "3.8 playing with the code",
    "text": "3.8 playing with the code\nI encourage you to play with the code you just ran. An easy way of learning what each line does is to comment something out and see what changes in the output you see. If you feel brave, try to modify the code a little bit."
  },
  {
    "objectID": "basics/AI-policy.html",
    "href": "basics/AI-policy.html",
    "title": "4  AI policy",
    "section": "",
    "text": "The guidelines below are an adaptation of Ethan Mollick’s extremely useful ideas on AI as an assistant tool for teaching.\nI EXPECT YOU to use LLMs (large language models) such as ChatGPT, Bing AI, Google Bard, or whatever else springs up since the time of this writing. You should familiarize yourself with the AI’s capabilities and limitations.\nUse LLMs to help you learn, chat with them about what you want to accomplish and learn from them how to do it. Ask your LLM what each part of the code means, copy and pasting blindly is unacceptable. You are here to learn.\nConsider the following important points:\n\nUltimately, you, the student, are responsible for the assignment.\nAcknowledge the use of AI in your assignment. Be transparent about your use of the tool and the extent of assistance it provided."
  },
  {
    "objectID": "resampling/motivation.html#jerusalem-2019",
    "href": "resampling/motivation.html#jerusalem-2019",
    "title": "5  motivation",
    "section": "5.1 Jerusalem, 2019",
    "text": "5.1 Jerusalem, 2019\nData from the Israel Meteorological Service, IMS.\nSee the temperature at a weather station in Jerusalem, for the whole 2019 year. This is an interactive graph: to zoom in, play with the bottom panel.\n\n\n\n\n\n\n\n\n discussion\nThe temperature fluctuates on various time scales, from daily to yearly. Let’s think together a few questions we’d like to ask about the data above.\n\nNow let’s see precipitation data:\n\n\n\n\n\n\n\n\n discussion\nWhat would be interesting to know about precipitation?\n\nWe have not talked about what kind of data we have in our hands here. The csv file provided by the IMS looks like this:\n\n\n\n\n\n\n  \n    \n      \n      Station\n      Date & Time (Winter)\n      Diffused radiation (W/m^2)\n      Global radiation (W/m^2)\n      Direct radiation (W/m^2)\n      Relative humidity (%)\n      Temperature (°C)\n      Maximum temperature (°C)\n      Minimum temperature (°C)\n      Wind direction (°)\n      Gust wind direction (°)\n      Wind speed (m/s)\n      Maximum 1 minute wind speed (m/s)\n      Maximum 10 minutes wind speed (m/s)\n      Time ending maximum 10 minutes wind speed (hhmm)\n      Gust wind speed (m/s)\n      Standard deviation wind direction (°)\n      Rainfall (mm)\n    \n  \n  \n    \n      0\n      Jerusalem Givat Ram\n      01/01/2019 00:00\n      0.0\n      0.0\n      0.0\n      80.0\n      8.7\n      8.8\n      8.6\n      75.0\n      84.0\n      3.3\n      4.3\n      3.5\n      23:58\n      6.0\n      15.6\n      0.0\n    \n    \n      1\n      Jerusalem Givat Ram\n      01/01/2019 00:10\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      74.0\n      82.0\n      3.3\n      4.1\n      3.3\n      00:01\n      4.9\n      14.3\n      0.0\n    \n    \n      2\n      Jerusalem Givat Ram\n      01/01/2019 00:20\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      76.0\n      82.0\n      3.2\n      4.1\n      3.3\n      00:19\n      4.9\n      9.9\n      0.0\n    \n    \n      3\n      Jerusalem Givat Ram\n      01/01/2019 00:30\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.7\n      8.6\n      78.0\n      73.0\n      3.6\n      4.2\n      3.6\n      00:30\n      5.2\n      11.7\n      0.0\n    \n    \n      4\n      Jerusalem Givat Ram\n      01/01/2019 00:40\n      0.0\n      0.0\n      0.0\n      79.0\n      8.6\n      8.7\n      8.5\n      80.0\n      74.0\n      3.6\n      4.4\n      3.8\n      00:35\n      5.4\n      10.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      52549\n      Jerusalem Givat Ram\n      31/12/2019 22:20\n      0.0\n      0.0\n      1.0\n      81.0\n      7.4\n      7.6\n      7.3\n      222.0\n      255.0\n      0.5\n      0.9\n      1.0\n      22:11\n      1.0\n      47.9\n      0.0\n    \n    \n      52550\n      Jerusalem Givat Ram\n      31/12/2019 22:30\n      0.0\n      0.0\n      1.0\n      83.0\n      7.3\n      7.4\n      7.3\n      266.0\n      259.0\n      0.6\n      0.8\n      0.6\n      22:28\n      1.1\n      22.8\n      0.0\n    \n    \n      52551\n      Jerusalem Givat Ram\n      31/12/2019 22:40\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.3\n      331.0\n      317.0\n      0.5\n      0.8\n      0.6\n      22:35\n      1.0\n      31.6\n      0.0\n    \n    \n      52552\n      Jerusalem Givat Ram\n      31/12/2019 22:50\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.4\n      312.0\n      285.0\n      0.6\n      1.0\n      0.6\n      22:50\n      1.4\n      31.3\n      0.0\n    \n    \n      52553\n      Jerusalem Givat Ram\n      31/12/2019 23:00\n      0.0\n      0.0\n      1.0\n      83.0\n      7.6\n      7.7\n      7.4\n      315.0\n      321.0\n      0.7\n      1.0\n      0.8\n      22:54\n      1.3\n      23.5\n      0.0\n    \n  \n\n52554 rows × 18 columns\n\n\n\nWe see that we have data points spaced out evenly every 10 minutes."
  },
  {
    "objectID": "resampling/motivation.html#challenges",
    "href": "resampling/motivation.html#challenges",
    "title": "5  motivation",
    "section": "5.2 Challenges",
    "text": "5.2 Challenges\nLet’s try to answer the following questions:\n\n\n\n\n\n\n What is the mean temperature for each month?\n\n\n\n\n\nFirst we have to divide temperature data by month, and then take the average for each month.\n\n\na possible solution\n\ndf_month = df['temperature'].resample('M').mean()\n\n\n\n\n\n\n\n\n\n\n For each month, what is the mean of the daily maximum temperature? What about the minimun?\n\n\n\n\n\nThis is a bit trickier.\n\nWe need to find the maximum/minimum temperature for each day.\nOnly then we split the daily data by month and take the average.\n\n\n\na possible solution\n\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_month['max temp'] = df_day['max temp'].resample('MS').mean()\n\n\n\n\n\n\n\n\n\n\n What is the average night temperature for every season? What about the day temperature?\n\n\n\n\n\n\nWe need to filter our data to contain only night times.\nWe need to divide rain data by seasons (3 months), and then take the mean for each season.\n\n\n\na possible solution\n\n# filter only night data\ndf_night = df.loc[((df.index.hour < 6) | (df.index.hour >= 18))]\nseason_average_night_temp = df_night['temperature'].resample('Q').mean()\n\n\n\nanother possible solution\n\n# filter using between_time\ndf_night = df.between_time('18:00', '06:00', inclusive='left')\nseason_average_night_temp = df_night['temperature'].resample('Q').mean()\n\n\n\n\n\n\n\n\n\n\n What is the daily precipitation?\n\n\n\n\n\nFirst we have to divide rain data by day, and then take the sum for each day.\n\n\na possible solution\n\ndaily_precipitation = df['rain'].resample('D').sum()\n\n\n\n\n\n\n\n\n\n\n How much rain was there every month?\n\n\n\n\n\nWe have to divide rain data by month, and then sum the totals of each month.\n\n\na possible solution\n\nmonthly_precipitation = df['rain'].resample('M').sum()\n\n\n\n\n\n\n\n\n\n\n How many rainy days were there each month?\n\n\n\n\n\n\nWe need to sum rain by day.\nWe need to count how many days are there each month where rain > 0.\n\n\n\na possible solution\n\ndaily_precipitation = df['rain'].resample('D').sum()\nonly_rainy_days = daily_precipitation.loc[daily_precipitation > 0]\nrain_days_per_month = only_rainy_days.resample('M').count()\n\n\n\n\n\n\n\n\n\n\n How many days, hours, and minutes were between the last rain of the season (Malkosh) to the first (Yoreh)?\n\n\n\n\n\n\nWe need to divide our data into two: rainy_season_1 and rainy_season_2.\nWe need to find the time of the last rain in rainy_season_1.\nWe need to find the time of the first rain in rainy_season_2.\nWe need to compute the time difference between the two dates.\n\n\n\na possible solution\n\nsplit_date = '2019-08-01'\nrainy_season_1 = df[:split_date]  # everything before split date\nrainy_season_2 = df[split_date:]  # everything after split date\nmalkosh = rainy_season_1['rain'].loc[rainy_season_1['rain'] > 0].last_valid_index()\nyoreh = rainy_season_2['rain'].loc[rainy_season_2['rain'] > 0].first_valid_index()\ndry_period = yoreh - malkosh\n# extracting days, hours, and minutes\ndays = dry_period.days\nhours = dry_period.components.hours\nminutes = dry_period.components.minutes\nprint(f'The dry period of 2019 was {days} days, {hours} hours and {minutes} minutes.')\n\n\n\n\n\n\n\n\n\n\n What was the rainiest morning (6am-12pm) of the year? Bonus, what about the rainiest night (6pm-6am)?\n\n\n\n\n\n\nWe need to filter our data to contain only morning times.\nWe need to sum rain by day.\nWe need to find the day with the maximum value.\n\n\n\na possible solution\n\n# filter to only day data\nmorning_df = df.loc[((df.index.hour >= 6) & (df.index.hour < 18))]\nmorning_rain = morning_df['rain'].resample('D').sum()\nrainiest_morning = morning_rain.idxmax()\n# plot\nmorning_rain.plot()\nplt.axvline(rainiest_morning, c='r', alpha=0.5, linestyle='--')\n\n\n\nbonus solution\n\n# filter to only night data\ndf_night = df.loc[((df.index.hour < 6) | (df.index.hour >= 18))]\n# resampling night for each day is tricky because the date changes at 12:00. We can do this trick:\n# we shift the time back by 6 hours so all the data for the same night will have the same date.\ndf_shifted = df_night.tshift(-6, freq='H')\nnight_rain = df_shifted['rain'].resample('D').sum()\nrainiest_night = night_rain.idxmax()\n# plot\nnight_rain.plot()\nplt.axvline(rainiest_night, c='r', alpha=0.5, linestyle='--')\n\n\n\n\nNote: this whole webpage is actually a Jupyter Notebook rendered as html. If you want to know how to make interactive graphs, go to the top of the page and click on “ Code”\nUseful functions compatible with pandas.resample() can be found here. The full list of resampling frequencies can be found here."
  },
  {
    "objectID": "resampling/resampling.html",
    "href": "resampling/resampling.html",
    "title": "6  resampling",
    "section": "",
    "text": "We can only really understand how to calculate monthly means if we do it ourselves.\nFirst, let’s import a bunch of packages we need to use.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\nNow we load the csv file for Jerusalem (2019), provided by the IMS.\n\ndiscussion\nWe will go to the IMS website together and see what are the options available and how to download. If you just need the csv right away, download it here.\n\n\nWe substitute every occurence of - for NaN (not a number, that is, the data is missing).\nWe call the columns Temperature (°C) and Rainfall (mm) with more convenient names, since we will be using them a lot.\nWe interpret the column Date & Time (Winter) as a date, saying to python that day comes first.\nWe make date the index of the dataframe.\n\n\nfilename = \"../archive/data/jerusalem2019.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Temperature (°C)': 'temperature',\n                   'Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      Station\n      Date & Time (Winter)\n      Diffused radiation (W/m^2)\n      Global radiation (W/m^2)\n      Direct radiation (W/m^2)\n      Relative humidity (%)\n      temperature\n      Maximum temperature (°C)\n      Minimum temperature (°C)\n      Wind direction (°)\n      Gust wind direction (°)\n      Wind speed (m/s)\n      Maximum 1 minute wind speed (m/s)\n      Maximum 10 minutes wind speed (m/s)\n      Time ending maximum 10 minutes wind speed (hhmm)\n      Gust wind speed (m/s)\n      Standard deviation wind direction (°)\n      rain\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-01 00:00:00\n      Jerusalem Givat Ram\n      01/01/2019 00:00\n      0.0\n      0.0\n      0.0\n      80.0\n      8.7\n      8.8\n      8.6\n      75.0\n      84.0\n      3.3\n      4.3\n      3.5\n      23:58\n      6.0\n      15.6\n      0.0\n    \n    \n      2019-01-01 00:10:00\n      Jerusalem Givat Ram\n      01/01/2019 00:10\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      74.0\n      82.0\n      3.3\n      4.1\n      3.3\n      00:01\n      4.9\n      14.3\n      0.0\n    \n    \n      2019-01-01 00:20:00\n      Jerusalem Givat Ram\n      01/01/2019 00:20\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      76.0\n      82.0\n      3.2\n      4.1\n      3.3\n      00:19\n      4.9\n      9.9\n      0.0\n    \n    \n      2019-01-01 00:30:00\n      Jerusalem Givat Ram\n      01/01/2019 00:30\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.7\n      8.6\n      78.0\n      73.0\n      3.6\n      4.2\n      3.6\n      00:30\n      5.2\n      11.7\n      0.0\n    \n    \n      2019-01-01 00:40:00\n      Jerusalem Givat Ram\n      01/01/2019 00:40\n      0.0\n      0.0\n      0.0\n      79.0\n      8.6\n      8.7\n      8.5\n      80.0\n      74.0\n      3.6\n      4.4\n      3.8\n      00:35\n      5.4\n      10.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2019-12-31 22:20:00\n      Jerusalem Givat Ram\n      31/12/2019 22:20\n      0.0\n      0.0\n      1.0\n      81.0\n      7.4\n      7.6\n      7.3\n      222.0\n      255.0\n      0.5\n      0.9\n      1.0\n      22:11\n      1.0\n      47.9\n      0.0\n    \n    \n      2019-12-31 22:30:00\n      Jerusalem Givat Ram\n      31/12/2019 22:30\n      0.0\n      0.0\n      1.0\n      83.0\n      7.3\n      7.4\n      7.3\n      266.0\n      259.0\n      0.6\n      0.8\n      0.6\n      22:28\n      1.1\n      22.8\n      0.0\n    \n    \n      2019-12-31 22:40:00\n      Jerusalem Givat Ram\n      31/12/2019 22:40\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.3\n      331.0\n      317.0\n      0.5\n      0.8\n      0.6\n      22:35\n      1.0\n      31.6\n      0.0\n    \n    \n      2019-12-31 22:50:00\n      Jerusalem Givat Ram\n      31/12/2019 22:50\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.4\n      312.0\n      285.0\n      0.6\n      1.0\n      0.6\n      22:50\n      1.4\n      31.3\n      0.0\n    \n    \n      2019-12-31 23:00:00\n      Jerusalem Givat Ram\n      31/12/2019 23:00\n      0.0\n      0.0\n      1.0\n      83.0\n      7.6\n      7.7\n      7.4\n      315.0\n      321.0\n      0.7\n      1.0\n      0.8\n      22:54\n      1.3\n      23.5\n      0.0\n    \n  \n\n52554 rows × 18 columns\n\n\n\nWith resample it’s easy to compute monthly averages. Resample by itself only divides the data into buckets (in this case monthly buckets), and waits for a further instruction. Here, the next instruction is mean.\n\ndf_month = df['temperature'].resample('M').mean()\ndf_month\n\ndate\n2019-01-31     9.119937\n2019-02-28     9.629812\n2019-03-31    10.731571\n2019-04-30    14.514329\n2019-05-31    22.916894\n2019-06-30    23.587361\n2019-07-31    24.019403\n2019-08-31    24.050822\n2019-09-30    22.313287\n2019-10-31    20.641868\n2019-11-30    17.257153\n2019-12-31    11.224131\nFreq: M, Name: temperature, dtype: float64\n\n\nInstead of M for month, which other options do I have? The full list can be found here, but the most commonly used are:\nM         month end frequency\nMS        month start frequency\nA         year end frequency\nAS, YS    year start frequency\nD         calendar day frequency\nH         hourly frequency\nT, min    minutely frequency\nS         secondly frequency\nThe results we got for the monthly means were given as a pandas series, not dataframe. Let’s correct this:\n\ndf_month = (df['temperature'].resample('M')         # resample by month\n                             .mean()                # take the mean\n                             .to_frame('mean temp') # make output a dafaframe\n           )\ndf_month\n\n\n\n\n\n  \n    \n      \n      mean temp\n    \n    \n      date\n      \n    \n  \n  \n    \n      2019-01-31\n      9.119937\n    \n    \n      2019-02-28\n      9.629812\n    \n    \n      2019-03-31\n      10.731571\n    \n    \n      2019-04-30\n      14.514329\n    \n    \n      2019-05-31\n      22.916894\n    \n    \n      2019-06-30\n      23.587361\n    \n    \n      2019-07-31\n      24.019403\n    \n    \n      2019-08-31\n      24.050822\n    \n    \n      2019-09-30\n      22.313287\n    \n    \n      2019-10-31\n      20.641868\n    \n    \n      2019-11-30\n      17.257153\n    \n    \n      2019-12-31\n      11.224131\n    \n  \n\n\n\n\n\nhot tip\nSometimes, a line of code can get too long and messy. In the code above, we broke line for every step, which makes the process so much cleaner. We highly advise you to do the same. Attention: This trick works as long as all the elements are inside the same parenthesis.\n\nNow it’s time to plot!\n\nfig, ax = plt.subplots()\nax.plot(df_month['mean temp'], color='black')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\")\n\n[Text(0, 0.5, 'Temperature (°C)'),\n [<matplotlib.axis.YTick at 0x7faf784c6d60>,\n  <matplotlib.axis.YTick at 0x7faf7843a220>,\n  <matplotlib.axis.YTick at 0x7faf784c62b0>,\n  <matplotlib.axis.YTick at 0x7faf784f3400>,\n  <matplotlib.axis.YTick at 0x7faf784f3760>,\n  <matplotlib.axis.YTick at 0x7faf784fa5b0>],\n Text(0.5, 1.0, 'Jerusalem, 2019')]\n\n\n\n\n\nThe dates in the horizontal axis are not great. An easy fix is to use the month numbers instead of dates. \n\nfig, ax = plt.subplots()\nax.plot(df_month.index.month, df_month['mean temp'], color='black')\nax.set(xlabel=\"month\",\n       ylabel='Temperature (°C)',\n       yticks=np.arange(5,35,5),\n       title=\"Jerusalem, 2019\",);\n\n\n\n\n\ndiscussion\nWhen you have datetime as the dataframe index, you don’t need to give the function plot two arguments, date and values. You can just tell plot to use the column you want, the function will take the dates by itself.\n What does this line mean?\ndf_month['mean temp'].index.month\nPrint on the screen the following, and see yourself what each thing is:\n\ndf_month\ndf_month.index\ndf_month.index.month\ndf_month.index.day\n\n\nWe’re done! Congratulations :)\nNow we need to calculate the average minimum/maximum daily temperatures. We start by creating an empty dataframe.\n\ndf_day = pd.DataFrame()\n\nNow resample data by day (D), and take the min/max of each day.\n\ndf_day['min temp'] = df['temperature'].resample('D').min()\ndf_day['max temp'] = df['temperature'].resample('D').max()\ndf_day\n\n\n\n\n\n  \n    \n      \n      min temp\n      max temp\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2019-01-01\n      7.5\n      14.1\n    \n    \n      2019-01-02\n      6.6\n      11.5\n    \n    \n      2019-01-03\n      6.3\n      10.7\n    \n    \n      2019-01-04\n      6.6\n      14.6\n    \n    \n      2019-01-05\n      7.0\n      11.4\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2019-12-27\n      4.4\n      7.4\n    \n    \n      2019-12-28\n      6.6\n      10.3\n    \n    \n      2019-12-29\n      8.1\n      12.5\n    \n    \n      2019-12-30\n      6.9\n      13.0\n    \n    \n      2019-12-31\n      5.2\n      13.3\n    \n  \n\n365 rows × 2 columns\n\n\n\nThe next step is to calculate the average minimum/maximum for each month. This is similar to what we did above.\n\ndf_month['min temp'] = df_day['min temp'].resample('M').mean()\ndf_month['max temp'] = df_day['max temp'].resample('M').mean()\ndf_month\n\n\n\n\n\n  \n    \n      \n      mean temp\n      min temp\n      max temp\n    \n    \n      date\n      \n      \n      \n    \n  \n  \n    \n      2019-01-31\n      9.119937\n      5.922581\n      12.470968\n    \n    \n      2019-02-28\n      9.629812\n      6.825000\n      13.089286\n    \n    \n      2019-03-31\n      10.731571\n      7.532258\n      14.661290\n    \n    \n      2019-04-30\n      14.514329\n      10.866667\n      19.113333\n    \n    \n      2019-05-31\n      22.916894\n      17.296774\n      29.038710\n    \n    \n      2019-06-30\n      23.587361\n      19.163333\n      28.860000\n    \n    \n      2019-07-31\n      24.019403\n      19.367742\n      29.564516\n    \n    \n      2019-08-31\n      24.050822\n      19.903226\n      29.767742\n    \n    \n      2019-09-30\n      22.313287\n      18.430000\n      28.456667\n    \n    \n      2019-10-31\n      20.641868\n      16.945161\n      26.190323\n    \n    \n      2019-11-30\n      17.257153\n      14.066667\n      21.436667\n    \n    \n      2019-12-31\n      11.224131\n      8.806452\n      14.448387\n    \n  \n\n\n\n\nLet’s plot…\n\nfig, ax = plt.subplots()\nax.plot(df_month['max temp'], color='tab:red', label='max')\nax.plot(df_month['mean temp'], color='black', label='mean')\nax.plot(df_month['min temp'], color='tab:blue', label='min')\nax.set(ylabel='Temperature (°C)',\n       yticks=np.arange(10,35,5),\n       title=\"Jerusalem, 2019\")\nax.xaxis.set_major_locator(mdates.MonthLocator(range(1, 13, 2), bymonthday=15))\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nax.legend(fontsize=12, frameon=False);\n\n\n\n\nVoilà! You made a beautiful graph!\n\ndiscussion\nThis time we did not put month numbers in the horizontal axis, we now have month names. How did we do this black magic, you ask? See lines 8–10 above. Matplotlib gives you absolute power over what to put in the axis, if you can only know how to tell it to… Wanna know more? Click here."
  },
  {
    "objectID": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "href": "resampling/upsampling.html#potential-evapotranspiration-using-penmans-equation",
    "title": "7  upsampling",
    "section": "7.1 Potential Evapotranspiration using Penman’s equation",
    "text": "7.1 Potential Evapotranspiration using Penman’s equation\nWe want to calculate the daily potential evapotranspiration using Penman’s equation. Part of the calculation involves characterizing the energy budget on soil surface. When direct solar radiation measurements are not available, we can estimate the energy balance by knowing the “cloudless skies mean solar radiation”, R_{so}. This is the amount of energy (MJ/m^2/d) that hits the surface, assuming no clouds. This radiation depends on the season and on the latitude you are. For Israel, located at latitude 32° N, we can use the following data for 30°:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\ndates = pd.date_range(start='2021-01-01', periods=13, freq='MS')\nvalues = [17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04, 17.46]\ndf = pd.DataFrame({'date': dates, 'radiation': values})\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      radiation\n    \n    \n      date\n      \n    \n  \n  \n    \n      2021-01-01\n      17.46\n    \n    \n      2021-02-01\n      21.65\n    \n    \n      2021-03-01\n      25.96\n    \n    \n      2021-04-01\n      29.85\n    \n    \n      2021-05-01\n      32.11\n    \n    \n      2021-06-01\n      33.20\n    \n    \n      2021-07-01\n      32.66\n    \n    \n      2021-08-01\n      30.44\n    \n    \n      2021-09-01\n      26.67\n    \n    \n      2021-10-01\n      22.48\n    \n    \n      2021-11-01\n      18.30\n    \n    \n      2021-12-01\n      16.04\n    \n    \n      2022-01-01\n      17.46\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None')\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nWe only have 12 values for the whole year, and we can’t use this dataframe to compute daily ET. We need to upsample!\nIn the example below, we resample the monthly data into daily data, and do nothing else. Pandas doesn’t know what to do with the new points, so it fills them with NaN.\n\ndf_nan = df['radiation'].resample('D').asfreq().to_frame()\ndf_nan.head(33)\n\n\n\n\n\n  \n    \n      \n      radiation\n    \n    \n      date\n      \n    \n  \n  \n    \n      2021-01-01\n      17.46\n    \n    \n      2021-01-02\n      NaN\n    \n    \n      2021-01-03\n      NaN\n    \n    \n      2021-01-04\n      NaN\n    \n    \n      2021-01-05\n      NaN\n    \n    \n      2021-01-06\n      NaN\n    \n    \n      2021-01-07\n      NaN\n    \n    \n      2021-01-08\n      NaN\n    \n    \n      2021-01-09\n      NaN\n    \n    \n      2021-01-10\n      NaN\n    \n    \n      2021-01-11\n      NaN\n    \n    \n      2021-01-12\n      NaN\n    \n    \n      2021-01-13\n      NaN\n    \n    \n      2021-01-14\n      NaN\n    \n    \n      2021-01-15\n      NaN\n    \n    \n      2021-01-16\n      NaN\n    \n    \n      2021-01-17\n      NaN\n    \n    \n      2021-01-18\n      NaN\n    \n    \n      2021-01-19\n      NaN\n    \n    \n      2021-01-20\n      NaN\n    \n    \n      2021-01-21\n      NaN\n    \n    \n      2021-01-22\n      NaN\n    \n    \n      2021-01-23\n      NaN\n    \n    \n      2021-01-24\n      NaN\n    \n    \n      2021-01-25\n      NaN\n    \n    \n      2021-01-26\n      NaN\n    \n    \n      2021-01-27\n      NaN\n    \n    \n      2021-01-28\n      NaN\n    \n    \n      2021-01-29\n      NaN\n    \n    \n      2021-01-30\n      NaN\n    \n    \n      2021-01-31\n      NaN\n    \n    \n      2021-02-01\n      21.65\n    \n    \n      2021-02-02\n      NaN"
  },
  {
    "objectID": "resampling/upsampling.html#forwardbackward-fill",
    "href": "resampling/upsampling.html#forwardbackward-fill",
    "title": "7  upsampling",
    "section": "7.2 Forward/Backward fill",
    "text": "7.2 Forward/Backward fill\nWe can forward/backward fill these NaNs:\n\ndf_forw = df['radiation'].resample('D').ffill().to_frame()\ndf_back = df['radiation'].resample('D').bfill().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_forw['radiation'], color='tab:blue', label=\"forward fill\")\nax.plot(df_back['radiation'], color='tab:orange', label=\"backward fill\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThis does the job, but I want something better, not step functions. The radiation should vary smoothly from day to day. Let’s use interpolation."
  },
  {
    "objectID": "resampling/upsampling.html#interpolation",
    "href": "resampling/upsampling.html#interpolation",
    "title": "7  upsampling",
    "section": "7.3 Interpolation",
    "text": "7.3 Interpolation\n\ndf_linear = df['radiation'].resample('D').interpolate(method='time').to_frame()\ndf_cubic = df['radiation'].resample('D').interpolate(method='cubic').to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df['radiation'], color='black', marker='d', linestyle='None', label=\"original\")\nax.plot(df_linear['radiation'], color='tab:blue', label=\"linear interpolation\")\nax.plot(df_cubic['radiation'], color='tab:orange', label=\"cubic interpolation\")\nax.set(ylabel=r'radiation (MJ/m$^2$/d)',\n       title=\"cloudless skies mean solar radiation for latitude 30° N\")\nax.legend(frameon=False, fontsize=12)\nax.xaxis.set_major_locator(mdates.MonthLocator())\ndate_form = DateFormatter(\"%b\")\nax.xaxis.set_major_formatter(date_form)\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThere are many ways to fill NaNs and to interpolate. A nice detailed guide can be found here."
  },
  {
    "objectID": "resampling/interpolation.html",
    "href": "resampling/interpolation.html",
    "title": "8  interpolation",
    "section": "",
    "text": "Interpolation is the act of getting data you don’t have from data you alreay have. We used some interpolation when upsampling, and now it is time to talk about it a little bit more in depth.\nThere is no one correct way of interpolating, the method you use depends in the end on what you want to accomplish, what are your (hidden or explicit) assumptions, etc. Let’s see a few examples."
  },
  {
    "objectID": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "href": "resampling/FAQ.html#how-to-resample-by-year-but-have-it-end-in-september",
    "title": "9  FAQ",
    "section": "9.1 How to resample by year, but have it end in September?",
    "text": "9.1 How to resample by year, but have it end in September?\nThis is called anchored offset. One possible use to it is to calculate statistics according to the hydrological year that, for example, ends in September.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n\nfilename = \"../archive/data/Kinneret_Kvuza_daily_rainfall.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Date': 'date',\n                   'Daily Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace\ndf\n\n\n\n\n\n  \n    \n      \n      Station\n      rain\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      1980-01-02\n      Kinneret Kvuza 09/1977-08/2023\n      0.0\n    \n    \n      1980-01-03\n      0\n      0.0\n    \n    \n      1980-01-04\n      0\n      0.0\n    \n    \n      1980-01-05\n      Kinneret Kvuza 09/1977-08/2023\n      35.5\n    \n    \n      1980-01-06\n      Kinneret Kvuza 09/1977-08/2023\n      2.2\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2019-12-26\n      Kinneret Kvuza 09/1977-08/2023\n      39.4\n    \n    \n      2019-12-27\n      Kinneret Kvuza 09/1977-08/2023\n      5.2\n    \n    \n      2019-12-28\n      Kinneret Kvuza 09/1977-08/2023\n      1.6\n    \n    \n      2019-12-29\n      0\n      0.0\n    \n    \n      2019-12-30\n      Kinneret Kvuza 09/1977-08/2023\n      0.1\n    \n  \n\n14608 rows × 2 columns\n\n\n\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(df['rain'], color='black')\nax[1].plot(df.loc['1998':'2000', 'rain'], color='black')\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=8)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\nfig.text(0.02, 0.5, 'daily precipitation (mm)', va='center', rotation='vertical')\nax[0].set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')\n\n\n\n\n\nWe see a marked dry season during the summer, so let’s assume the Hydrological Year ends in September.\n\ndf_year = df.resample('A-SEP').sum()\ndf_year = df_year.loc['1980':'2003']\ndf_year\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_94063/2047090134.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_year = df.resample('A-SEP').sum()\n\n\n\n\n\n\n  \n    \n      \n      rain\n    \n    \n      date\n      \n    \n  \n  \n    \n      1980-09-30\n      355.5\n    \n    \n      1981-09-30\n      463.1\n    \n    \n      1982-09-30\n      221.7\n    \n    \n      1983-09-30\n      557.1\n    \n    \n      1984-09-30\n      335.3\n    \n    \n      1985-09-30\n      379.8\n    \n    \n      1986-09-30\n      300.7\n    \n    \n      1987-09-30\n      424.7\n    \n    \n      1988-09-30\n      421.6\n    \n    \n      1989-09-30\n      251.6\n    \n    \n      1990-09-30\n      432.5\n    \n    \n      1991-09-30\n      328.3\n    \n    \n      1992-09-30\n      738.4\n    \n    \n      1993-09-30\n      434.9\n    \n    \n      1994-09-30\n      255.4\n    \n    \n      1995-09-30\n      408.6\n    \n    \n      1996-09-30\n      373.0\n    \n    \n      1997-09-30\n      416.2\n    \n    \n      1998-09-30\n      451.9\n    \n    \n      1999-09-30\n      227.8\n    \n    \n      2000-09-30\n      378.9\n    \n    \n      2001-09-30\n      273.9\n    \n    \n      2002-09-30\n      445.2\n    \n    \n      2003-09-30\n      602.4\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(df_year.index, df_year['rain'], color='black',\n       width=365)\nax.set_ylabel(\"yearly precipitation (mm)\")\nax.set_title(\"Kvutzat Kinneret\")\n\nText(0.5, 1.0, 'Kvutzat Kinneret')"
  },
  {
    "objectID": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "href": "resampling/FAQ.html#when-upsampling-how-to-fill-missing-values-with-zero",
    "title": "9  FAQ",
    "section": "9.2 When upsampling, how to fill missing values with zero?",
    "text": "9.2 When upsampling, how to fill missing values with zero?\nWe did that in the example above, like this:\ndf = df.resample('D').asfreq().fillna(0)  # asfreq = replace"
  },
  {
    "objectID": "smoothing/motivation.html#tumbling-vs-sliding",
    "href": "smoothing/motivation.html#tumbling-vs-sliding",
    "title": "10  motivation",
    "section": "10.1 Tumbling vs Sliding",
    "text": "10.1 Tumbling vs Sliding"
  },
  {
    "objectID": "smoothing/sliding.html#convolution",
    "href": "smoothing/sliding.html#convolution",
    "title": "11  sliding window",
    "section": "11.1 convolution",
    "text": "11.1 convolution\nConvolution is a fancy word for averaging a time series using a sliding window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel.\n\n\n\n\nThe pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result.\n\n\n\n\nThere is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes."
  },
  {
    "objectID": "smoothing/sliding.html#kernels",
    "href": "smoothing/sliding.html#kernels",
    "title": "11  sliding window",
    "section": "11.2 kernels",
    "text": "11.2 kernels\nWe can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points).\n\n\n\n\nThings can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation.\n\n\n\n\nSee how the three kernel shapes compare. There are many kernels to chose from."
  },
  {
    "objectID": "smoothing/sliding.html#math",
    "href": "smoothing/sliding.html#math",
    "title": "11  sliding window",
    "section": "11.3 math",
    "text": "11.3 math\nThe definition of a convolution between signal f(t) and kernel k(t) is\n\n(f * k)(t) = \\int f(\\tau)k(t-\\tau)d\\tau.\n\nThe expression f*k denotes the convolution of these two functions. The argument of k is t-\\tau, meaning that the kernel runs from left to right (as t does), and at every point the two signals (f and k) are multiplied together. It is the product of the signal with the weight function k that gives us an average. Because of -\\tau, the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but\n\n\\frac{(f * k)(t)}{\\displaystyle \\int k(t)dt}.\n\nWhenever the integral of the kernel is 1, then the convolution will be identical with the running average."
  },
  {
    "objectID": "smoothing/sliding.html#numerics",
    "href": "smoothing/sliding.html#numerics",
    "title": "11  sliding window",
    "section": "11.4 numerics",
    "text": "11.4 numerics\nRunning averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes:\n\ndf['temp_smoothed'] = (\n                       df['TD'].rolling(window='500min',\n                                        min_periods=50   # comment this to see what happens\n                                        )\n                               .mean()\n                      )\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(df['TD'], color='black')\nax.plot(df['temp_smoothed'], color='xkcd:hot pink')\nax.set(**plot_settings)\ncentered_dates(ax)\n\n\n\n\nThe pink curve looks smooth, but why does it lag behind the data?! What’s going on?\n\n11.4.1 7-day average of COVID-19 infections\nDuring the COVID-19 pandemic, we would see graphs like this all the time in the news:\n\n\nimport COVID-19 data for Israel, process it\n# data from https://health.google.com/covid-19/open-data/raw-data?loc=IL\n# define the local file path\nlocal_file_path = 'COVID_19_israel.csv'\n# check if the local file exists\nif os.path.exists(local_file_path):\n    # if the local file exists, load it\n    covid_IL = pd.read_csv(local_file_path, parse_dates=['date'], index_col='date')\nelse:\n    # if the local file doesn't exist, download from the URL\n    url = \"https://storage.googleapis.com/covid19-open-data/v3/location/IL.csv\"\n    covid_IL = pd.read_csv(url, parse_dates=['date'], index_col='date')\n    # save the downloaded data to the local file for future use\n    covid_IL.to_csv(local_file_path)\n\ndf_covid = covid_IL['new_confirmed'].to_frame()\ndf_covid['7d_avg'] = df_covid['new_confirmed'].rolling(window='7D').mean()\n\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(8,5))\nst = '2022-01-01'\nen = '2022-03-30'\nnew_cases = ax.bar(df_covid[st:en].index, df_covid.loc[st:en,'new_confirmed'],\n       color=\"tab:blue\", width=1)\nmov_avg, = ax.plot(df_covid.loc[st:en,'7d_avg'],\n        color='xkcd:hot pink')\nax.legend(handles=[new_cases, mov_avg],\n          labels=['new confirmed cases', '7-day moving average'],\n          frameon=False)\nweird_day = \"2022-02-12\"\nweird_day_x = mdates.date2num(dt.datetime.strptime(weird_day, \"%Y-%m-%d\"))\nax.text(weird_day_x, df_covid.loc[weird_day,'new_confirmed'], \"?\")\n# formating dates on x axis\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\nax.xaxis.set_major_locator(locator)\nax.xaxis.set_major_formatter(formatter)\n\n\n\n\n\nTake a look at the moving average next to the question mark. How can it be that high, when all the bars around that date are lower? Is the calculation right?\nThe answer is that the result of the moving average is assigned to the right-most date in the running window. This is reasonable for COVID-19 cases: for a given day, I can only calculate a 7-day average based on past values, I don’t know what the future will be.\nThere is a simple way of assigning the result to the center of the window:\n\ndf_covid['7d_avg_center'] = (\n                             df_covid['new_confirmed']\n                                 .rolling(window='7D',\n                                          center=True)  # THIS\n                                 .mean()\n                            )\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(8,5))\nst = '2022-01-01'\nen = '2022-03-30'\nnew_cases = ax.bar(df_covid[st:en].index, df_covid.loc[st:en,'new_confirmed'],\n       color=\"tab:blue\", width=1)\nmov_avg, = ax.plot(df_covid.loc[st:en,'7d_avg'],\n        color='xkcd:hot pink')\nmov_avg_center, = ax.plot(df_covid.loc[st:en,'7d_avg_center'],\n                          color='xkcd:mustard')\nax.legend(handles=[new_cases, mov_avg, mov_avg_center],\n          labels=['new confirmed cases',\n                  '7-day moving average',\n                  'CENTERED 7-day\\nmoving average'],\n          frameon=False)\n# formating dates on x axis\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\nax.xaxis.set_major_locator(locator)\nax.xaxis.set_major_formatter(formatter)\n\n\n\n\n\nAs a rule, we will used a centered moving average (center=True), unless stated otherwise. Also, only use min_periods if you know what you are doing.\n\n\n11.4.2 gaussian\nYou can easily change the kernel shape by using the win_type argument. See how to perform a rolling mean with a gaussian kernel:\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"gaussian\")\n                 .mean(std=std_gaussian)\n)\nwhere\n\nwindow_width is an integer, number of points in your window\nstd_gaussian is the standard deviation of your gaussian, measured in sample points, not time!\n\nFor instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this:\n\nwindow_width = 50  # in points = 500 min\nstd = 6  # in points = 60 min\nfig, ax = plt.subplots(figsize=(8,5))\ng = scipy.signal.gaussian(window_width, std)\nax.plot(g)\nax.set(xlabel=\"window width (points)\",\n       ylabel=\"kernel weights\",\n       title=\"gaussian kernel\");\n\n\n\n\nYou can take a look at various options for kernel shapes here, provided by the scipy package.\n\n\n11.4.3 triangular\nSame idea as gaussian, but simpler, because we don’t need to think about standard deviation.\n(\ndf['temperature'].rolling(window=window_width,\n                          center=True,\n                          win_type=\"triang\")\n                 .mean()\n)"
  },
  {
    "objectID": "smoothing/sliding.html#which-window-shape-and-width-to-choose",
    "href": "smoothing/sliding.html#which-window-shape-and-width-to-choose",
    "title": "11  sliding window",
    "section": "11.5 which window shape and width to choose?",
    "text": "11.5 which window shape and width to choose?\n🤷‍♂️\nSorry, there is not definite answer here… It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above.\n\nOne important question you need to ask is: what are the time scales associated with the processes I’m interested in? For example, if I’m interested in the daily temperature pattern, getting rid of 1-minute-long fluctuations would probably be ok. On the other hand, if we were to smooth the signal so much that all that can be seen are the temperature changes between summer and winter, then my smoothing got out of hand, and I threw away the very process I wanted to study.\nAll this is to say that you need to know in advance a few things about the system you are studying, otherwise you can’t know what is “noise” that can be smoothed away."
  },
  {
    "objectID": "smoothing/not-only-averages.html#confidence-interval",
    "href": "smoothing/not-only-averages.html#confidence-interval",
    "title": "12  not only averages",
    "section": "12.1 Confidence Interval",
    "text": "12.1 Confidence Interval\nWe can calculate anything we want inside the sliding window. One good example is the Confidence Interval of the Mean, given by:\n\nCI(\\alpha) = Z(\\alpha) \\cdot SE.\n\n\n\nThis is called “רווח בר-סֶמֶך” in hebrew.\n\nZ(\\alpha)= Z-score.\nSE = standard error.\n\nZ(\\alpha) is the Z-score corresponding to the chosen confidence level \\alpha. The most commonly used confidence level is 95%, which corresponds to a Z-score of 1.96. What does this mean? This means that we expect to find 95% of the points within \\pm 1.96 standard deviations away from the mean.\n\n\n\nSource: Dhaval Raval’s Medium article\nYou can find the Z-score using the following python code:\n\nfrom scipy.stats import norm\n\nconfidence_level = 0.95\n# 5% outside\nout = 1 - confidence_level\n# 0.975 of points to the left of right boundary\np = 1 - out/2\n# inverse of cdf: 0.975 of the points will be smaller than what distance (in sigma units)?\nz_score = norm.ppf(p)\nprint(f\"z-score = {z_score}\")\n\nz-score = 1.959963984540054\n\n\nIf you are still not convinced why we need 0.975 instead of 0.95, read this excellent response on stackoverflow.\nSE is the standard error:\n\nSE = \\frac{\\sigma}{ \\sqrt{N} }.\n\n\n\n\n\\sigma= standard deviation.\nN= number of points.\n\nWe can write a function to calculate the confidence interval of the mean, and use it with the sliding window:\n\ndef std_error_of_the_mean(window):\n    return window.std() / np.sqrt(window.count())\n\ndef confidence_interval(window):\n    return z_score * std_error_of_the_mean(window)\n\ndf['std_error'] = (\n                   df['temp'].rolling('3H',\n                                      center=True)\n                             .apply(std_error_of_the_mean)\n                  )\ndf['confidence_int'] = (\n                        df['temp'].rolling('3H',\n                                           center=True)\n                                  .apply(confidence_interval)\n                       )\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(8,5))\n\nplot_std = ax.fill_between(df.index,\n                            df['mean'] + df['confidence_int'],\n                            df['mean'] - df['confidence_int'],\n                            color=\"xkcd:pink\", alpha=0.5)\nplot_data, = ax.plot(df['temp'], color='black', alpha=0.3)\nplot_mean, =ax.plot(df['mean'], color='xkcd:hot pink')\n\nax.legend([plot_data, plot_mean, plot_std],\n          ['data', '3-hour running average', r\"95% confidence interval\"],\n          frameon=False)\n\n# applying the settings to the ax object\nax.set(**plot_settings)\ncentered_dates(ax)\n# fig.savefig(\"YF-temperature_2022_jan.png\", dpi=300)\n\n\n\n\n\nWhen the time series has a regular sampling frequency, all positions of the running window will have the same number of data points in them. Because the Confidence Interval is proportional to the Standard Error, and the SE is proportional to the Standard Deviation (\\sqrt{N} is constant), then the envelope created by the CI is identical to the envelope created by the standard deviation, up to a multiplying constant. Nice.\n\n\nCI and std\nfig, ax = plt.subplots(figsize=(8,5))\nplot_ci, = ax.plot(df['confidence_int'], color='tab:red')\nplot_std, = ax.plot(df['std'], color=\"black\")\nax.legend([plot_ci, plot_std],\n          ['confidence interval', 'standard deviation'],\n          frameon=False)\n\n# applying the settings to the ax object\n# ax.set(**plot_settings)\nax.set(xlim=[df.index[0], df.index[-1]])\ncentered_dates(ax)"
  },
  {
    "objectID": "smoothing/fit.html#linear-fit",
    "href": "smoothing/fit.html#linear-fit",
    "title": "13  fit",
    "section": "13.1 linear fit",
    "text": "13.1 linear fit\nThe following is a very short introduction to curve fitting. The natural place to start is with a linear fit.\n\n\nlinear fit\n# the \"fit\" process can't deal with datetimes\n# we therefore make a new column 'minutes', that will be used here\ndf_fit['minutes'] = (df_fit.index - df_fit.index[0]).total_seconds() / 60\n# linear Fit (degree 1)\ndegree = 1\ncoeffs = np.polyfit(df_fit['minutes'], df_fit['T_in'], degree)\n# linear Function\nlinear_function = np.poly1d(coeffs)\n\n\n\n\nsee result of linear fit\nfig, ax = plt.subplots(figsize=(8,5))\n\nax.scatter(df_fit['minutes'], df_fit['T_in'],\n           color='tab:green', label='data')\nax.plot(df_fit['minutes'], linear_function(df_fit['minutes']),\n        color='black', label='linear fit')\n\nax.set(xlabel='minutes',\n       ylabel='temperature (°C)',\n       title=\"temperature inside the greenhouse\")\n\nax.legend(frameon=False)\nprint(f\"starting at {coeffs[1]:.2f} degrees,\\nthe temperature decreases by {-coeffs[0]:.2f} degrees every minute.\")\n\n\nstarting at 19.80 degrees,\nthe temperature decreases by 0.05 degrees every minute.\n\n\n\n\n\nThe line above is the “best” straight line that describes our data. Defining the residual as the difference between our data and our model (straight line),\n\ne = T_{\\text{data}} - T_{\\text{model}},\n\nthe straight line above is the one that minimizes the sum of the squares of residuals. For this reason, the method used above to fit a curve to the data is called “least-squares method”.\n\n\nit minimizes the sum\n\nS = \\sum_i e_i^2\n\nCan we do better than a straight line?"
  },
  {
    "objectID": "smoothing/fit.html#polynomial-fit",
    "href": "smoothing/fit.html#polynomial-fit",
    "title": "13  fit",
    "section": "13.2 polynomial fit",
    "text": "13.2 polynomial fit\n\n\npolynomial fit\n# polynomial fit (degree 2)\ndegree = 2\ncoeffs2 = np.polyfit(df_fit['minutes'], df_fit['T_in'], degree)\nquad_function = np.poly1d(coeffs2)\n\n# polynomial fit (degree 2)\ndegree = 3\ncoeffs3 = np.polyfit(df_fit['minutes'], df_fit['T_in'], degree)\ncubic_function = np.poly1d(coeffs3)\n\n\n\n\nsee result of polynomial fit\nfig, ax = plt.subplots(figsize=(8,5))\n\nax.scatter(df_fit['minutes'], df_fit['T_in'],\n           color='tab:green', label='data')\nax.plot(df_fit['minutes'], quad_function(df_fit['minutes']),\n        color='black', label='order = 2')\nax.plot(df_fit['minutes'], cubic_function(df_fit['minutes']),\n        color='tab:olive', label='order = 3')\n\nax.set(xlabel='minutes',\n       ylabel='temperature (°C)',\n       title=\"temperature inside the greenhouse\")\nax.legend(frameon=False)\n\n\n<matplotlib.legend.Legend at 0x7fd0a0833eb0>"
  },
  {
    "objectID": "smoothing/fit.html#any-function-you-want",
    "href": "smoothing/fit.html#any-function-you-want",
    "title": "13  fit",
    "section": "13.3 any function you want",
    "text": "13.3 any function you want\nNow let’s get back to our original assumption, that the greenhouse cools according to Newton’s cooling law. We can still use the least-squares method for any function we want!\n\n\ndefine new function\ndef cooling(t, T_env, T0, r):\n    \"\"\"\n    t = time\n    other stuff = parameters to be fitted\n    \"\"\"\n    return T_env + (T0 - T_env)*np.exp(-r*t)\n\n\n\n\nuse scipy’s curve_fit\nt = df_fit['minutes'].values\ny = df_fit['T_in'].values\n\nT_init = df_fit['T_in'][0]\n\npopt, pcov = curve_fit(f=cooling,             # model function\n                     xdata=t,                 # x data\n                     ydata=y,                 # y data\n                     p0=(2, T_init, 0.5),     # initial guess of the parameters\n                     )\nprint(f\"the optimal parameters are {popt}\")\n\n\nthe optimal parameters are [14.01663586 21.0074623   0.02121802]\n\n\n\n\nsee result of exponential fit\nfig, ax = plt.subplots(sharex=True)\n\nax.scatter(df_fit['minutes'], df_fit['T_in'],\n           color='tab:green', label='data')\nax.plot(t, cooling(t, *popt),\n        color='black', label='exponential fit')\n\nax.set(xlabel='minutes',\n       ylabel='temperature (°C)',\n       title=\"temperature inside the greenhouse\")\n\nax.legend(frameon=False)\n\n\n<matplotlib.legend.Legend at 0x7fd0b0140850>\n\n\n\n\n\nThat looks really good :)\nWe can use curve fitting to retrieve important parameters from our data. Let’s write a function that executes the fit and returns two of the fitted parameters: T_env and r.\n\n\ndefine function to retrieve parameters\ndef run_fit(data):\n    data['minutes'] = (data.index - data.index[0]).total_seconds() / 60\n    t = data['minutes'].values\n    y = data['T_in'].values\n    T_init = data['T_in'][0]\n    popt, pcov = curve_fit(f=cooling,             # model function\n                        xdata=t,              # x data\n                        ydata=y,              # y data\n                        p0=(2, T_init, 0.5),   # initial guess of the parameters\n                        )\n    return popt[0],popt[2]\n\n\nWe now apply this function to several consecutive evenings, and we keep the results in a new dataframe.\n\n\ndefine function to retrieve parameters\ndf_night = df.between_time('20:01', '22:01', inclusive='left')\n\n# group by day and apply the function\n# this is where the magic happens.\n# if you are not familiar with \"groupby\", this will be hard to understand\nresult_series = df_night.groupby(df_night.index.date).apply(run_fit)\n\n# convert the series to a dataframe\nresult_df = pd.DataFrame(result_series.tolist(), index=result_series.index, columns=['T_env', 'r'])\nresult_df.index = pd.to_datetime(result_df.index)\nresult_df\n\n\n\n\n\n\n  \n    \n      \n      T_env\n      r\n    \n  \n  \n    \n      2023-06-25\n      13.275540\n      0.019354\n    \n    \n      2023-06-26\n      13.331949\n      0.027034\n    \n    \n      2023-06-27\n      13.254827\n      0.018753\n    \n    \n      2023-06-28\n      13.392919\n      0.020449\n    \n    \n      2023-06-29\n      14.016636\n      0.021218\n    \n    \n      2023-06-30\n      13.807517\n      0.021749\n    \n    \n      2023-07-01\n      14.994207\n      0.023504\n    \n    \n      2023-07-02\n      14.314220\n      0.023705\n    \n    \n      2023-07-03\n      14.585848\n      0.019438\n    \n    \n      2023-07-04\n      14.377220\n      0.019504\n    \n    \n      2023-07-05\n      14.814939\n      0.021202\n    \n    \n      2023-07-06\n      14.667792\n      0.022264\n    \n    \n      2023-07-07\n      15.535115\n      0.024421\n    \n  \n\n\n\n\n\n\nShow the code\nfig, ax = plt.subplots(3,1,sharex=True, figsize=(8,8))\n\nax[0].plot(df['T_in'], c='tab:blue', label='inside')\nax[0].plot(df['T_out'], c='tab:orange', label='outside')\nax[0].set(ylabel='temperature (°C)',\n          title=\"actual temperatures\",\n          ylim=[10,45])\n\n# formating dates on x axis\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\nax[0].xaxis.set_major_locator(locator)\nax[0].xaxis.set_major_formatter(formatter)\n\nax[0].legend(ncol=2, loc='upper center', frameon=False)\n\nax[1].plot(result_df['r'], color='black')\nax[1].set(ylabel=r\"parameter $r$\",\n          ylim=[0, 0.04])\n\nax[2].plot(result_df['T_env'], color='black')\nax2b = ax[2].twinx()\nax2b.plot(df_night['T_out'].resample('D').mean(), color='tab:orange')\nax[2].set(ylim=[12, 17])\nax2b.set(ylim=[19, 24],\n        ylabel='outside temperature')\n# color the xticks\nfor tick in ax[2].get_yticklabels():\n    tick.set_color('tab:orange')\n# color the xlabel\nax[2].set_ylabel(r'\"outside\" temp.'+'\\ninferred from\\nanalysis', color='tab:orange')\n\n\nText(0, 0.5, '\"outside\" temp.\\ninferred from\\nanalysis')\n\n\n\n\n\nConclusions:\n\nThe cooling coefficient r seems quite stable throughout the two weeks of measurements. This probably says that the greenhouse and AC properties did not change much. For instance, the greenhouse thermal insulation stayed constant, and the AC power output stayed constant.\nThe AC tracks very well the outside temperature! This is to say: the AC works better (more easily) when temperatures outsides are low, and vice-versa."
  },
  {
    "objectID": "smoothing/savgol.html",
    "href": "smoothing/savgol.html",
    "title": "14  Savitzky–Golay",
    "section": "",
    "text": "The Savitzky-Golay filter, also known as LOESS, smoothes a noisy signal by performing a polynomial fit over a sliding window.\nPolynomial fit of order 3, window size = 51 pts\n\n\n\n\nPolynomial fit of order 2, window size = 51 pts\n\n\n\n\nThe simulations look different because the order of the polynomial makes a very different impression on us, but in reality the outcome of the two filtering is almost identical:\n\n\nimport stuff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport datetime as dt\nimport matplotlib.ticker as ticker\nfrom scipy.signal import savgol_filter\nimport os\nimport warnings\nimport scipy\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n# %matplotlib widget\n\n\n\n\ndefine useful functions\n# dirty trick to have dates in the middle of the 24-hour period\n# make minor ticks in the middle, put the labels there!\n# from https://matplotlib.org/stable/gallery/ticks/centered_ticklabels.html\n\ndef centered_dates(ax):\n    date_form = DateFormatter(\"%d %b\")  # %d 3-letter-Month\n    # major ticks at midnight, every day\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n    ax.xaxis.set_major_formatter(date_form)\n    # minor ticks at noon, every day\n    ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=[12]))\n    # erase major tick labels\n    ax.xaxis.set_major_formatter(ticker.NullFormatter())\n    # set minor tick labels as define above\n    ax.xaxis.set_minor_formatter(date_form)\n    # completely erase minor ticks, center tick labels\n    for tick in ax.xaxis.get_minor_ticks():\n        tick.tick1line.set_markersize(0)\n        tick.tick2line.set_markersize(0)\n        tick.label1.set_horizontalalignment('center')\n\n\n\n\nload data\ndf = pd.read_csv('shani_2022_january.csv', parse_dates=['date'], index_col='date')\nstart = \"2022-01-02\"\nend = \"2022-01-05\"\ndf = df.loc[start:end]\n\n\n\ndf['sg_3_51'] = savgol_filter(df['TD'], window_length=51, polyorder=3)\ndf['sg_2_51'] = savgol_filter(df['TD'], window_length=51, polyorder=2)\n\n\n\nplot temperature data\nfig, ax = plt.subplots(figsize=(8,5))\n\nplot_data, = ax.plot(df['TD'], color='black')\nplot_sg2, = ax.plot(df['sg_2_51'], color='xkcd:hot pink')\nplot_sg3, = ax.plot(df['sg_3_51'], color='xkcd:mustard')\n\nax.legend(handles=[plot_data, plot_sg2, plot_sg3],\n          labels=['data', 'sg order 2', 'sg order 3'],\n          frameon=False)\n\nplot_settings = {\n    'ylim': [5, 17.5],\n    'xlim': [df.index[0], df.index[-1]],\n    'ylabel': \"Temperature (°C)\",\n    'title': \"Yatir Forest, 2022\",\n    'yticks': [5, 10, 15]\n}\n\nax.set(**plot_settings)\ncentered_dates(ax)\n\n\n\n\n\nTo really see the difference between window width and polynomial order, we need to play with their ratio,\n\n\\text{ratio} = \\frac{w}{p} = \\frac{\\text{window width}}{\\text{polynomial order}}\n\n\n\nchose only one day\nstart = \"2022-01-02 00:00:00\"\nend = \"2022-01-02 23:50:00\"\ndf = df.loc[start:end]\n\n\n\n# window_length, polyorder\ndf['sg_1'] = savgol_filter(df['TD'], 5, 3)\ndf['sg_2'] = savgol_filter(df['TD'], 11, 2)\ndf['sg_3'] = savgol_filter(df['TD'], 25, 3)\n\n\n\ncompare different ratio choices\nfig, ax = plt.subplots(figsize=(8,5))\n\nplot_data, = ax.plot(df['TD'], color='black')\nplot_sg1, = ax.plot(df['sg_1'], color='xkcd:hot pink')\nplot_sg2, = ax.plot(df['sg_2'], color='xkcd:mustard')\nplot_sg3, = ax.plot(df['sg_3'], color='xkcd:royal blue')\n\nax.legend(handles=[plot_data, plot_sg1, plot_sg2, plot_sg3],\n          labels=['data', r'$w/p=1.5$', r'$w/p=5.5$', r'$w/p=8.3$'],\n          frameon=False)\n\nplot_settings = {\n    'ylim': [5, 17.5],\n    'xlim': [df.index[0], df.index[-1]],\n    'ylabel': \"Temperature (°C)\",\n    'title': \"Yatir Forest, 2022\",\n    'yticks': [5, 10, 15]\n}\n\nax.set(**plot_settings)\n\nlocator = mdates.AutoDateLocator(minticks=7, maxticks=11)\nformatter = mdates.ConciseDateFormatter(locator)\n\nax.xaxis.set_major_locator(locator)\nax.xaxis.set_major_formatter(formatter)\n\n\n\n\n\nThe higher the ratio, the more aggressive the smoothing.\nThere is a lot more about the Savitzky-Golay filter, but for our purposes this is enough. If you want some more discussion about how to choose the parameters of the filter, read this."
  },
  {
    "objectID": "outliers/motivation.html",
    "href": "outliers/motivation.html",
    "title": "15  motivation",
    "section": "",
    "text": "Outliers are observations significantly different from all other observations. Consider, for example, this temperature graph:\n\nWhile most measured points are between 20 and 30 °C, there is obviously something very wrong with the one data point above 80 °C.\nHow could such a thing come about? This could be the result of non-natural causes, such as measurement errors, wrong data collection, or wrong data entry. On the other hand, this point could have natural sources, such as a very hot spark flying next to the temperature sensor.\nIdentifying outliers is important, because they might greatly impact measures like mean and standard deviation. When left untouched, outliers might make us reach wrong conclusions about our data. See what happens to the slope of this linear regression with and without the outliers.\n\n\n\n\n\n\nSource: Zhang (2020)\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.” ouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "outliers/outlier-identification.html#visual-inspection",
    "href": "outliers/outlier-identification.html#visual-inspection",
    "title": "16  outlier identification",
    "section": "16.1 visual inspection",
    "text": "16.1 visual inspection\nI produced a stationary signal and added to it a few ouliers. Can you tell where just by looking at the graph? \nThe easiest way of identifying the outliers is:\n\nFirst plot the time series.\nChoose upper and lower boundaries. Whatever falls outside these boundaries is an outlier.\n\nEasy.\n\nIf all you have is this one time series, you’re done, congratulations. However, it is often the case that one has very long time series, or a great number of time series to analyze. In this case it is impractical to use the visual inspection method. We would like to devise an algorithm to automate this task."
  },
  {
    "objectID": "outliers/outlier-identification.html#z-score",
    "href": "outliers/outlier-identification.html#z-score",
    "title": "16  outlier identification",
    "section": "16.2 Z-score",
    "text": "16.2 Z-score\nThe Z-score is the distance, in units of 1 standard deviation, of a point in the series with respect to the mean:\n\nz  = \\frac{x-\\mu}{\\sigma},\n\n\n\nwhere\n\nx= data point,\n\n\\mu= time series mean\n\n\\sigma= time series standard deviation.\n\nA common choice is to consider an outlier a point whose Z-score is greater that 3, in absolute value. In other words: If a point is more than 3 standard deviations away form the mean, then we call it an outlier.\n\nYou can now use this algorithm to any number of time series, let the computer do the hard work.\nOf course, there is nothing sacred about the number 3. You can choose any Z-score you want to perform an analysis on your own data, depending on your needs.\n\n16.2.1 ATTENTION!\nFor data that is gaussianly distributed, we expect that 99.73% of data to fall within 3 standard deviations from the mean. In other words, 0.27% of points would be considered as outliers according to the Z-score method.\n\n\n\nSource: Wikimedia Commons\nAssume you have a time series gaussianly distributed, with 10k measurements. We would expect to find about 27 outliers in this time series.\nSo what is the problem?!\nThe thing is, outliers are not supposed to be only data points far from the other points. That’s not enough. A better way of understanding outliers is to imagine that our expected measurements are sampled from a given distribution, and every now an then we have measurements that are sampled from another distribution.\n\n\n\nSource: Taylor Wilson’s “Dealing with Outliers (Part 1): Ignore Them at Your Peril”\nWe should have this in mind always. We wouldn’t want to single out good data as something weird. Our true task is to identify which points in our time series were sampled from a different distribution. This can be a very challenging task."
  },
  {
    "objectID": "outliers/outlier-identification.html#iqr",
    "href": "outliers/outlier-identification.html#iqr",
    "title": "16  outlier identification",
    "section": "16.3 IQR",
    "text": "16.3 IQR\nAnother super common criterion for identifying outliers is the IQR, or InterQuartile Range.\nTake a look at the statistics below of the time series we have been working with so far. The IQR is the distance between the first quartile (Q1) and the third quartile (Q3), where exactly 50% of the data is.\nThe algorithm here is to determine two thresholds, whose distance is 1.5 times the IQR from Q1 and Q3. Whatever falls outside these two thresholds is an outlier.\n\nWe are used to see this in box plots:\n\n\n\nSource: McDonald (2022)\nAgain, the distance 1.5 is not sacred, it’s only the most common. You might want to choose other values depending on your needs. Let’s now apply the IQR method to our time series.\n\nIt works pretty well! Notice that now we have an additional outlier (a bit before 06:00). What do we do with that?"
  },
  {
    "objectID": "outliers/outlier-identification.html#non-stationary-time-series",
    "href": "outliers/outlier-identification.html#non-stationary-time-series",
    "title": "16  outlier identification",
    "section": "16.4 non-stationary time series",
    "text": "16.4 non-stationary time series\nI have produced a new time series, one that on average goes up with time. Can you point in the graph where are the outliers?\n\nNow, see what happens when we apply the previous two methods to this time series.\nZ-score\n\nIQR\n\nWhat happened? Do you have ideas how to solve this?"
  },
  {
    "objectID": "outliers/outlier-identification.html#sources",
    "href": "outliers/outlier-identification.html#sources",
    "title": "16  outlier identification",
    "section": "16.5 Sources",
    "text": "16.5 Sources\n\n\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python Library.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57."
  },
  {
    "objectID": "outliers/robust-analysis.html#mad",
    "href": "outliers/robust-analysis.html#mad",
    "title": "17  robust analysis",
    "section": "17.1 MAD",
    "text": "17.1 MAD\nAnother rubust method is MAD, the Median Absolute Deviation, given by\n\n\\text{MAD} = \\text{median}(\\left| x_i - \\text{median}(x)  \\right|),\n\nwhere |\\cdot| is the absolute value.\nApplying MAD to the stationary time series from before, yields\n\nHere, the threshold is the median \\pm3k\\cdot MAD, where the value k=1.4826 scales MAD so that when the data is gaussianly distributed, 3k equals 1 standard deviation."
  },
  {
    "objectID": "outliers/sliding-algorithms.html#sliding-z-score",
    "href": "outliers/sliding-algorithms.html#sliding-z-score",
    "title": "18  sliding algorithms",
    "section": "18.1 Sliding Z-score",
    "text": "18.1 Sliding Z-score\n\nNow the Z-score seems to give really nice results (but not perfect). Maybe playing with the window width and Z-score threshold would give better results?\nIn any case, we clearly see why the Z-score is not a robust algorithm. See how the standard deviation is sensitive to outliers?"
  },
  {
    "objectID": "outliers/sliding-algorithms.html#sliding-iqr",
    "href": "outliers/sliding-algorithms.html#sliding-iqr",
    "title": "18  sliding algorithms",
    "section": "18.2 Sliding IQR",
    "text": "18.2 Sliding IQR\nLet’s see how well the sliding IQR method fares.\n\nIt identified all the outliers, but also found that a few other points should be considered outliers? What do you think of that?\nSee that the threshold does not jump abruptly when the sliding window includes an outlier. In fact, the threshold doesn’t even care! This is what it means to be robust.\nHowever, we do see large fluctuations in the threshold. When does this happen? Why?"
  },
  {
    "objectID": "outliers/sliding-algorithms.html#sliding-mad",
    "href": "outliers/sliding-algorithms.html#sliding-mad",
    "title": "18  sliding algorithms",
    "section": "18.3 Sliding MAD",
    "text": "18.3 Sliding MAD\nNow it’s MAD’s time to shine.\n\nCompare this result to the previous two. Which yields best results?\nMAD is robust to outliers, and again we see that the threshold envelope widens when there is a rising or falling trend in the data."
  },
  {
    "objectID": "outliers/sliding-algorithms.html#challenges",
    "href": "outliers/sliding-algorithms.html#challenges",
    "title": "18  sliding algorithms",
    "section": "18.4 Challenges",
    "text": "18.4 Challenges\nNow it’s your turn to work, I’m tired! Write algorithms for the following outlier identification methods:\n\nvisual inspection\nZ-score\nIQR\nMAD\n\nExcluding the visual inspection method, write first an algorithm that operates on a full time series, and then write a new version that can work with sliding windows."
  },
  {
    "objectID": "outliers/substituting-outliers.html#do-nothing",
    "href": "outliers/substituting-outliers.html#do-nothing",
    "title": "19  substituting outliers",
    "section": "19.1 Do nothing",
    "text": "19.1 Do nothing\nAssuming the outlier indeed happened in real life, and is not the result of faulty data transmission or bad data recording, then excluding an outlier might be the last thing you want to do. Sometimes extreme events do happen, such as a one-in-a-hundred-year storm, and they have a disproportionate weight on the system you are studying. The outliers might actually be the most interesting points in your data for all you know!\nIn case the outliers are not of interest to you, if you are using robust methods to analyze your data, you don’t necessarily need to do anything either. For instance, let’s say that you want to smooth your time series. If instead of taking the mean inside a sliding window you choose to calculate the median, then outliers shouldn’t be a problem. Test it and see if it’s true. Go on.\nFor many things you need to do (not only smoothing), you might be able to find robust methods. What do you do if you have to use a non-robust method? Well, then you can substitute the outlier for two things: NaN or imputated values."
  },
  {
    "objectID": "outliers/substituting-outliers.html#nan",
    "href": "outliers/substituting-outliers.html#nan",
    "title": "19  substituting outliers",
    "section": "19.2 NaN",
    "text": "19.2 NaN\nSubstitute outliers for NaN.\nNaN means “Not a Number”, and is what you get when you try to perform a mathematical operation like 0/0. It is common to see NaN in dataset rows when data was not collected for some reason.\nThis might seem like a neutral solution, but it actually can generate problems down the line. See this example:\n\n\nimport stuff\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\nfrom scipy.signal import savgol_filter\n\n\n\n# example using numpy\nseries = np.array([2, 4, 5, np.nan, 8, 15])\nmean = np.mean(series)\nprint(f\"the series average is {mean}\")\n\nthe series average is nan\n\n\nA single NaN in your time series ruins the whole calculation! There is a workaround though:\n\nmean = np.nanmean(series)\nprint(f\"the series average is {mean}\")\n\nthe series average is 6.8\n\n\nYou have to make sure what is the behavior of each function you use with respect to NaNs, and if possible, use a suitable substitute.\nThe same example in pandas would not fail:\n\ndate_range = pd.date_range(start='2024-01-01', periods=len(series), freq='1D')\ndf = pd.DataFrame({'series': series}, index=date_range)\nmean = df['series'].mean()\nprint(f\"the series average is {mean}\")\n\nthe series average is 6.8"
  },
  {
    "objectID": "outliers/substituting-outliers.html#imputate-values",
    "href": "outliers/substituting-outliers.html#imputate-values",
    "title": "19  substituting outliers",
    "section": "19.3 imputate values",
    "text": "19.3 imputate values\nTo “imputate values” means to fill in the missing value with a guess, an estimation of what this data point “should have been” if it were measured in the first place. Why should we bother to do so? Because many tools that we know and love don’t do well with missing values.\nWe learned about the Savitzky-Golay filter for smoothing data. See what happens when there is a single NaN in the series:\n\n\ncreate time series\nsteps = np.random.randint(low=-2, high=2, size=100)\ndata = steps.cumsum()\ndate_range = pd.date_range(start='2023-01-01', periods=len(data), freq='1D')\ndf = pd.DataFrame({'series': data}, index=date_range)\ndf.loc['2023-02-05', 'series'] = np.nan\n\n\n\n\nsmooth it and then plot\ndf['sg'] = savgol_filter(df['series'], window_length=15, polyorder=2)\n\ndef concise(ax):\n    locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n    formatter = mdates.ConciseDateFormatter(locator)\n    ax.xaxis.set_major_locator(locator)\n    ax.xaxis.set_major_formatter(formatter)\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(df['series'], color=\"tab:blue\", label=\"series with 1 NaN\")\nax.plot(df['sg'], color=\"tab:orange\", label=\"SavGol filter has many more NaNs\")\nconcise(ax)\nax.legend(frameon=False);\n\n\n\n\n\nWe will deal with this topic in the next chapter, “interpolation”. There, we will learn a few methods to fill in missing data, and basic NaN operations you should be acquainted with."
  },
  {
    "objectID": "outliers/outliers_challenge.html#importing-bad-.csv-files",
    "href": "outliers/outliers_challenge.html#importing-bad-.csv-files",
    "title": "20  challenge",
    "section": "20.1 importing bad .csv files",
    "text": "20.1 importing bad .csv files\nHere we will get a taste of what it feels like to work with bad .csv files and how to fix them.\nTO DO:\n\ncreate a folder for this challenge, name it whatever you want.\ndownlad this jupyter notebook and move to that folder.\ndownload these 3 .csv files and part 2 notebook:\n\ncleaning1\ncleaning2-\ncleaning3\npart_2\n\nmove them files into your folder\n\n\n20.1.1 import\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n# %matplotlib widget"
  },
  {
    "objectID": "outliers/outliers_challenge.html#dataset-1",
    "href": "outliers/outliers_challenge.html#dataset-1",
    "title": "20  challenge",
    "section": "20.2 dataset 1",
    "text": "20.2 dataset 1\n\ndf1 = pd.read_csv('cleaning1.csv')\ndf1\n\n\n\n\n\n  \n    \n      \n      date\n      A\n      B\n      C\n      D\n      E\n    \n  \n  \n    \n      0\n      2023-01-01 00:00:00\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      1\n      2023-01-01 00:05:00\n      1\n      2.245251\n      -1.757193\n      1.899602\n      -0.999300\n    \n    \n      2\n      2023-01-01 00:10:00\n      2\n      2.909648\n      0.854732\n      2.050146\n      -0.559504\n    \n    \n      3\n      2023-01-01 00:15:00\n      3\n      3.483155\n      0.946937\n      1.921080\n      -0.402084\n    \n    \n      4\n      2023-01-01 00:20:00\n      2\n      4.909169\n      0.462239\n      1.368470\n      -0.698579\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      105115\n      2023-12-31 23:35:00\n      -37\n      1040.909898\n      -14808.285199\n      1505.020266\n      423.595984\n    \n    \n      105116\n      2023-12-31 23:40:00\n      -36\n      1040.586547\n      -14808.874072\n      1503.915566\n      423.117110\n    \n    \n      105117\n      2023-12-31 23:45:00\n      -37\n      1042.937417\n      -14808.690745\n      1505.479671\n      423.862810\n    \n    \n      105118\n      2023-12-31 23:50:00\n      -36\n      1042.411572\n      -14809.212002\n      1506.174334\n      423.862432\n    \n    \n      105119\n      2023-12-31 23:55:00\n      -35\n      1043.053520\n      -14809.990338\n      1505.767197\n      423.647007\n    \n  \n\n105120 rows × 6 columns\n\n\n\nNow let’s put the column ‘date’ in the index with datetime format\n\n# we can change the format of the column to datetime and then set it as the index.\ndf1['date'] = pd.to_datetime(df1['date'])\ndf1.set_index('date', inplace=True)\ndf1\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      2023-01-01 00:05:00\n      1\n      2.245251\n      -1.757193\n      1.899602\n      -0.999300\n    \n    \n      2023-01-01 00:10:00\n      2\n      2.909648\n      0.854732\n      2.050146\n      -0.559504\n    \n    \n      2023-01-01 00:15:00\n      3\n      3.483155\n      0.946937\n      1.921080\n      -0.402084\n    \n    \n      2023-01-01 00:20:00\n      2\n      4.909169\n      0.462239\n      1.368470\n      -0.698579\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 23:35:00\n      -37\n      1040.909898\n      -14808.285199\n      1505.020266\n      423.595984\n    \n    \n      2023-12-31 23:40:00\n      -36\n      1040.586547\n      -14808.874072\n      1503.915566\n      423.117110\n    \n    \n      2023-12-31 23:45:00\n      -37\n      1042.937417\n      -14808.690745\n      1505.479671\n      423.862810\n    \n    \n      2023-12-31 23:50:00\n      -36\n      1042.411572\n      -14809.212002\n      1506.174334\n      423.862432\n    \n    \n      2023-12-31 23:55:00\n      -35\n      1043.053520\n      -14809.990338\n      1505.767197\n      423.647007\n    \n  \n\n105120 rows × 5 columns\n\n\n\nIf we know that in advance, we can write everything in one command when we import the csv.\n\ndf1 = pd.read_csv('cleaning1.csv', \n                  index_col='date',     # set the column date as index \n                  parse_dates=True)     # turn to datetime format\ndf1\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      2023-01-01 00:05:00\n      1\n      2.245251\n      -1.757193\n      1.899602\n      -0.999300\n    \n    \n      2023-01-01 00:10:00\n      2\n      2.909648\n      0.854732\n      2.050146\n      -0.559504\n    \n    \n      2023-01-01 00:15:00\n      3\n      3.483155\n      0.946937\n      1.921080\n      -0.402084\n    \n    \n      2023-01-01 00:20:00\n      2\n      4.909169\n      0.462239\n      1.368470\n      -0.698579\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 23:35:00\n      -37\n      1040.909898\n      -14808.285199\n      1505.020266\n      423.595984\n    \n    \n      2023-12-31 23:40:00\n      -36\n      1040.586547\n      -14808.874072\n      1503.915566\n      423.117110\n    \n    \n      2023-12-31 23:45:00\n      -37\n      1042.937417\n      -14808.690745\n      1505.479671\n      423.862810\n    \n    \n      2023-12-31 23:50:00\n      -36\n      1042.411572\n      -14809.212002\n      1506.174334\n      423.862432\n    \n    \n      2023-12-31 23:55:00\n      -35\n      1043.053520\n      -14809.990338\n      1505.767197\n      423.647007\n    \n  \n\n105120 rows × 5 columns\n\n\n\nNow let’s plot all the columns and see what we have.\n\ndef plot_all_columns(data):\n    column_list = data.columns\n    \n    fig, ax = plt.subplots(len(column_list),1, sharex=True, figsize=(10,len(column_list)*2))\n\n    if len(column_list) == 1:\n        ax.plot(data[column_list[0]])\n        return\n    for i, column in enumerate(column_list):\n        ax[i].plot(data[column])\n        ax[i].set(ylabel=column)\n    \n    locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n    formatter = mdates.ConciseDateFormatter(locator)\n    ax[i].xaxis.set_major_locator(locator)\n    ax[i].xaxis.set_major_formatter(formatter)\n\n    return\n\n\nplot_all_columns(df1)\n\n\n\n\nLooks like some of this data needs cleaning…"
  },
  {
    "objectID": "outliers/outliers_challenge.html#dataset-2",
    "href": "outliers/outliers_challenge.html#dataset-2",
    "title": "20  challenge",
    "section": "20.3 dataset 2",
    "text": "20.3 dataset 2\n\ndf2 = pd.read_csv('cleaning2-.csv')\ndf2\n\n\n\n\n\n  \n    \n      \n      A B date time\n    \n  \n  \n    \n      0\n      0.0 0.0 01012023 00:00:00\n    \n    \n      1\n      -2.0275363548598184 0.011984922825112581 01012...\n    \n    \n      2\n      -2.690616715983192 -0.29792822981957684 010120...\n    \n    \n      3\n      -1.9859899758267612 -0.30940867922490206 01012...\n    \n    \n      4\n      -2.290897621584889 -2.8666633353521624 0101202...\n    \n    \n      ...\n      ...\n    \n    \n      8755\n      -74.51464473079395 293.8680858227996 31122023 ...\n    \n    \n      8756\n      -74.73805809332175 294.7593463919649 31122023 ...\n    \n    \n      8757\n      -75.84842465358788 294.07634907736116 31122023...\n    \n    \n      8758\n      -77.27218272637339 293.526461290973 31122023 2...\n    \n    \n      8759\n      -76.55739976945038 293.35336890454107 31122023...\n    \n  \n\n8760 rows × 1 columns\n\n\n\nSomething is wrong…\nLet’s open the actual .csv file and take a quick look.\nIt seems that the values are seperated by spaces  and not by commas ,.\n\ndf2 = pd.read_csv('cleaning2-.csv', delimiter=' ')\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      date\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.0\n      1012023\n      00:00:00\n    \n    \n      1\n      -2.027536\n      0.011984922825112581\n      1012023\n      01:00:00\n    \n    \n      2\n      -2.690617\n      -0.29792822981957684\n      1012023\n      02:00:00\n    \n    \n      3\n      -1.985990\n      -0.30940867922490206\n      1012023\n      03:00:00\n    \n    \n      4\n      -2.290898\n      -2.8666633353521624\n      1012023\n      04:00:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8755\n      -74.514645\n      293.8680858227996\n      31122023\n      19:00:00\n    \n    \n      8756\n      -74.738058\n      294.7593463919649\n      31122023\n      20:00:00\n    \n    \n      8757\n      -75.848425\n      294.07634907736116\n      31122023\n      21:00:00\n    \n    \n      8758\n      -77.272183\n      293.526461290973\n      31122023\n      22:00:00\n    \n    \n      8759\n      -76.557400\n      293.35336890454107\n      31122023\n      23:00:00\n    \n  \n\n8760 rows × 4 columns\n\n\n\n\n# convert the date column to datetime\ndf2['date_corrected'] = pd.to_datetime(df2['date'])\n# df2['date_corrected'] = pd.to_datetime(df2['date'], format='%d%m%Y')\n\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      date\n      time\n      date_corrected\n    \n  \n  \n    \n      0\n      0.000000\n      0.0\n      1012023\n      00:00:00\n      1970-01-01 00:00:00.001012023\n    \n    \n      1\n      -2.027536\n      0.011984922825112581\n      1012023\n      01:00:00\n      1970-01-01 00:00:00.001012023\n    \n    \n      2\n      -2.690617\n      -0.29792822981957684\n      1012023\n      02:00:00\n      1970-01-01 00:00:00.001012023\n    \n    \n      3\n      -1.985990\n      -0.30940867922490206\n      1012023\n      03:00:00\n      1970-01-01 00:00:00.001012023\n    \n    \n      4\n      -2.290898\n      -2.8666633353521624\n      1012023\n      04:00:00\n      1970-01-01 00:00:00.001012023\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8755\n      -74.514645\n      293.8680858227996\n      31122023\n      19:00:00\n      1970-01-01 00:00:00.031122023\n    \n    \n      8756\n      -74.738058\n      294.7593463919649\n      31122023\n      20:00:00\n      1970-01-01 00:00:00.031122023\n    \n    \n      8757\n      -75.848425\n      294.07634907736116\n      31122023\n      21:00:00\n      1970-01-01 00:00:00.031122023\n    \n    \n      8758\n      -77.272183\n      293.526461290973\n      31122023\n      22:00:00\n      1970-01-01 00:00:00.031122023\n    \n    \n      8759\n      -76.557400\n      293.35336890454107\n      31122023\n      23:00:00\n      1970-01-01 00:00:00.031122023\n    \n  \n\n8760 rows × 5 columns\n\n\n\n\n# df2['date_corrected'] = pd.to_datetime(df2['date'][:780], format='%d%m%Y')\n# df2['date_corrected'] = pd.to_datetime(df2['date'][780:800], format='%d%m%Y')\n# df2['date'][780:800]\n\n\ndata_types = {'date': str , 'time':str}\n\n# Read the CSV file with specified data types\ndf2 = pd.read_csv('cleaning2-.csv', delimiter=' ', dtype=data_types)\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      date\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.0\n      01012023\n      00:00:00\n    \n    \n      1\n      -2.027536\n      0.011984922825112581\n      01012023\n      01:00:00\n    \n    \n      2\n      -2.690617\n      -0.29792822981957684\n      01012023\n      02:00:00\n    \n    \n      3\n      -1.985990\n      -0.30940867922490206\n      01012023\n      03:00:00\n    \n    \n      4\n      -2.290898\n      -2.8666633353521624\n      01012023\n      04:00:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8755\n      -74.514645\n      293.8680858227996\n      31122023\n      19:00:00\n    \n    \n      8756\n      -74.738058\n      294.7593463919649\n      31122023\n      20:00:00\n    \n    \n      8757\n      -75.848425\n      294.07634907736116\n      31122023\n      21:00:00\n    \n    \n      8758\n      -77.272183\n      293.526461290973\n      31122023\n      22:00:00\n    \n    \n      8759\n      -76.557400\n      293.35336890454107\n      31122023\n      23:00:00\n    \n  \n\n8760 rows × 4 columns\n\n\n\n\ndf2['date_corrected'] = pd.to_datetime(df2['date'], format='%d%m%Y')\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      date\n      time\n      date_corrected\n    \n  \n  \n    \n      0\n      0.000000\n      0.0\n      01012023\n      00:00:00\n      2023-01-01\n    \n    \n      1\n      -2.027536\n      0.011984922825112581\n      01012023\n      01:00:00\n      2023-01-01\n    \n    \n      2\n      -2.690617\n      -0.29792822981957684\n      01012023\n      02:00:00\n      2023-01-01\n    \n    \n      3\n      -1.985990\n      -0.30940867922490206\n      01012023\n      03:00:00\n      2023-01-01\n    \n    \n      4\n      -2.290898\n      -2.8666633353521624\n      01012023\n      04:00:00\n      2023-01-01\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8755\n      -74.514645\n      293.8680858227996\n      31122023\n      19:00:00\n      2023-12-31\n    \n    \n      8756\n      -74.738058\n      294.7593463919649\n      31122023\n      20:00:00\n      2023-12-31\n    \n    \n      8757\n      -75.848425\n      294.07634907736116\n      31122023\n      21:00:00\n      2023-12-31\n    \n    \n      8758\n      -77.272183\n      293.526461290973\n      31122023\n      22:00:00\n      2023-12-31\n    \n    \n      8759\n      -76.557400\n      293.35336890454107\n      31122023\n      23:00:00\n      2023-12-31\n    \n  \n\n8760 rows × 5 columns\n\n\n\n\ndf2['datetime'] = pd.to_datetime(df2['date'] + ' ' + df2['time'], format='%d%m%Y %H:%M:%S')\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      date\n      time\n      date_corrected\n      datetime\n    \n  \n  \n    \n      0\n      0.000000\n      0.0\n      01012023\n      00:00:00\n      2023-01-01\n      2023-01-01 00:00:00\n    \n    \n      1\n      -2.027536\n      0.011984922825112581\n      01012023\n      01:00:00\n      2023-01-01\n      2023-01-01 01:00:00\n    \n    \n      2\n      -2.690617\n      -0.29792822981957684\n      01012023\n      02:00:00\n      2023-01-01\n      2023-01-01 02:00:00\n    \n    \n      3\n      -1.985990\n      -0.30940867922490206\n      01012023\n      03:00:00\n      2023-01-01\n      2023-01-01 03:00:00\n    \n    \n      4\n      -2.290898\n      -2.8666633353521624\n      01012023\n      04:00:00\n      2023-01-01\n      2023-01-01 04:00:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8755\n      -74.514645\n      293.8680858227996\n      31122023\n      19:00:00\n      2023-12-31\n      2023-12-31 19:00:00\n    \n    \n      8756\n      -74.738058\n      294.7593463919649\n      31122023\n      20:00:00\n      2023-12-31\n      2023-12-31 20:00:00\n    \n    \n      8757\n      -75.848425\n      294.07634907736116\n      31122023\n      21:00:00\n      2023-12-31\n      2023-12-31 21:00:00\n    \n    \n      8758\n      -77.272183\n      293.526461290973\n      31122023\n      22:00:00\n      2023-12-31\n      2023-12-31 22:00:00\n    \n    \n      8759\n      -76.557400\n      293.35336890454107\n      31122023\n      23:00:00\n      2023-12-31\n      2023-12-31 23:00:00\n    \n  \n\n8760 rows × 6 columns\n\n\n\n\ndf2.drop(columns=['date', 'time', 'date_corrected'], inplace=True)\ndf2.set_index('datetime', inplace=True)\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n    \n      datetime\n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0.000000\n      0.0\n    \n    \n      2023-01-01 01:00:00\n      -2.027536\n      0.011984922825112581\n    \n    \n      2023-01-01 02:00:00\n      -2.690617\n      -0.29792822981957684\n    \n    \n      2023-01-01 03:00:00\n      -1.985990\n      -0.30940867922490206\n    \n    \n      2023-01-01 04:00:00\n      -2.290898\n      -2.8666633353521624\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 19:00:00\n      -74.514645\n      293.8680858227996\n    \n    \n      2023-12-31 20:00:00\n      -74.738058\n      294.7593463919649\n    \n    \n      2023-12-31 21:00:00\n      -75.848425\n      294.07634907736116\n    \n    \n      2023-12-31 22:00:00\n      -77.272183\n      293.526461290973\n    \n    \n      2023-12-31 23:00:00\n      -76.557400\n      293.35336890454107\n    \n  \n\n8760 rows × 2 columns\n\n\n\n\nplot_all_columns(df2)\n\n\n\n\nWhat happened in the second ax?\n\ndf2.dtypes\n\nA    float64\nB     object\ndtype: object\n\n\n\n# use pd.to_numeric to convert column 'B' to float\ndf2['B'] = pd.to_numeric(df2['B'],\n                          errors='coerce'  # 'coerce' will convert non-numeric values to NaN if they can't be converted\n                          )\ndf2.dtypes\n\nA    float64\nB    float64\ndtype: object\n\n\n\nplot_all_columns(df2)\n\n\n\n\n\ndata_types = {'date': str , 'time':str}\n\n# Read the CSV file with specified data types\ndf2 = pd.read_csv('cleaning2-.csv', delimiter=' ', dtype=data_types, na_values='-')\ndf2['datetime'] = pd.to_datetime(df2['date'] + ' ' + df2['time'], format='%d%m%Y %H:%M:%S')\ndf2.drop(columns=['date', 'time'], inplace=True)\ndf2.set_index('datetime', inplace=True)\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n    \n      datetime\n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0.000000\n      0.000000\n    \n    \n      2023-01-01 01:00:00\n      -2.027536\n      0.011985\n    \n    \n      2023-01-01 02:00:00\n      -2.690617\n      -0.297928\n    \n    \n      2023-01-01 03:00:00\n      -1.985990\n      -0.309409\n    \n    \n      2023-01-01 04:00:00\n      -2.290898\n      -2.866663\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 19:00:00\n      -74.514645\n      293.868086\n    \n    \n      2023-12-31 20:00:00\n      -74.738058\n      294.759346\n    \n    \n      2023-12-31 21:00:00\n      -75.848425\n      294.076349\n    \n    \n      2023-12-31 22:00:00\n      -77.272183\n      293.526461\n    \n    \n      2023-12-31 23:00:00\n      -76.557400\n      293.353369\n    \n  \n\n8760 rows × 2 columns\n\n\n\n\ndf2.to_csv('cleaning2_formated.csv')"
  },
  {
    "objectID": "outliers/outliers_challenge.html#dataset-3",
    "href": "outliers/outliers_challenge.html#dataset-3",
    "title": "20  challenge",
    "section": "20.4 dataset 3",
    "text": "20.4 dataset 3\n\ndf3 = pd.read_csv('cleaning3.csv')\ndf3\n\n\n\n\n\n  \n    \n      \n      #\n    \n  \n  \n    \n      0\n      # data created by\n    \n    \n      1\n      # Yair Mau\n    \n    \n      2\n      # for time series data analysis\n    \n    \n      3\n      #\n    \n    \n      4\n      # time format: unix (s)\n    \n    \n      ...\n      ...\n    \n    \n      370\n      6.651300774019661 1703635200.0\n    \n    \n      371\n      6.4151748349408715 1703721600.0\n    \n    \n      372\n      7.603140054178304 1703808000.0\n    \n    \n      373\n      8.668182044560869 1703894400.0\n    \n    \n      374\n      8.472767724946076 1703980800.0\n    \n  \n\n375 rows × 1 columns\n\n\n\nAgain, let’s look at the actual .csv file.\n\ndf3 = pd.read_csv('cleaning3.csv', comment='#')\ndf3\n\n\n\n\n\n  \n    \n      \n      A time\n    \n  \n  \n    \n      0\n      0.0 1672531200.0\n    \n    \n      1\n      -0.03202661701444382 1672617600.0\n    \n    \n      2\n      -0.5863508675173621 1672704000.0\n    \n    \n      3\n      -1.5759721567247762 1672790400.0\n    \n    \n      4\n      -2.7267995149281266 1672876800.0\n    \n    \n      ...\n      ...\n    \n    \n      360\n      6.651300774019661 1703635200.0\n    \n    \n      361\n      6.4151748349408715 1703721600.0\n    \n    \n      362\n      7.603140054178304 1703808000.0\n    \n    \n      363\n      8.668182044560869 1703894400.0\n    \n    \n      364\n      8.472767724946076 1703980800.0\n    \n  \n\n365 rows × 1 columns\n\n\n\n\ndf3 = pd.read_csv('cleaning3.csv', comment='#', delimiter=' ')\ndf3\n\n\n\n\n\n  \n    \n      \n      A\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      1.672531e+09\n    \n    \n      1\n      -0.032027\n      1.672618e+09\n    \n    \n      2\n      -0.586351\n      1.672704e+09\n    \n    \n      3\n      -1.575972\n      1.672790e+09\n    \n    \n      4\n      -2.726800\n      1.672877e+09\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      360\n      6.651301\n      1.703635e+09\n    \n    \n      361\n      6.415175\n      1.703722e+09\n    \n    \n      362\n      7.603140\n      1.703808e+09\n    \n    \n      363\n      8.668182\n      1.703894e+09\n    \n    \n      364\n      8.472768\n      1.703981e+09\n    \n  \n\n365 rows × 2 columns\n\n\n\n\ndf3.dtypes\n\nA       float64\ntime    float64\ndtype: object\n\n\nTime is in unix.\nunix converter\n\nprint(df3['time'][0])\n\n1672531200.0\n\n\n\ndf3['time'] = pd.to_datetime(df3['time'], unit='s')\ndf3.set_index('time', inplace=True)\ndf3\n\n\n\n\n\n  \n    \n      \n      A\n    \n    \n      time\n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n    \n    \n      2023-01-02\n      -0.032027\n    \n    \n      2023-01-03\n      -0.586351\n    \n    \n      2023-01-04\n      -1.575972\n    \n    \n      2023-01-05\n      -2.726800\n    \n    \n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n    \n    \n      2023-12-28\n      6.415175\n    \n    \n      2023-12-29\n      7.603140\n    \n    \n      2023-12-30\n      8.668182\n    \n    \n      2023-12-31\n      8.472768\n    \n  \n\n365 rows × 1 columns\n\n\n\n\ndf3 = pd.read_csv('cleaning3.csv', \n                  index_col='time',     # set the column date as index \n                  parse_dates=True,     # turn to datetime format\n                  comment='#', \n                  delimiter=' ')\ndf3.index = pd.to_datetime(df3.index, unit='s')\ndf3\n\n\n\n\n\n  \n    \n      \n      A\n    \n    \n      time\n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n    \n    \n      2023-01-02\n      -0.032027\n    \n    \n      2023-01-03\n      -0.586351\n    \n    \n      2023-01-04\n      -1.575972\n    \n    \n      2023-01-05\n      -2.726800\n    \n    \n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n    \n    \n      2023-12-28\n      6.415175\n    \n    \n      2023-12-29\n      7.603140\n    \n    \n      2023-12-30\n      8.668182\n    \n    \n      2023-12-31\n      8.472768\n    \n  \n\n365 rows × 1 columns\n\n\n\n\nplot_all_columns(df3)\n\n\n\n\n\ndf3.to_csv('cleaning3_formated.csv')"
  },
  {
    "objectID": "outliers/outliers_challenge_part2.html#outliers-and-missing-values",
    "href": "outliers/outliers_challenge_part2.html#outliers-and-missing-values",
    "title": "21  challenge part 2",
    "section": "21.1 outliers and missing values",
    "text": "21.1 outliers and missing values\nHere you have 3 dataframes that need cleaning. Use the methods learned in class to process the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n# %matplotlib widget\n\n\ndf1 = pd.read_csv('cleaning1.csv', \n                  index_col='date',     # set the column date as index \n                  parse_dates=True)     # turn to datetime format\ndf1\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      2023-01-01 00:05:00\n      1\n      2.245251\n      -1.757193\n      1.899602\n      -0.999300\n    \n    \n      2023-01-01 00:10:00\n      2\n      2.909648\n      0.854732\n      2.050146\n      -0.559504\n    \n    \n      2023-01-01 00:15:00\n      3\n      3.483155\n      0.946937\n      1.921080\n      -0.402084\n    \n    \n      2023-01-01 00:20:00\n      2\n      4.909169\n      0.462239\n      1.368470\n      -0.698579\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 23:35:00\n      -37\n      1040.909898\n      -14808.285199\n      1505.020266\n      423.595984\n    \n    \n      2023-12-31 23:40:00\n      -36\n      1040.586547\n      -14808.874072\n      1503.915566\n      423.117110\n    \n    \n      2023-12-31 23:45:00\n      -37\n      1042.937417\n      -14808.690745\n      1505.479671\n      423.862810\n    \n    \n      2023-12-31 23:50:00\n      -36\n      1042.411572\n      -14809.212002\n      1506.174334\n      423.862432\n    \n    \n      2023-12-31 23:55:00\n      -35\n      1043.053520\n      -14809.990338\n      1505.767197\n      423.647007\n    \n  \n\n105120 rows × 5 columns\n\n\n\n\ndf2 = pd.read_csv('cleaning2_formated.csv', \n                  index_col='datetime',     # set the column date as index \n                  parse_dates=True)     # turn to datetime format\ndf2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n    \n      datetime\n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0.000000\n      0.000000\n    \n    \n      2023-01-01 01:00:00\n      -2.027536\n      0.011985\n    \n    \n      2023-01-01 02:00:00\n      -2.690617\n      -0.297928\n    \n    \n      2023-01-01 03:00:00\n      -1.985990\n      -0.309409\n    \n    \n      2023-01-01 04:00:00\n      -2.290898\n      -2.866663\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 19:00:00\n      -74.514645\n      293.868086\n    \n    \n      2023-12-31 20:00:00\n      -74.738058\n      294.759346\n    \n    \n      2023-12-31 21:00:00\n      -75.848425\n      294.076349\n    \n    \n      2023-12-31 22:00:00\n      -77.272183\n      293.526461\n    \n    \n      2023-12-31 23:00:00\n      -76.557400\n      293.353369\n    \n  \n\n8760 rows × 2 columns\n\n\n\n\ndf3 = pd.read_csv('cleaning3_formated.csv', \n                  index_col='time',     # set the column date as index \n                  parse_dates=True,     # turn to datetime format\n                )\ndf3\n\n\n\n\n\n  \n    \n      \n      A\n    \n    \n      time\n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n    \n    \n      2023-01-02\n      -0.032027\n    \n    \n      2023-01-03\n      -0.586351\n    \n    \n      2023-01-04\n      -1.575972\n    \n    \n      2023-01-05\n      -2.726800\n    \n    \n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n    \n    \n      2023-12-28\n      6.415175\n    \n    \n      2023-12-29\n      7.603140\n    \n    \n      2023-12-30\n      8.668182\n    \n    \n      2023-12-31\n      8.472768\n    \n  \n\n365 rows × 1 columns"
  },
  {
    "objectID": "outliers/outliers_challenge_part2.html#cleaning-df1-from-outliers",
    "href": "outliers/outliers_challenge_part2.html#cleaning-df1-from-outliers",
    "title": "21  challenge part 2",
    "section": "21.2 cleaning df1 from outliers",
    "text": "21.2 cleaning df1 from outliers\n\n21.2.1 method 1: rolling standard deviation envelope\nVisual inspection of all the data:\n\ndef plot_all_columns(data):\n    column_list = data.columns\n    \n    fig, ax = plt.subplots(len(column_list),1, sharex=True, figsize=(10,len(column_list)*2))\n\n    if len(column_list) == 1:\n        ax.plot(data[column_list[0]])\n        return\n    for i, column in enumerate(column_list):\n        ax[i].plot(data[column])\n        ax[i].set(ylabel=column)\n    \n    locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n    formatter = mdates.ConciseDateFormatter(locator)\n    ax[i].xaxis.set_major_locator(locator)\n    ax[i].xaxis.set_major_formatter(formatter)\n\n    return\n\n\nplot_all_columns(df1)\n\n\n\n\nApplying the rolling std method on column A:\n\n# find the rolling std\ndf1['A_std'] = df1['A'].rolling(50, center=True, min_periods=1).std()\n# find the rolling mean\ndf1['A_mean'] = df1['A'].rolling(50, center=True, min_periods=1).mean()\n# define the k parameter -> the number of standard deviations from the mean which above them we classify as outliar\nk = 2\n# finding the top and bottom threshold for each datapoint\ndf1['A_top'] = df1['A_mean'] + k*df1['A_std']\ndf1['A_bot'] = df1['A_mean'] - k*df1['A_std']\n# creating a mask of booleans that places true if the row is an outliar and false if its not.\ndf1['A_out'] = ((df1['A'] > df1['A_top']) | (df1['A'] < df1['A_bot']))\n# applying the mask and replacing all outliers with nans.\ndf1['A_filtered'] = np.where(df1['A_out'],\n                                np.nan,  # use this if A_out is True\n                                df1['A']) # otherwise\ndf1\n                            \n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n      A_std\n      A_mean\n      A_top\n      A_bot\n      A_out\n      A_filtered\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.262273\n      1.520000\n      4.044546\n      -1.004546\n      False\n      0.0\n    \n    \n      2023-01-01 00:05:00\n      1\n      2.245251\n      -1.757193\n      1.899602\n      -0.999300\n      1.331858\n      1.423077\n      4.086793\n      -1.240639\n      False\n      1.0\n    \n    \n      2023-01-01 00:10:00\n      2\n      2.909648\n      0.854732\n      2.050146\n      -0.559504\n      1.462738\n      1.296296\n      4.221771\n      -1.629179\n      False\n      2.0\n    \n    \n      2023-01-01 00:15:00\n      3\n      3.483155\n      0.946937\n      1.921080\n      -0.402084\n      1.649114\n      1.142857\n      4.441085\n      -2.155371\n      False\n      3.0\n    \n    \n      2023-01-01 00:20:00\n      2\n      4.909169\n      0.462239\n      1.368470\n      -0.698579\n      1.880022\n      0.965517\n      4.725561\n      -2.794527\n      False\n      2.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 23:35:00\n      -37\n      1040.909898\n      -14808.285199\n      1505.020266\n      423.595984\n      2.988291\n      -32.633333\n      -26.656751\n      -38.609916\n      False\n      -37.0\n    \n    \n      2023-12-31 23:40:00\n      -36\n      1040.586547\n      -14808.874072\n      1503.915566\n      423.117110\n      3.038748\n      -32.655172\n      -26.577676\n      -38.732669\n      False\n      -36.0\n    \n    \n      2023-12-31 23:45:00\n      -37\n      1042.937417\n      -14808.690745\n      1505.479671\n      423.862810\n      3.077483\n      -32.714286\n      -26.559320\n      -38.869251\n      False\n      -37.0\n    \n    \n      2023-12-31 23:50:00\n      -36\n      1042.411572\n      -14809.212002\n      1506.174334\n      423.862432\n      3.088901\n      -32.814815\n      -26.637012\n      -38.992617\n      False\n      -36.0\n    \n    \n      2023-12-31 23:55:00\n      -35\n      1043.053520\n      -14809.990338\n      1505.767197\n      423.647007\n      3.128283\n      -32.884615\n      -26.628050\n      -39.141181\n      False\n      -35.0\n    \n  \n\n105120 rows × 11 columns\n\n\n\n\n\n21.2.2 ploting the results:\nUse %matplotlib widget to visualy inspect the results.\n\nfig, ax = plt.subplots(figsize=(10,4))\n\nax.plot(df1['A'], c='r', label='original')\nax.plot(df1['A_filtered'], c='b', label='filtered')\nax.plot(df1['A_bot'], c='orange',linestyle='--', label='envelope', alpha=0.5)\nax.plot(df1['A_top'], c='orange',linestyle='--', label='envelope', alpha=0.5)\n\nax.legend()\n\n<matplotlib.legend.Legend at 0x7f9028c8b5e0>\n\n\n\n\n\nNow let’s write a function so we can easly apply it to all our data:\n\ndef rolling_std_envelop(series, window_size=50, k=2):\n    series.name = 'original'\n    data = series.to_frame()\n    data['std'] = data['original'].rolling(window_size, center=True, min_periods=1).std()\n    # find the rolling mean\n    data['mean'] = data['original'].rolling(window_size, center=True, min_periods=1).mean()\n    # finding the top and bottom threshold for each datapoint\n    data['top'] = data['mean'] + k*data['std']\n    data['bottom'] = data['mean'] - k*data['std']\n    # creating a mask of booleans that places true if the row is an outliar and false if its not.\n    data['outliers'] = ((data['original'] > data['top']) | (data['original'] < data['bottom']))\n    # applying the mask and replacing all outliers with nans.\n    data['filtered'] = np.where(data['outliers'],\n                                np.nan,  # use this if outliers is True\n                                data['original']) # otherwise\n    return data['filtered']\n\nLet’s test the new function:\n\nfig, ax = plt.subplots(figsize=(10,4))\n\nax.plot(df1['A'], c='r', label='original')\nax.plot(rolling_std_envelop(df1['A']), c='b', label='filtered')\nax.legend()\n\n<matplotlib.legend.Legend at 0x7f9029b58070>\n\n\n\n\n\nNow let’s reload df1 so it will be clean (without all the added columns from before) and apply the function on all columns:\n\ndf1 = pd.read_csv('cleaning1.csv', \n                  index_col='date',     # set the column date as index \n                  parse_dates=True)     # turn to datetime format\n\n\ndf1_filtered = df1.copy()\ndf1_filtered\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-01 00:00:00\n      0\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      2023-01-01 00:05:00\n      1\n      2.245251\n      -1.757193\n      1.899602\n      -0.999300\n    \n    \n      2023-01-01 00:10:00\n      2\n      2.909648\n      0.854732\n      2.050146\n      -0.559504\n    \n    \n      2023-01-01 00:15:00\n      3\n      3.483155\n      0.946937\n      1.921080\n      -0.402084\n    \n    \n      2023-01-01 00:20:00\n      2\n      4.909169\n      0.462239\n      1.368470\n      -0.698579\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-12-31 23:35:00\n      -37\n      1040.909898\n      -14808.285199\n      1505.020266\n      423.595984\n    \n    \n      2023-12-31 23:40:00\n      -36\n      1040.586547\n      -14808.874072\n      1503.915566\n      423.117110\n    \n    \n      2023-12-31 23:45:00\n      -37\n      1042.937417\n      -14808.690745\n      1505.479671\n      423.862810\n    \n    \n      2023-12-31 23:50:00\n      -36\n      1042.411572\n      -14809.212002\n      1506.174334\n      423.862432\n    \n    \n      2023-12-31 23:55:00\n      -35\n      1043.053520\n      -14809.990338\n      1505.767197\n      423.647007\n    \n  \n\n105120 rows × 5 columns\n\n\n\n\ncolumns = df1_filtered.columns\n\nfor column in columns:\n    filtered_column = rolling_std_envelop(df1_filtered[column], window_size=50, k=2)\n    df1_filtered[column] = filtered_column\n\nNow let’s plot the results:\n\nfig, ax = plt.subplots(len(columns),1, sharex=True, figsize=(10,len(columns)*2))\n\nfor i, column in enumerate(columns):\n    ax[i].plot(df1[column], c='r', label='original')\n    ax[i].plot(df1_filtered[column], c='b', label='filtered')\n    ax[i].legend()\n    ax[i].set(ylabel=column)"
  },
  {
    "objectID": "outliers/outliers_challenge_part2.html#plateaus",
    "href": "outliers/outliers_challenge_part2.html#plateaus",
    "title": "21  challenge part 2",
    "section": "21.3 plateaus",
    "text": "21.3 plateaus\nNow what about the plateaus in the series D?\nThat’s another form of outliers.\nThe following function will find rows wich have more than n equal following values and replace them with NaNs.\n\n# function to copy paste:\ndef conseq_series(series, N):\n    \"\"\"\n    part A:\n    1. assume a string of 5 equal values. that's what we want to identify\n    2. diff produces a string of only 4 consecutive zeros\n    3. no problem, because when applying cumsum, the 4 zeros turn into a plateau of 5, that's what we want\n    so far, so good\n    part B:\n    1. groupby value_grp splits data into groups according to cumsum.\n    2. because cumsum is monotonically increasing, necessarily all groups will be composed of neighboring rows, no funny business\n    3. what are those groups made of? of rows of column 'series'. this specific column is not too important, because:\n    4. count 'counts' the number of elements inside each group.\n    5. the real magic here is that 'transform' assigns each row of the original group with the count result.\n    6. finally, we can ask the question: which rows belong to a string of identical values greater-equal than some threshold.\n    zehu, you now have a mask (True-False) with the same shape as the original series.\n\n    \"\"\"\n    # part A:\n    sumsum_series = (\n                   (series.diff() != 0)         # diff zero becomes false, otherwise true\n                      .astype('int')           # true -> 1  , false -> 0\n                      .cumsum()                # cumulative sum, monotonically increasing\n                  )\n    # part B:\n    mask_outliers = (\n                    series.groupby(sumsum_series)           # take original series and group it by values of cumsum\n                                .transform('count')        # now count how many are in each group, assign result to each existing row. that's what transform does\n                                .ge(N)                    # if row count >= than user-defined n_consecutives, assign True, otherwise False\n                    )\n    \n    # apply mask:\n    result = pd.Series(np.where(mask_outliers,\n                                np.nan,  # use this if mask_outliers is True\n                                series), # otherwise\n                            index=series.index)\n    return result\n\nLet’s apply it to the df1_filtered df so we will end with a cleaner signal\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.plot(df1_filtered['D'], color=\"tab:red\", label='original')\nax.plot(conseq_series(df1_filtered['D'], 10), c='tab:blue', label='filtered')\nax.set_ylabel('D')\nax.legend(frameon=False)\n\n<matplotlib.legend.Legend at 0x7f9029b0e9a0>\n\n\n\n\n\n\n21.3.1 TO DO:\nit’s not homework but you should definitely do it\n\nTry other filtering methods\nTweak the parameters\nUse other dataframes (1,2,3 and if you have your own so better)\nWrite custom filtering functions that you can save and use in your future work.\n\nYalla have fun"
  },
  {
    "objectID": "stationarity/motivation.html#questions",
    "href": "stationarity/motivation.html#questions",
    "title": "22  motivation",
    "section": "22.1 questions",
    "text": "22.1 questions\n\nIf I know the temperature right now, what does that tell me about the temperature 10 minutes from now? How about 100 minutes? 1000 minutes?\nThe same applies to the past: how heavily does past information inform today’s measurements?\ninformation degradation: how fast does information from a signal degrade, and gets swamped by noise?"
  },
  {
    "objectID": "stationarity/motivation.html#goals",
    "href": "stationarity/motivation.html#goals",
    "title": "22  motivation",
    "section": "22.2 goals",
    "text": "22.2 goals\n\ndiscuss what is noise, signal, trend, cycles, etc.\nlearn a useful framework to make sense of all the above.\nacquire statistical and time-series tools to analyze my data\neventually, all the above will be crucial to forcast future states.\n\nLet’s go! 🚀"
  },
  {
    "objectID": "stationarity/random.html#white-noise",
    "href": "stationarity/random.html#white-noise",
    "title": "23  random variables",
    "section": "23.1 white noise",
    "text": "23.1 white noise\nSee below three time series made up by three different stochastic (random) processes. All terms in each of the series are independent and identically distributed (iid), meaning that they are uncorrelated and taken from the same underlying distribution.\n\nAs we increase the length of the series, the statistics of each series reveal hints of the distributions they were sampled from:\n\n\nThe mathematical way of describing these series is thus: \\{X\\} represents the stochastic process (binary, uniform, etc), from which a specific series is randomly drawn:\n\n\\{ x_0, x_1, x_2, \\cdots \\}\n\nAll of these processes above have zero mean (\\mu=0) and a finite variance (\\sigma^2), which qualify them as white noise."
  },
  {
    "objectID": "stationarity/random.html#random-walk",
    "href": "stationarity/random.html#random-walk",
    "title": "23  random variables",
    "section": "23.2 random walk",
    "text": "23.2 random walk\nA random walk S_t (for t=0,1,2,\\cdots) is obtained by cumulatively summing iid random variables:\n\nS_t = X_1 + X_2 + X_3 + \\cdots + X_{t-1} + X_t\n\nwhere S_0=0.\nIn the case of a binary process, you can think of the random walk as the position of a person who takes a step forward every time a coin toss yields heads, and a step backward for tails. Of course, by differencing the random walk, we can recover the original random sequence:\n\nX_i = S_{i} - S_{i-1}.\n\nSee below the random walks associated with the three white noise processes from before:\n  \n\n\n\n\nBrockwell, Peter J., and Richard A. Davis. 2016. Introduction to Time Series and Forecasting. 3rd ed. Springer."
  },
  {
    "objectID": "stationarity/AR.html#ar1",
    "href": "stationarity/AR.html#ar1",
    "title": "24  autoregressive processes",
    "section": "24.1 AR(1)",
    "text": "24.1 AR(1)\nLet’s add some white noise (\\varepsilon) to this process.\n\nX_{t} = \\phi\\,X_{t-1} + \\varepsilon.\n\nThis is called an Autoregressive Process of order 1, or AR(1). Here, the current value x_{t} is dependent on the immediately preceding value x_{t-1}.\n\nWhat can we call these special cases?\n\n1 \\phi=1.0\n2 \\phi=0.0\n\nThe time series clearly explodes to infinity if \\phi>1, and seems to stay bounded for values equal or smaller than 1. We will come back to this observation in a little while, when we discuss stationarity."
  },
  {
    "objectID": "stationarity/AR.html#ar2",
    "href": "stationarity/AR.html#ar2",
    "title": "24  autoregressive processes",
    "section": "24.2 AR(2)",
    "text": "24.2 AR(2)\nWe can define a process that the current state is dependent on the two previous states, each with a different weight.\n\nX_{t} = \\phi_1\\,X_{t-1} + \\phi_2\\,X_{t-2} + \\varepsilon"
  },
  {
    "objectID": "stationarity/AR.html#arp",
    "href": "stationarity/AR.html#arp",
    "title": "24  autoregressive processes",
    "section": "24.3 AR(p)",
    "text": "24.3 AR(p)\nThe next thing to do is to generalize, and define an autoregressive process that depends on p previous states:\n\nx_{t} = \\phi_1\\,x_{t-1} + \\phi_2\\,x_{t-2} + \\cdots + \\phi_p\\,x_{t-p} + \\varepsilon\n\n\n\n\n\nShumway, Robert H., and David S. Stoffer. 2017. Time Series Analysis and Its Applications With R Examples. 4th ed. Springer. http://www.stat.ucla.edu/~frederic/415/S23/tsa4.pdf."
  },
  {
    "objectID": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "href": "stationarity/autocorrelation.html#mean-and-standard-deviation",
    "title": "25  autocorrelation",
    "section": "25.1 mean and standard deviation",
    "text": "25.1 mean and standard deviation\nLet’s call our time series X, and its length N. Then:\n\n\\begin{aligned}\n\\text{mean}& &\\mu &= \\frac{\\displaystyle\\sum_{i=1}^N X_i}{N} \\\\\n\\text{standard deviation}& &\\sigma &= \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^N (X_i-\\mu)^2}{N}}\n\\end{aligned}\n\nThe mean and standard deviation can be visualized thus:"
  },
  {
    "objectID": "stationarity/autocorrelation.html#expected-value",
    "href": "stationarity/autocorrelation.html#expected-value",
    "title": "25  autocorrelation",
    "section": "25.2 expected value",
    "text": "25.2 expected value\nThe expected value (or expectation) of a variable X is given by \nE[X] = \\sum_{i=1}^N X_i p_i.\n\np_i is the weight or probability that X_i occurs. For a time series, the probability p_i that a given point X_i is in the dataset is simply 1/N, therefore we can write the following measures in terms of expected values:\n\nmean, also called 1st moment: \n\\mu = E[X].\n\nvariance, also called 2nd moment: \n\\sigma^2 = E[(X-E[X])^2] = E[(X-\\mu)^2].\n Of course, \\sigma is called the standard deviation."
  },
  {
    "objectID": "stationarity/autocorrelation.html#covariance",
    "href": "stationarity/autocorrelation.html#covariance",
    "title": "25  autocorrelation",
    "section": "25.3 covariance",
    "text": "25.3 covariance\nThe covariance between two time series X and Y is given by\n\n\\begin{split}\n\\text{cov}(X,Y) &= E[(X-E[X])(Y-E[Y])]\\\\\n                &= E[(X-\\mu_X)(Y-\\mu_Y)]\n\\end{split}\n\nCompare this to the definition of the variance, and it is obvious that the covariance \\text{cov(X,X)} of a time series with itself is its variance."
  },
  {
    "objectID": "stationarity/autocorrelation.html#correlation",
    "href": "stationarity/autocorrelation.html#correlation",
    "title": "25  autocorrelation",
    "section": "25.4 correlation",
    "text": "25.4 correlation\nWe are almost there. I promise.\nThe fact that \\text{cov(X,X)} = \\sigma_X^2 begs us to define a new measure, the correlation:\n\n\\text{corr}(X,Y) = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y}.\n\nThis is convenient, because now we can say that the correlation of a time series with itself is \\text{corr}(X,X)=1.\nThis is also called the Pearson correlation coefficient, and the result has a value between 1 and -1.\n\n\n\nSource: Wikimedia"
  },
  {
    "objectID": "stationarity/autocorrelation.html#autocorrelation",
    "href": "stationarity/autocorrelation.html#autocorrelation",
    "title": "25  autocorrelation",
    "section": "25.5 autocorrelation",
    "text": "25.5 autocorrelation\nThe autocorrelation of a time series X is the answer to the following question:\n\nif we shift X by \\tau units, how similar will this be to the original signal?\n\nIn other words:\n\nhow correlated are X(t) and X(t+\\tau)?\n\nThe autocorrelation is expressed as \n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\n\n\nIn other disciplines, the autocorrelation is simply the autocovariance, i.e., it is not normalized by dividing by \\sigma^2. In time series it is assumed that the autocorrelation is always normalized, therefore between -1 and 1.\nThe autocorrelation function \\rho_{XX}(\\tau) provides a useful measure of the degree of dependence among the values of a time series at different times.\nA video is worth a billion words, so let’s see the autocorrelation in action:\n\nA few comments:\n\nThe autocorrelation for \\tau=0 (zero shift) is always 1.\n[Can you prove this? All the necessary equations are above!]"
  },
  {
    "objectID": "stationarity/stationarity.html#weak-stationarity",
    "href": "stationarity/stationarity.html#weak-stationarity",
    "title": "26  stationarity",
    "section": "26.1 weak stationarity",
    "text": "26.1 weak stationarity\nA time series is called weakly stationary if the following conditions are met:\n\nits mean \\mu does not vary in time: \n\\mu_X(t) = \\mu_X(t+\\tau)\n for all values of t and \\tau.\nits variance is finite for any time t: \n\\sigma^2_X(t) < \\infty.\n\nThe autocorrelation function between two lagged versions of the same time series, X(t_1) and X(t_2), depends only on the difference \\tau=t_2-t_1."
  },
  {
    "objectID": "stationarity/stationarity.html#strictstrong-stationarity",
    "href": "stationarity/stationarity.html#strictstrong-stationarity",
    "title": "26  stationarity",
    "section": "26.2 strict/strong stationarity",
    "text": "26.2 strict/strong stationarity\nAs the name suggests, strict stationarity requires much more than the above. It requires that the joint CDF of two lagged versions of X is the same. It is rare to require such strong terms, we will assume weak stationarity always from now on."
  },
  {
    "objectID": "stationarity/stationarity.html#stationarity-of-arp",
    "href": "stationarity/stationarity.html#stationarity-of-arp",
    "title": "26  stationarity",
    "section": "26.3 stationarity of AR(p)",
    "text": "26.3 stationarity of AR(p)\nLet’s write once more the definition of AR(p):\n\nx_{t} = \\phi_1\\,x_{t-1} + \\phi_2\\,x_{t-2} + \\cdots + \\phi_p\\,x_{t-p} + \\varepsilon\n\nWe can define the backward shift operator B as\n\nB X_t = X_{t-1}\n\nOf course, if B is applied p times, it shifts X thus:\n\nB^p X_t = X_{t-p}.\n\nWith that in hand, we can rewrite the definition of AR(p) as follows:\n\nx_{t} = \\phi_1\\, B\\,x_t + \\phi_2\\,B^2\\,x_t + \\cdots + \\phi_p\\,B^p\\,x_t + \\varepsilon\n\nWe have many terms with x_{t}, so let’s group them on the left-hand side:\n\nx_{t}\\, \\phi(B) = \\varepsilon,\n\nwhere the characteristic polynomial \\phi(B) is\n\n\\phi(B) = 1 - \\phi_1\\, B - \\phi_2\\,B^2 - \\cdots - \\phi_p\\,B^p\n\nIn order to determine the stationarity of AR(p), we need to find the roots of\n\n\\phi(B) = 0,\n\ncalled characteristic roots. These roots are often complex numbers.\n\nAR(p) is stationary if ALL the characteristic roots lie OUTSIDE the unit circle.\n\n\n\nThe reason for this is not obvious. A nice explanation can be found in this StackExchange response. Another good text is Tsay (2010, chap. 2).\nBrockwell and Davis (2016, chap. 2 p. 49 and chapter 3 p. 75) explain that, strictly speaking, an AR(p) process is stationary as long as the roots do not lie on the unit circle. However, in the case that the roots lie inside the unit circle, the process is noncausal, meaning that the present state depends on future states. In reality, everyone just ignores this point, and simply say that we require the roots to lie outside the unit circle to guarantee stationarity."
  },
  {
    "objectID": "stationarity/stationarity.html#stationarity-of-ar1",
    "href": "stationarity/stationarity.html#stationarity-of-ar1",
    "title": "26  stationarity",
    "section": "26.4 stationarity of AR(1)",
    "text": "26.4 stationarity of AR(1)\nWe need to solve the roots of the characteristic equation\n\n1 - \\phi_1\\, B = 0.\n\nThe only root is\n\nB = \\frac{1}{\\phi_1}\n\nBecause we require that this root is greater than 1 (in absolute value), we have that:\n\n\\left|\\frac{1}{\\phi_1}\\right| > 1\\quad \\longrightarrow \\quad |\\phi| < 1.\n\nThis result corroborates our conclusion from before:"
  },
  {
    "objectID": "stationarity/stationarity.html#stationarity-of-ar2",
    "href": "stationarity/stationarity.html#stationarity-of-ar2",
    "title": "26  stationarity",
    "section": "26.5 stationarity of AR(2)",
    "text": "26.5 stationarity of AR(2)\nWe need to solve the roots of the characteristic equation\n\n1 - \\phi_1 B - \\phi_2 B^2 = 0.\n\nFor the AR(2) time series from before, here are the roots of the characteristic polynomial plotten on the complex plane:\n\n\nBecause complex roots come in pairs, I plotted above only one of the roots, the one with positive imaginary component. The second panel (\\phi_1=1.16,\\phi_2=-0.33) has two real roots, so both are plotted in blue.\nThe AR(2) time series will have a periodic component if the roots are complex, and the frequency of oscillation is\n\nf_0 = \\frac{1}{2\\pi}\\cos^{-1}\\left( \\frac{\\phi_1}{2\\sqrt{(-\\phi_2)}} \\right)\n\n\n\nSee an interesting discussion in David Josephs’ excellent time series webpage\nPlay with the widget below to get a feel of how the complex roots depend on the values of \\phi_1 and \\phi_2. In the left panel you can move the point A to choose different \\phi values, and on the right you see the complex roots of the polynomial instantly updated. As long as the point A is inside the blue triangle, the roots will be outside the unit circle, and therefore the process will be stationary. For \\phi_2 values above (below) the red line, the complex roots will be real (complex conjugates).\n\n\n\n\nGeogebra app made by Yair Mau (2024)\nConversely, play with the position of one of the complex roots, and see how this influences the value of \\phi_1,\\phi_2.\n\n\nThe two roots are easy to find from \\phi_1,\\phi_2, you just need to solve\n\n1 - \\phi_1 B - \\phi_2 B^2 = 0\n\nfor B. What if you choose two roots z_1,z_2 and want to derive from them the value of \\phi_1,\\phi_2? Just use this:\n\n\\begin{split}\n\\phi_1 &= \\frac{z_1+z_2}{z_1\\cdot z_2}\\\\\n& \\\\\n\\phi_2 &= -\\frac{1}{z_1\\cdot z_2}\n\\end{split}"
  },
  {
    "objectID": "stationarity/stationarity.html#stationarity-of-ar4",
    "href": "stationarity/stationarity.html#stationarity-of-ar4",
    "title": "26  stationarity",
    "section": "26.6 stationarity of AR(4)",
    "text": "26.6 stationarity of AR(4)\nConceptually, this is exactly like AR(2). In case you ever need to choose AR(4) \\phi values, play with the widget below and see if you can put all the roots outside the unit circle.\n\n\n\n\n\n\nBrockwell, Peter J., and Richard A. Davis. 2016. Introduction to Time Series and Forecasting. 3rd ed. Springer.\n\n\nTsay, R. S. 2010. Analysis of Financial Time Series. Wiley."
  },
  {
    "objectID": "stationarity/ACF_and_PACF_graphs.html#acf",
    "href": "stationarity/ACF_and_PACF_graphs.html#acf",
    "title": "27  ACF and PACF graphs",
    "section": "27.1 ACF",
    "text": "27.1 ACF\nWe need to calculate now the autocorrelation function of our series: \n\\rho_{XX}(\\tau) = \\frac{E\\left[ (X_t - \\mu)(X_{t+\\tau} - \\mu) \\right]}{\\sigma^2}\n\n\ndef compute_acf(series):\n    N = len(series)\n    lags = np.arange(N)\n    acf = np.zeros_like(lags)\n    series = (series - series.mean()) / series.std()\n    for i in lags:\n        acf[i] = np.sum(series[i:] * series[:N-i])\n    acf = acf / N\n    return lags, acf\n\n\n\nplot acf\nfig, ax = plt.subplots()\nlags, acf = compute_acf(ar2_values)\nax.plot([0, N], [0]*2, color=\"black\", ls=\"--\")\nax.plot(lags, acf)\nax.set(xlabel=\"lag\",\n       ylabel=\"acf\",\n       xlim=[0, N]);\n\n\n\n\n\nNotice that the ACF always starts at 1 for zero lag, and it gets closer to zero as the lag increases.\n\nIntuitive interpretation: Two measurements taken within a short time interval (lag) should be similar, therefore their correlation is expected to be high. As we compare measurements from increasing time intervals, they are less and less similar to one another, therefore their correlation goes down.\nMathematical interpretation: Take a look at the code we wrote above. As the lags grows, the length of both arrays keeps shrinking, but we still divide the result by N. The logical conclusion is that when \\tau=N the ACF will be exactly zero.\n\nThe statsmodels package also offers an easy way to plot the ACF, let’s compare our calculation with the built-in function:\n\n\nplot\nfig, ax = plt.subplots()\nax.plot([0, N], [0]*2, color=\"black\", ls=\"--\")\nsm.graphics.tsa.plot_acf(ar2_values, lags= N-1, ax=ax, label=\"statsmodels\", alpha=None, use_vlines=False)\nax.plot(lags, acf, color=\"tab:orange\", label=\"our own acf\")\nax.legend()\nax.set(ylim=[-0.5, 1.1],\n       xlim=[0, N],\n       title=\"compare acf\",\n       xlabel=\"lag\",\n       ylabel=\"acf\");\n\n\n\n\n\nExcellent! From now on we will continue using statsmodels functions. We can spice up the ACF graph, by showing an envelope of 95% confidence interval.\n\n\nplot\nfig, ax = plt.subplots()\nsm.graphics.tsa.plot_acf(ar2_values, lags= N-1, ax=ax, alpha=.05)\nax.set(ylim=[-1.1, 1.1],\n       title=\"now confidence interval\")\nax.set(ylim=[-1.1, 1.1],\n       title=\"now with 95% confidence interval\",\n       xlabel=\"lag\",\n       ylabel=\"acf\");\n\n\n\n\n\nIf an autocorrelation value at a specific lag falls outside the confidence interval, it suggests that the autocorrelation at that lag is statistically significant. In other words, there is evidence of correlation at that lag. If an autocorrelation value is within the confidence interval, it suggests that the autocorrelation at that lag is not statistically significant, and any observed correlation might be due to random noise. The width of the confidence interval is influenced by the significance level. For a 95% confidence interval, it means that you are 95% confident that the true autocorrelation lies within the interval. If you choose a higher confidence level, the interval will become wider, making it harder to reject the null hypothesis of no correlation.\n\n27.1.1 problem?\nThere is something a bit troubling about the ACF graph. We can learn from it how fast the correlation between two points in time falls, but this analysis is not too clean. Assume that the present state x_t is only dependent on one time step back, x_{t-1}. Because x_{t-1} is dependent on x_{t-2}, the result is that we will find that x_{t} is weakly dependent on x_{t-2}, although the direct dependence is zero.\nThe Partial ACF (PACF) solves this problem. It removes the intermediate effects between two points, and returns only the direct influence of one time instant on another one lagged by \\tau. Let’s see how it looks like for the proceess above."
  },
  {
    "objectID": "stationarity/ACF_and_PACF_graphs.html#pacf",
    "href": "stationarity/ACF_and_PACF_graphs.html#pacf",
    "title": "27  ACF and PACF graphs",
    "section": "27.2 PACF",
    "text": "27.2 PACF\n\n\nplot\nfig, (ax1, ax2) = plt.subplots(2,1)\nfig.subplots_adjust(hspace=0.05)\nsm.graphics.tsa.plot_pacf(ar2_values, lags=25, ax=ax1, alpha=.05)\nax1.set(ylim=[-1.1, 1.1],\n       title=\"now confidence interval\")\nax1.set(ylim=[-1.1, 1.1],\n       title=\"Comparison between ACF and PACF\",\n       xlabel=\"lag\",\n       ylabel=\"PACF\");\n\nsm.graphics.tsa.plot_acf(ar2_values, lags= 25, ax=ax2, alpha=.05, title=None)\nax2.set(ylim=[-1.1, 1.1],\n       xlabel=\"lag\",\n       ylabel=\"ACF\");\n\n\n\n\n\nWe see three bars significantly far from the confidence interval. The leftmost shows \\text{PACF}(\\tau=0)=1, which is expected, so let’s not discuss it. The two next bars are the really important ones, they show the greatest correlation. From then on, the correlation for lags greater than 2 is not significant. With PACF’s help, we can infer that the original AR processes must have been of order 2."
  },
  {
    "objectID": "stationarity/ACF_and_PACF_graphs.html#discussion",
    "href": "stationarity/ACF_and_PACF_graphs.html#discussion",
    "title": "27  ACF and PACF graphs",
    "section": "27.3 discussion",
    "text": "27.3 discussion\nWhat can we say about the following series?\n\n27.3.1 sine wave\n\n\nShow the code\nN = 500\ntime = np.arange(N)\nperiod = 70\nomega = 2.0 * np.pi / period\nsignal = np.sin(omega * time)\n\nfig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\nfig.subplots_adjust(hspace=0.4)\n\nax1.plot(time, signal)\nax1.set(ylabel=\"signal\",\n        title=\"sine wave\",\n        xlabel=\"time\")\n\nsm.graphics.tsa.plot_acf(signal, ax=ax2, alpha=.05, title=None, lags=N-1)\nax2.set(ylim=[-1.1, 1.1],\n       xlabel=\"lag\",\n       ylabel=\"ACF\");\n\nsm.graphics.tsa.plot_pacf(signal, ax=ax3, alpha=.05, title=None)\nax3.set(ylim=[-1.1, 1.1],\n       ylabel=\"PACF\",\n       xlabel=\"lag\");\n\n\n\n\n\n\n\n27.3.2 white noise\n\n\nShow the code\nN = 500\ntime = np.arange(N)\nsignal = np.random.normal(size=N)\n\nfig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\nfig.subplots_adjust(hspace=0.4)\n\nax1.plot(time, signal)\nax1.set(ylabel=\"signal\",\n        title=\"white noise\",\n        xlabel=\"time\")\n\nsm.graphics.tsa.plot_acf(signal, ax=ax2, alpha=.05, title=None, lags=25)\nax2.set(ylim=[-1.1, 1.1],\n       xlabel=\"lag\",\n       ylabel=\"ACF\");\n\nsm.graphics.tsa.plot_pacf(signal, ax=ax3, alpha=.05, title=None)\nax3.set(ylim=[-1.1, 1.1],\n       ylabel=\"PACF\",\n       xlabel=\"lag\");\n\n\n\n\n\n\n\n27.3.3 random walk\n\n\nShow the code\nN = 500\ntime = np.arange(N)\nsignal = np.cumsum(np.random.normal(size=N))\n\nfig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\nfig.subplots_adjust(hspace=0.4)\n\nax1.plot(time, signal)\nax1.set(ylabel=\"signal\",\n        title=\"random walk\",\n        xlabel=\"time\")\n\nsm.graphics.tsa.plot_acf(signal, ax=ax2, alpha=.05, title=None, lags=N-1)\nax2.set(ylim=[-1.1, 1.1],\n       xlabel=\"lag\",\n       ylabel=\"ACF\");\n\nsm.graphics.tsa.plot_pacf(signal, ax=ax3, alpha=.05, title=None)\nax3.set(ylim=[-1.1, 1.1],\n       ylabel=\"PACF\",\n       xlabel=\"lag\");"
  },
  {
    "objectID": "stationarity/from_AR_to_ARIMA.html#arp",
    "href": "stationarity/from_AR_to_ARIMA.html#arp",
    "title": "28  from AR to ARIMA",
    "section": "28.1 AR(p)",
    "text": "28.1 AR(p)\nAn autoregressive process X is one that depend on p past states:\n\nX_{t} = \\phi_1\\,X_{t-1} + \\phi_2\\,X_{t-2} + \\cdots + \\phi_p\\,X_{t-p} + \\varepsilon\n\nFrom what we already learned, if the complex roots of the polynomial\n\n\\phi(B) = 1 - \\phi_1\\, B - \\phi_2\\,B^2 - \\cdots - \\phi_p\\,B^p\n\nlie outside the unit circle, then the AR process is causal and stationary."
  },
  {
    "objectID": "stationarity/from_AR_to_ARIMA.html#maq",
    "href": "stationarity/from_AR_to_ARIMA.html#maq",
    "title": "28  from AR to ARIMA",
    "section": "28.2 MA(q)",
    "text": "28.2 MA(q)\nSimilarly, a moving average process X is one that depend on q past noise steps:\n\nX_{t} = \\varepsilon_t + \\theta_1\\,\\varepsilon_{t-1} + \\theta_2\\,\\varepsilon_{t-2} + \\cdots + \\theta_p\\,\\varepsilon_{t-q}\n\nThis has nothing to do with the sliding averages used for smoothing we studied before, it’s just the same name for a different concept.\nNote that this equation is identical in structure to that of AR(p), but with weights \\theta standing for \\phi, and past noise \\varepsilon_{t-i} standing in for past states X_{t-i}.\nThis process also has its characteristic polynomial:\n\n\\theta(B) = 1 - \\theta_1\\, B - \\theta_2\\,B^2 - \\cdots - \\theta_p\\,B^p\n\nThe complex roots of this polynomial are also important. As long as the roots are outside the unit circle, the MA(q) process will be considered invertible, which is to say that it can be transformed into an AR(\\infty) process.\n\n\nThe story is of course more complex than that. Using intelligent mathematical tricks (substitutions), one can change the noise term to make roots move from inside the unit circle to the outside, so effectively there shouldn’t be any problems as long as there aren’t any roots exacly on the unit circle."
  },
  {
    "objectID": "stationarity/from_AR_to_ARIMA.html#armapq",
    "href": "stationarity/from_AR_to_ARIMA.html#armapq",
    "title": "28  from AR to ARIMA",
    "section": "28.3 ARMA(p,q)",
    "text": "28.3 ARMA(p,q)\nAn ARMA(p,q) process is simply the combination of an AR(p) and an MA(q) process:\n\n\\begin{split}\nX_{t} &= \\phi_1\\,X_{t-1} + \\phi_2\\,X_{t-2} + \\cdots + \\phi_p\\,X_{t-p} \\\\\n      &+ \\varepsilon_t + \\theta_1\\,\\varepsilon_{t-1} + \\theta_2\\,\\varepsilon_{t-2} + \\cdots + \\theta_p\\,\\varepsilon_{t-p}\n\\end{split}"
  },
  {
    "objectID": "stationarity/from_AR_to_ARIMA.html#acf-and-pacf",
    "href": "stationarity/from_AR_to_ARIMA.html#acf-and-pacf",
    "title": "28  from AR to ARIMA",
    "section": "28.4 ACF and PACF",
    "text": "28.4 ACF and PACF\nThe graphs for the autocorrelation and partial autocorrrelation functions can be very useful to identify the order p and q of an ARMA(p,q) process.\n\n\nimport stuff\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.stattools import adfuller\n\n\n\n\ngenerate ARMA(4,0) process\nnp.random.seed(0)\nn = 1000  # number of data points\nphi_list = np.array([0.8, -0.28, 0.8, -0.36])\nar_coefs = np.insert(-phi_list, 0, 1)  # AR coefficients. append 1 at the beginning\nma_coefs = [1] # MA coefficients\n\narma_process = ArmaProcess(ar_coefs, ma_coefs)\ndata = arma_process.generate_sample(nsample=n)\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 8))\nfig.subplots_adjust(hspace=0.4)  # increase vertical space between panels\n\nax1 = axes[0]\nax1.plot(data)\nax1.set(xlabel='time',\n        ylabel='value',\n        title='ARMA(4,0) process',\n       )\n\n# plot ACF and PACF graphs\nax2 = axes[1]\nsm.graphics.tsa.plot_acf(data, lags=n-1, ax=ax2, title=None)\nax2.set(ylabel=\"ACF\",\n        xlabel=\"lags\")\n\nax3 = axes[2]\nsm.graphics.tsa.plot_pacf(data, lags=30, ax=ax3, title=None)\nax3.set(ylim=[-1.1, 1.1],\n        ylabel='PACF',\n        xlabel=\"lags\");\n\n\n\n\n\nNote that for the ARMA(4,0) process, the last significant PACF value is at lag \\tau=4.\n\n\ngenerate ARMA(0,3) process\nnp.random.seed(0)\nn = 1000  # number of data points\ntheta_list = np.array([0.4, -0.3, 0.8])\nma_coefs = np.insert(-theta_list, 0, 1)  # MA coefficients. append 1 at the beginning\nar_coefs = [1]\n\narma_process = ArmaProcess(ar_coefs, ma_coefs)\ndata = arma_process.generate_sample(nsample=n)\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 8))\nfig.subplots_adjust(hspace=0.4)\n\nax1 = axes[0]\nax1.plot(data)\nax1.set(xlabel='time',\n        ylabel='value',\n        title='ARMA(0,3) process',\n       )\n\n# plot ACF and PACF graphs\nax2 = axes[1]\nsm.graphics.tsa.plot_acf(data, lags=20, ax=ax2, title=None)\nax2.set(ylabel=\"ACF\",\n        xlabel=\"lags\")\n\nax3 = axes[2]\nsm.graphics.tsa.plot_pacf(data, lags=30, ax=ax3, title=None)\nax3.set(ylim=[-1.1, 1.1],\n        ylabel='PACF',\n        xlabel=\"lags\");\n\n\n\n\n\nFor the ARMA(0,3) process, the last significant ACF value is at lag \\tau=3.\n\n\ngenerate ARMA(4,3) process\nnp.random.seed(0)\nn = 1000  # number of data points\ntheta_list = np.array([0.4, -0.3, 0.8])\nphi_list = np.array([0.8, -0.28, 0.8, -0.36])\nar_coefs = np.insert(-phi_list, 0, 1)  # AR coefficients\nma_coefs = np.insert(-theta_list, 0, 1)  # MA coefficients\n\narma_process = ArmaProcess(ar_coefs, ma_coefs)\ndata = arma_process.generate_sample(nsample=n)\n\n# Create a single figure with panels\nfig, axes = plt.subplots(3, 1, figsize=(8, 8))\nfig.subplots_adjust(hspace=0.4)\n\n\n# Plot the ARMA process\nax1 = axes[0]\nax1.plot(data)\nax1.set(xlabel='time',\n        ylabel='value',\n        title='ARMA(4,3) process',\n       )\n\n# Plot ACF and PACF graphs\nax2 = axes[1]\nsm.graphics.tsa.plot_acf(data, lags=20, ax=ax2, title=None)\nax2.set(ylabel=\"ACF\",\n        xlabel=\"lags\")\n\nax3 = axes[2]\nsm.graphics.tsa.plot_pacf(data, lags=30, ax=ax3, title=None)\nax3.set(ylim=[-1.1, 1.1],\n        ylabel='PACF',\n        xlabel=\"lags\");\n\n\n\n\n\nThis table from Shumway and Stoffer (2017, 108) is useful to sum up what we’ve learned so far.\n\n\n\n\n\n\n\n\n\n\nAR(p)\nMA(q)\nARMA(p,q)\n\n\n\n\nACF\ngradually goes down\ncuts off after lag q\ngradually goes down\n\n\nPACF\ncuts off after lag p\ngradually goes down\ngradually goes down"
  },
  {
    "objectID": "stationarity/from_AR_to_ARIMA.html#non-stationary-data-and-adf-test",
    "href": "stationarity/from_AR_to_ARIMA.html#non-stationary-data-and-adf-test",
    "title": "28  from AR to ARIMA",
    "section": "28.5 Non-stationary data and ADF test",
    "text": "28.5 Non-stationary data and ADF test\nThe following is partially based on Chatfield (2016, chap. 3, page 63).\nWhat do we do if it turns out that our data is not stationary? Heck, how can we even tell if our data is stationary or not? The most common stationarity test is the Augmented Dickey–Fuller (ADF) test. This is not a trivial subject that can be completely understood in a few words, so I’ll give the very basic intuition here.\nA stationary time series has a constant mean \\mu. If at a given instant t our state X_t is way above the mean, we would expect that, with a high probability, the next step brings it closer to the mean. This is to say that the the difference between two consecutive states X_t-X_{t-1} depends on the value of X_t! Nonstationary time series do not show this behavior: the differences between two time steps do not depend on the state value. The idea described here is for the Dickey-Fuller test. The Augmented Dickey-Fuller test is basically the same, but for time lags p between states, not only 1.\nThe ADF test has a null hypothesis that the time series is not stationary. By applying the test to a given time series, we get a p-value as one of the results. The smaller the p-value, the more evidence we have to reject the null hypothesis, and therefore conclude that our time series is indeed stationary.\nLet’s see an example:\n\n\nShow the code\n# Generate ARIMA(1,1,2) process with differencing\nN = 500\nnp.random.seed(1)\narima_112_diff = sm.tsa.arma_generate_sample(ar=[1, -0.5], ma=[1, 0.7, 0.3], nsample=N)\narima_112 = np.cumsum(arima_112_diff)\n\nfig, ax = plt.subplots()\nax.plot(arima_112)\nax.set(xlabel=\"time\",\n       ylabel=\"signal\",\n       title=\"a non-stationary time series\")\nplt.show()\n\nresult = adfuller(arima_112)\nprint('p-value: ', result[1])\n\n\n\n\n\np-value:  0.591478751185507\n\n\nSo what do we do if we have a non-stationary time series? One common solution is to apply successive differencing operations, until the outcome becomes stationary.\nLet’s define the difference operator \\nabla as\n\n\\nabla X_t = X_t - X_{t-1}.\n\nNow recalling that the backward shift operator B is defined as\n\nB X_t = X_{t-1},\n\nwe can rewrite the difference operator as\n\n\\begin{split}\n\\nabla X_t &= X_t - BX_t\\\\\n           &= (1-B)X_t.\n\\end{split}\n\nIf we apply the difference operator d times, then we denote this as\n\nW_t = \\nabla^d X_t = (1-B)^d X_t.\n\nLet’s apply the difference operator once to the time series plotted above, and then apply the ADF test.\n\n\nShow the code\nfig, ax = plt.subplots()\nax.plot(arima_112_diff)\nax.set(xlabel=\"time\",\n       ylabel=\"signal\",\n       title=\"the same time series as above, after one differencing\")\nplt.show()\n\nresult = adfuller(arima_112_diff)\nprint('p-value: ', result[1])\n\n\n\n\n\np-value:  4.7341140554650393e-14"
  },
  {
    "objectID": "stationarity/from_AR_to_ARIMA.html#arimapdq",
    "href": "stationarity/from_AR_to_ARIMA.html#arimapdq",
    "title": "28  from AR to ARIMA",
    "section": "28.6 ARIMA(p,d,q)",
    "text": "28.6 ARIMA(p,d,q)\nWe are ready to describe an Autoregressive (AR) Integrated (I) Moving Average (MA) process:\n\n\\begin{split}\nW_t &= \\phi_1 W_{t-1} + \\phi_2 W_{t-2}+\\cdots \\phi_q W_{t-q} \\\\\n    &+ \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2}+\\cdots \\theta_q \\varepsilon_{t-q}\n\\end{split}\n\nRearranging the terms in this equation, we can also express an ARIMA(p,d,q) process as\n\n\\phi(B)(1-B)^d X_t = \\theta(B)\\epsilon_t.\n\nLet’s try to put this in a context we already know. We saw that a random walk is the integrated version of a white noise. The random walk can be interpreted as a special case of an AR(1) process for \\phi=1. However, an AR process is usually called as such when it is stationary. Because a white noise can be understood as an ARMA(0,0) process, and because differencing the random walk yields a white noise, we can say that the white noise is an ARIMA(0,1,0) process.\nLet’s see two examples of ARIMA processes.\n\n\nShow the code\n\n\n# Generate ARIMA(2,2,1) process with differencing\narima_221_diff = sm.tsa.arma_generate_sample(ar=[1, -0.18, 0.06], ma=[1, -0.5], nsample=N)\n# arima_221 = np.cumsum(arima_221_diff) \narima_221 = np.cumsum(np.cumsum(arima_221_diff)) # \n\n# Plot the ARIMA(1,1,2) process\nfig, ax = plt.subplots()\nax.plot(arima_112)\nax.set_title('ARIMA(1,1,2) process')\nplt.show()\n\n# Plot the ARIMA(2,2,1) process\nfig, ax = plt.subplots()\nax.plot(arima_221)\nax.set_title('ARIMA(2,2,1) process')\nplt.show()\n\n\n\n\n\n\n\n\nWe will not fully delve into forecasting right now, but it would be nice to see a real application of ARIMA. If we can reasonably well estimate the parameters associated with a given ARIMA(p,d,q) process, we can use this knowledge to predict future states within a confidence interval. In the simulations below, we see forecasts an ARIMA(2,2,0) process.\n\n\nwrite function to forecast\ndef arima_forecast(series, ar_coeff):\n    s = series.copy()\n    phi1 = -ar_coeff[1]\n    phi2 = -ar_coeff[2]\n    start_index = np.argmax(np.isnan(s))\n    for i in np.arange(start_index,len(series)):\n        s[i] = phi1 * s[i-1] + phi2 * s[i-2] + np.random.normal()\n    return s\n\nnp.random.seed(1998)\narima_220_diff = sm.tsa.arma_generate_sample(ar=[1, -0.18, 0.06], ma=[1], nsample=N)\narima_220 = np.cumsum(np.cumsum(arima_220_diff)) # \n\n\nl = 380\nmissing = arima_220_diff.copy()\nmissing[l:] = np.nan\n\n\n\n\nplot forecasts\nfig, axes = plt.subplots(3, 1, figsize=(8, 8))\nfig.subplots_adjust(hspace=0.1)\n\nxlim = [0, len(missing)]\nylim = [-1000,10000]\n\nax0 = axes[0]\nntries = 10\nfor i in range(ntries):\n    np.random.seed(i)\n    try_diff = arima_forecast(missing, [1, -0.18, 0.06])\n    t = np.cumsum(np.cumsum(try_diff))\n    ax0.plot(t, color=\"black\", alpha=0.3)\nax0.plot(arima_220, color=\"xkcd:hot pink\", lw=3, label=\"data\")\nax0.plot([len(missing),len(missing)+1], [0]*2, color=\"black\", alpha=0.3, label=\"forecast\")\nax0.set(xticklabels=[],\n        xlim=xlim,\n        ylim=ylim)\nax0.text(0.5, 0.98, f\"{ntries} forecasts\",\n         transform=ax0.transAxes, ha=\"center\", va=\"top\")\nax0.legend(frameon=False)\nax0.plot([380]*2, ylim, color=\"black\", ls=\":\")\n\nax1 = axes[1]\nntries = 100\nfor i in range(ntries):\n    np.random.seed(i)\n    try_diff = arima_forecast(missing, [1, -0.18, 0.06])\n    t = np.cumsum(np.cumsum(try_diff))\n    ax1.plot(t, color=\"black\", alpha=0.1)\nax1.plot(arima_220, color=\"xkcd:hot pink\", lw=3)\nax1.set(xticklabels=[],\n        ylabel=\"signal\",\n        xlim=xlim,\n        ylim=ylim)\nax1.text(0.5, 0.98, f\"{ntries} forecasts\",\n         transform=ax1.transAxes, ha=\"center\", va=\"top\")\nax1.plot([380]*2, ylim, color=\"black\", ls=\":\")\n\n\nax2 = axes[2]\nntries = 1000\nfor i in range(ntries):\n    np.random.seed(i)\n    try_diff = arima_forecast(missing, [1, -0.18, 0.06])\n    t = np.cumsum(np.cumsum(try_diff))\n    ax2.plot(t, color=\"black\", alpha=0.03)\nax2.plot(arima_220, color=\"xkcd:hot pink\", lw=3)\nax2.set(xlabel='time',\n        xlim=xlim,\n        ylim=ylim)\nax2.text(0.5, 0.98, f\"{ntries} forecasts\",\n         transform=ax2.transAxes, ha=\"center\", va=\"top\")\nax2.plot([380]*2, ylim, color=\"black\", ls=\":\")\n\n\n\n\n\nWe will discuss this later in the course, but estimating the parameters is quite easy:\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\narima_model = ARIMA(arima_220, order=(2,2,0))\nmodel = arima_model.fit()\nprint(model.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                      y   No. Observations:                  500\nModel:                 ARIMA(2, 2, 0)   Log Likelihood                -703.606\nDate:                Tue, 06 Feb 2024   AIC                           1413.212\nTime:                        13:12:24   BIC                           1425.844\nSample:                             0   HQIC                          1418.170\n                                - 500                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.2171      0.045      4.782      0.000       0.128       0.306\nar.L2          0.0636      0.045      1.401      0.161      -0.025       0.153\nsigma2         0.9878      0.072     13.739      0.000       0.847       1.129\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 5.16\nProb(Q):                              0.99   Prob(JB):                         0.08\nHeteroskedasticity (H):               1.08   Skew:                             0.05\nProb(H) (two-sided):                  0.62   Kurtosis:                         2.51\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n\n\n\nChatfield, C. 2016. The Analysis of Time Series: An Introduction, Sixth Edition. Chapman & Hall/CRC Texts in Statistical Science. CRC Press.\n\n\nShumway, Robert H., and David S. Stoffer. 2017. Time Series Analysis and Its Applications With R Examples. 4th ed. Springer. http://www.stat.ucla.edu/~frederic/415/S23/tsa4.pdf."
  },
  {
    "objectID": "stationarity/practice.html",
    "href": "stationarity/practice.html",
    "title": "29  practice",
    "section": "",
    "text": "30 White noise\nIf we randomly draw values from the same distribution we will get white noise.\nIf we cumulatively sum the white noise, we then get a random walk\nX_{t} = \\phi\\,X_{t-1} + \\varepsilon.\nThis is called an Autoregressive Process of order 1, or AR(1). Here, the current value X_{t} is dependent on the immediately preceding value X_{t-1}."
  },
  {
    "objectID": "stationarity/practice.html#differencing",
    "href": "stationarity/practice.html#differencing",
    "title": "29  practice",
    "section": "31.1 Differencing",
    "text": "31.1 Differencing\nGiven an array\na = [a_0, a_1, a_2, ..., a_{n-1}] the operation performed by np.diff(a) can be represented as:\n\\Delta a = [\\Delta a_1, \\Delta a_2, ..., \\Delta a_{n-1}] where\n\\Delta a_i = a_{i} - a_{i-1} \\quad \\text{for} \\quad i = 1, 2, ..., n-1\nIf we difference the random walk we will get the white noise.\n\nfig, ax = plt.subplots(2,1, figsize=(10,4), sharex=True)\nax[0].plot(binary_noise, label='original')\nax[0].plot(np.diff(walk_binary, prepend=0), label='diff', linestyle='--')\nax[0].set_title('binary')\nax[0].legend()\nax[0].set_xlim(0,100)\n\nax[1].plot(gaussian_noise, label='original')\nax[1].plot(np.diff(walk_gaussian, prepend=0), label='diff', linestyle='--')\nax[1].set_title('gaussian')\nax[1].legend()\nax[1].set_xlim(0,100)\n\n(0.0, 100.0)\n\n\n\n\n\nAnother way of understanding this: the python operations cumsum and diff are each other’s inverse."
  },
  {
    "objectID": "stationarity/practice.html#arp",
    "href": "stationarity/practice.html#arp",
    "title": "29  practice",
    "section": "32.1 AR(p)",
    "text": "32.1 AR(p)\nThe next thing to do is to generalize, and define an autoregressive process that depends on p previous states:\n\nx_{t} = \\phi_1\\,x_{t-1} + \\phi_2\\,x_{t-2} + \\cdots + \\phi_p\\,x_{t-p} + \\varepsilon\n\n\n# Function to generate AR(p) time series\n# this function can recive p as an integer and then is will draw random phi values\n# or, you can pass p as a np array of the specific phi values you want.\ndef generate_ar(n, p):\n    # Check if p is an integer or an array\n    if isinstance(p, int):\n        # Generate random coefficients between -1 and 1\n        phi = np.random.uniform(-1, 1, size=p)\n    elif isinstance(p, np.ndarray):\n        phi = p  # Use the provided array as coefficients\n    else:\n        raise ValueError(\"p should be either an integer or a NumPy array\")\n    \n    print(phi)\n    # Generate white noise\n    noise = np.random.normal(0, 1, n)\n    \n    # Initialize time series array\n    ar_series = np.zeros(n)\n    \n    for i in range(phi.size, n):\n        ar_series[i] = np.dot(phi, ar_series[i-phi.size:i]) + noise[i]\n    \n    return ar_series\n\n\n# plot using p as an int\np = 4\nar = generate_ar(n, p)\nfig, ax = plt.subplots(figsize=(10,4))\nax.plot(ar)\nax.set_title(f'AR({p})')\n\n[-0.82679831 -0.50310415 -0.68089179  0.1555622 ]\n\n\nText(0.5, 1.0, 'AR(4)')\n\n\n\n\n\n\n32.1.1 using specific \\phi values\nIn the cell below we can specify specific \\phi values.\nUse the interactive tool from our website to chose the right values.\nRemember, if one of the roots is inside the unit circle, the series will be not stationary.\n\n# plot using p as an array of phi values\n# the order should be [phi2, phi1]\np = np.array([-0.97,-1.88])\n# p = np.array([-1.88,-0.97])\n\nar2 = generate_ar(n, p)\nfig, ax = plt.subplots(figsize=(10,4))\n\nax.plot(ar2)\nax.set_title(f'AR({p})')\n\n[-0.97 -1.88]\n\n\nText(0.5, 1.0, 'AR([-0.97 -1.88])')\n\n\n\n\n\n\n\n32.1.2 Weak stationarity\n\nits mean \\mu does not vary in time: \n\\mu_X(t) = \\mu_X(t+\\tau)\n for all values of t and \\tau.\nits variance is finite for any time t: \n\\sigma^2_X(t) < \\infty.\n\nThe autocorrelation function between two lagged versions of the same time series, X(t_1) and X(t_2), depends only on the difference \\tau=t_2-t_1.\n\nLet’s get a feeling by plotting\n\ndef test_stationarity(time_series, window=100):\n    series = pd.Series(time_series)\n    rolling_var = series.rolling(window=window, center=True).std()**2\n    rolling_mean = series.rolling(window=window, center=True).mean()\n\n    fig, ax = plt.subplots(2,1, figsize=(10,4), sharex=True)\n    ax[0].plot(series, label='series')\n    ax[0].plot(rolling_mean, c='r', label='mean')\n    ax[0].legend()\n    ax[0].set_title('rolling mean')\n\n    ax[1].plot(rolling_var)\n    ax[1].set_title('rolling variance')\n\n    return\n\n\ntest_stationarity(walk_binary)\n# test_stationarity(walk_gaussian)\n# test_stationarity(ar1_series)\ntest_stationarity(ar)\ntest_stationarity(ar2)"
  },
  {
    "objectID": "stationarity/practice.html#now-lets-work-with-actual-data",
    "href": "stationarity/practice.html#now-lets-work-with-actual-data",
    "title": "29  practice",
    "section": "33.1 Now let’s work with actual data",
    "text": "33.1 Now let’s work with actual data\n\nfilename = \"jerusalem2019.csv\"\ndf = pd.read_csv(filename, na_values=['-'])\ndf.rename(columns={'Temperature (°C)': 'temperature',\n                   'Rainfall (mm)': 'rain'}, inplace=True)\ndf['date'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True)\ndf = df.set_index('date')\ndf = df.fillna(method='ffill')\ndf\n\n\n\n\n\n  \n    \n      \n      Station\n      Date & Time (Winter)\n      Diffused radiation (W/m^2)\n      Global radiation (W/m^2)\n      Direct radiation (W/m^2)\n      Relative humidity (%)\n      temperature\n      Maximum temperature (°C)\n      Minimum temperature (°C)\n      Wind direction (°)\n      Gust wind direction (°)\n      Wind speed (m/s)\n      Maximum 1 minute wind speed (m/s)\n      Maximum 10 minutes wind speed (m/s)\n      Time ending maximum 10 minutes wind speed (hhmm)\n      Gust wind speed (m/s)\n      Standard deviation wind direction (°)\n      rain\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-01 00:00:00\n      Jerusalem Givat Ram\n      01/01/2019 00:00\n      0.0\n      0.0\n      0.0\n      80.0\n      8.7\n      8.8\n      8.6\n      75.0\n      84.0\n      3.3\n      4.3\n      3.5\n      23:58\n      6.0\n      15.6\n      0.0\n    \n    \n      2019-01-01 00:10:00\n      Jerusalem Givat Ram\n      01/01/2019 00:10\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      74.0\n      82.0\n      3.3\n      4.1\n      3.3\n      00:01\n      4.9\n      14.3\n      0.0\n    \n    \n      2019-01-01 00:20:00\n      Jerusalem Givat Ram\n      01/01/2019 00:20\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.8\n      8.7\n      76.0\n      82.0\n      3.2\n      4.1\n      3.3\n      00:19\n      4.9\n      9.9\n      0.0\n    \n    \n      2019-01-01 00:30:00\n      Jerusalem Givat Ram\n      01/01/2019 00:30\n      0.0\n      0.0\n      0.0\n      79.0\n      8.7\n      8.7\n      8.6\n      78.0\n      73.0\n      3.6\n      4.2\n      3.6\n      00:30\n      5.2\n      11.7\n      0.0\n    \n    \n      2019-01-01 00:40:00\n      Jerusalem Givat Ram\n      01/01/2019 00:40\n      0.0\n      0.0\n      0.0\n      79.0\n      8.6\n      8.7\n      8.5\n      80.0\n      74.0\n      3.6\n      4.4\n      3.8\n      00:35\n      5.4\n      10.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2019-12-31 22:20:00\n      Jerusalem Givat Ram\n      31/12/2019 22:20\n      0.0\n      0.0\n      1.0\n      81.0\n      7.4\n      7.6\n      7.3\n      222.0\n      255.0\n      0.5\n      0.9\n      1.0\n      22:11\n      1.0\n      47.9\n      0.0\n    \n    \n      2019-12-31 22:30:00\n      Jerusalem Givat Ram\n      31/12/2019 22:30\n      0.0\n      0.0\n      1.0\n      83.0\n      7.3\n      7.4\n      7.3\n      266.0\n      259.0\n      0.6\n      0.8\n      0.6\n      22:28\n      1.1\n      22.8\n      0.0\n    \n    \n      2019-12-31 22:40:00\n      Jerusalem Givat Ram\n      31/12/2019 22:40\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.3\n      331.0\n      317.0\n      0.5\n      0.8\n      0.6\n      22:35\n      1.0\n      31.6\n      0.0\n    \n    \n      2019-12-31 22:50:00\n      Jerusalem Givat Ram\n      31/12/2019 22:50\n      0.0\n      0.0\n      1.0\n      83.0\n      7.5\n      7.6\n      7.4\n      312.0\n      285.0\n      0.6\n      1.0\n      0.6\n      22:50\n      1.4\n      31.3\n      0.0\n    \n    \n      2019-12-31 23:00:00\n      Jerusalem Givat Ram\n      31/12/2019 23:00\n      0.0\n      0.0\n      1.0\n      83.0\n      7.6\n      7.7\n      7.4\n      315.0\n      321.0\n      0.7\n      1.0\n      0.8\n      22:54\n      1.3\n      23.5\n      0.0\n    \n  \n\n52554 rows × 18 columns\n\n\n\n\n# t = df['temperature'].values\nt = df['Relative humidity (%)'].values\n# t = df['Wind speed (m/s)'].values\n# t = df['Wind direction (°)'].values\nfig, ax = plt.subplots()\nax.plot(t)\n\n\n\n\n\ntest_stationarity(t, window=5000)\n\n\n\n\n\nt_stationary = t[27000:35000]\nfig, ax = plt.subplots()\nax.plot(t_stationary)\ntest_stationarity(t_stationary, window=1500)\n\n\n\n\n\n\n\n\nseries_to_plot = t_stationary\nfig, ax = plt.subplots()\nlags, acf = compute_acf(series_to_plot)\nax.plot([0, len(series_to_plot)], [0]*2, color=\"black\", ls=\"--\")\nax.plot(lags, acf)\nax.set(xlabel=\"lag\",\n       ylabel=\"acf\",\n       xlim=[0, len(series_to_plot)]);\n\n\n\n\n\nfig, ax = plt.subplots()\nsm.graphics.tsa.plot_pacf(series_to_plot, lags=25, ax=ax, alpha=.05);\n\n/opt/anaconda3/lib/python3.9/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn("
  },
  {
    "objectID": "stationarity/practice2.html",
    "href": "stationarity/practice2.html",
    "title": "30  filling missing values",
    "section": "",
    "text": "31 SARIMAX\nThe above looks good at the begining of the gap but at the end of the gap it looks bad. That is because we are forcasting and not filling in between the lines…"
  },
  {
    "objectID": "stationarity/practice2.html#forward-fill",
    "href": "stationarity/practice2.html#forward-fill",
    "title": "30  filling missing values",
    "section": "30.1 Forward fill",
    "text": "30.1 Forward fill\n\ndf_ffil = df.fillna(method='ffill')\nplot_missing_vals(df, df_ffil, title='ffill')"
  },
  {
    "objectID": "stationarity/practice2.html#backwrds-fill",
    "href": "stationarity/practice2.html#backwrds-fill",
    "title": "30  filling missing values",
    "section": "30.2 Backwrds fill",
    "text": "30.2 Backwrds fill\n\ndf_bfil = df.fillna(method='bfill')\nplot_missing_vals(df, df_bfil, title='bfill')"
  },
  {
    "objectID": "stationarity/practice2.html#interpolation",
    "href": "stationarity/practice2.html#interpolation",
    "title": "30  filling missing values",
    "section": "30.3 Interpolation",
    "text": "30.3 Interpolation\n\n30.3.1 linear\n\ninterpolated_df = df.interpolate(method='linear')\nplot_missing_vals(df, interpolated_df, title='linear interpolation')\n\n\n\n\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n\n\n30.3.2 cubic splines\nSource: https://docs.scipy.org/doc/scipy/tutorial/interpolate/1D.html#cubic-splines\nPiecewise linear interpolation produces corners at data points, where linear pieces join. To produce a smoother curve, you can use cubic splines, where the interpolating curve is made of cubic pieces with matching first and second derivatives.\n\n\n\nSource: https://kluge.in-chemnitz.de/opensource/spline/\n\ninterpolated_cubic_df = df.interpolate(method='cubic')\nplot_missing_vals(df, interpolated_cubic_df, title='cubic interpolation')\n\n\n\n\n\nplot_missing_vals(interpolated_df, interpolated_cubic_df, title='linear vs cubic interpolation')\n\n\n\n\n\ninterpolated_poly_df = df.interpolate(method='polynomial', order=3)\nplot_missing_vals(df, interpolated_poly_df, title='polynomial interpolation')\n\n\n\n\n\nplot_missing_vals(interpolated_cubic_df, interpolated_poly_df, title='cubic vs polynomial interpolation')\n\n\n\n\nAll available interpolation types can be found at the pandas documentation"
  },
  {
    "objectID": "stationarity/practice2.html#random-forest",
    "href": "stationarity/practice2.html#random-forest",
    "title": "30  filling missing values",
    "section": "30.4 random forest",
    "text": "30.4 random forest\nHere we explore the use of a commnly used Machine learning model - RandomForest\n\n\ndef fillna_randomforest(series):\n    # Ensure the series index is a datetime object\n    if not isinstance(series.index, pd.DatetimeIndex):\n        raise ValueError(\"Index must be a DatetimeIndex\")\n    \n    # Split series into observed and missing values\n    observed = series.dropna()\n    missing = series[series.isna()]\n    \n    # Extracting time-based features\n    def create_features(index):\n        return np.array([index.hour, index.day, index.month, index.year]).T\n    \n    # Create features for training data\n    X_train = create_features(observed.index)\n    y_train = observed.values\n    \n    # Train the Random Forest regression model\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    \n    # Create features for missing data and predict\n    X_missing = create_features(missing.index)\n    predicted_values = model.predict(X_missing)\n    \n    # Assign the predicted values to the missing positions\n    series_filled = series.copy()\n    series_filled[series_filled.isna()] = predicted_values\n\n    return series_filled\n\n\ncolumns = df.columns\ndf_rf = df.copy()\nfor i, column in enumerate(columns):\n    if i == 0:\n        continue\n    df_rf[column] = fillna_randomforest(df_rf[column])\n\n\nplot_missing_vals(df, df_rf, title='fandomforest')"
  },
  {
    "objectID": "seasonality/motivation.html#questions",
    "href": "seasonality/motivation.html#questions",
    "title": "31  motivation",
    "section": "31.1 questions",
    "text": "31.1 questions\n\ndisconsidering the expected annual cycle, how fast is the ocean warming up?\nwhat is a typical annual oscillation in temperature?\nhow can I model processes such as this, where there are both a clear trend and a seasonal component? \n\n\n\n\n\nWesten, René M. van, Michael Kliphuis, and Henk A. Dijkstra. 2024. “Physics-Based Early Warning Signal Shows That AMOC Is on Tipping Course.” Science Advances 10 (6): eadk1189. https://doi.org/10.1126/sciadv.adk1189."
  },
  {
    "objectID": "seasonality/seasonal-decomposition-from-scratch.html#sea-surface-temperature",
    "href": "seasonality/seasonal-decomposition-from-scratch.html#sea-surface-temperature",
    "title": "32  seasonal decomposition from scratch",
    "section": "32.1 sea surface temperature",
    "text": "32.1 sea surface temperature\n\n\nimport stuff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()  # datetime converter for a matplotlib\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom datetime import datetime as dt\nimport time\nfrom statsmodels.tsa.stattools import adfuller\n\n# %matplotlib widget\n\n\n\n\nload and plot data\ndf_north = pd.read_csv(\"sst_daily_north_atl.csv\", index_col='date', parse_dates=True)\ndf_world = pd.read_csv(\"sst_daily_world.csv\", index_col='date', parse_dates=True)\n\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True)\nfig.subplots_adjust(hspace=0.1)  # increase vertical space between panels\nax[0].plot(df_world['sst'], color=\"black\")\nax[1].plot(df_north['sst'], color=\"black\")\nfig.text(0.02, 0.5, 'sea surface temperature (°C)', va='center', rotation='vertical')\nax[1].set(yticks=[20, 25])\nax[0].text(0.5, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='center', verticalalignment='top',)\nax[1].text(0.5, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='center', verticalalignment='top',)\npass"
  },
  {
    "objectID": "seasonality/seasonal-decomposition-from-scratch.html#trend",
    "href": "seasonality/seasonal-decomposition-from-scratch.html#trend",
    "title": "32  seasonal decomposition from scratch",
    "section": "32.2 trend",
    "text": "32.2 trend\n\ndf_north['trend'] = df_north['sst'].rolling('365D', center=True).mean()\ndf_world['trend'] = df_world['sst'].rolling('365D', center=True).mean()\n\n\n\nplot trend\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True)\nfig.subplots_adjust(hspace=0.1)  # increase vertical space between panels\nax[0].plot(df_world['sst'], color=\"black\", label=\"daily\")\nax[0].plot(df_world['trend'], color=\"xkcd:hot pink\", label=\"365-day mean\")\nax[1].plot(df_north['sst'], color=\"black\")\nax[1].plot(df_north['trend'], color=\"xkcd:hot pink\")\nfig.text(0.02, 0.5, 'sea surface temperature (°C)', va='center', rotation='vertical')\nax[0].set(title=\"trend  = 365-day rolling mean\")\nax[1].set(yticks=[20, 25])\nax[0].text(0.5, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='center', verticalalignment='top',)\nax[1].text(0.5, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='center', verticalalignment='top',)\nax[0].legend(frameon=False)\npass"
  },
  {
    "objectID": "seasonality/seasonal-decomposition-from-scratch.html#detrend",
    "href": "seasonality/seasonal-decomposition-from-scratch.html#detrend",
    "title": "32  seasonal decomposition from scratch",
    "section": "32.3 detrend",
    "text": "32.3 detrend\n\n\\text{detrended} = \\text{signal} - \\text{trend}\n\n\ndf_north['detrended'] = df_north['sst'] - df_north['trend']\ndf_world['detrended'] = df_world['sst'] - df_world['trend']\n\n\n\nplot trend\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True)\nfig.subplots_adjust(hspace=0.1)  # increase vertical space between panels\nax[0].plot(df_world['detrended'])\nax[1].plot(df_north['detrended'])\nfig.text(0.02, 0.5, 'detrended sst temperature (°C)', va='center', rotation='vertical')\nax[0].set(ylim=[-0.4,0.4],\n          title=\"detrended\")\nax[1].set(ylim=[-4,4],)\nax[0].text(0.5, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='center', verticalalignment='top',)\nax[1].text(0.5, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='center', verticalalignment='top',)\npass"
  },
  {
    "objectID": "seasonality/seasonal-decomposition-from-scratch.html#seasonal-component",
    "href": "seasonality/seasonal-decomposition-from-scratch.html#seasonal-component",
    "title": "32  seasonal decomposition from scratch",
    "section": "32.4 seasonal component",
    "text": "32.4 seasonal component\nIt is useful to add two new columns to our dataframes: day of year and year:\n\ndf_north['doy'] = df_north.index.day_of_year\ndf_north['year'] = df_north.index.year\ndf_world['doy'] = df_world.index.day_of_year\ndf_world['year'] = df_world.index.year\ndf_world\n\n\n\n\n\n  \n    \n      \n      sst\n      trend\n      detrended\n      doy\n      year\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1981-09-01\n      20.15\n      20.006721\n      0.143279\n      244\n      1981\n    \n    \n      1981-09-02\n      20.14\n      20.007717\n      0.132283\n      245\n      1981\n    \n    \n      1981-09-03\n      20.13\n      20.008703\n      0.121297\n      246\n      1981\n    \n    \n      1981-09-04\n      20.13\n      20.009624\n      0.120376\n      247\n      1981\n    \n    \n      1981-09-05\n      20.12\n      20.010481\n      0.109519\n      248\n      1981\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2024-02-04\n      21.12\n      20.932460\n      0.187540\n      35\n      2024\n    \n    \n      2024-02-05\n      21.11\n      20.931882\n      0.178118\n      36\n      2024\n    \n    \n      2024-02-06\n      21.13\n      20.931297\n      0.198703\n      37\n      2024\n    \n    \n      2024-02-07\n      21.12\n      20.930707\n      0.189293\n      38\n      2024\n    \n    \n      2024-02-08\n      21.12\n      20.930164\n      0.189836\n      39\n      2024\n    \n  \n\n15501 rows × 5 columns\n\n\n\n\n32.4.1 group by\nThis is an extremely useful method. As the name suggests, it groups data according to some criterion.\n\ngb_year_north = df_north.groupby('year')\ngb_year_world = df_world.groupby('year')\n\nJust like resample and rolling, groupby doesn’t do anything after grouping the data, it waits for further instructions.\n\ngb_year_world['sst'].mean()\n\nyear\n1981    19.953197\n1982    20.051671\n1983    20.133260\n1984    20.029809\n1985    19.924055\n1986    19.994055\n1987    20.148712\n1988    20.108087\n1989    20.039863\n1990    20.152932\n1991    20.145781\n1992    20.069454\n1993    20.080877\n1994    20.098274\n1995    20.170959\n1996    20.118716\n1997    20.269945\n1998    20.328575\n1999    20.117973\n2000    20.151940\n2001    20.286356\n2002    20.321699\n2003    20.370849\n2004    20.328962\n2005    20.412548\n2006    20.374110\n2007    20.321068\n2008    20.312814\n2009    20.432301\n2010    20.436411\n2011    20.314192\n2012    20.369317\n2013    20.403918\n2014    20.525260\n2015    20.647151\n2016    20.671257\n2017    20.607507\n2018    20.567808\n2019    20.652603\n2020    20.678388\n2021    20.630658\n2022    20.648521\n2023    20.889671\n2024    21.042564\nName: sst, dtype: float64\n\n\nIf all you need is the average by years, then the most common way of writing the command would be in one single line:\ndf_world.groupby('year')['sst'].mean()\nLet’s use groupby to plot world SST for all years, see how handy this method is. Also note that groupby return an object that can be iterated upon, as we do below.\n\nfig, ax = plt.subplots()\nfor year, data in gb_year_world:\n    ax.plot(data['doy'], data['sst'], color=\"black\")\nax.set(xlabel=\"DOY\", ylabel=\"SST\");\n\n\n\n\nWith a few more lines of code we can make this look pretty.\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True)\nfig.subplots_adjust(hspace=0.1)\n\n# define the segment of the colormap to use\nstart, end = 0.3, 0.8\nbase_cmap = plt.cm.hot_r\nnew_colors = base_cmap(np.linspace(start, end, 256))\nnew_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"trunc({n},{a:.2f},{b:.2f})\".format(n=base_cmap.name, a=start, b=end), new_colors)\n# defining the years for plotting\nyears = [year for year, _ in gb_year_world] + [year for year, _ in gb_year_north]\nmin_year, max_year = min(years), max(years)\n# create a new normalization that matches the years to the new colormap\nnorm = mpl.colors.Normalize(vmin=min_year, vmax=max_year)\n# plotting the data with year-specific colors from the new colormap\nfor year, data in gb_year_world:\n    ax[0].plot(data.index.day_of_year, data['sst'], color=new_cmap(norm(year)))\nfor year, data in gb_year_north:\n    ax[1].plot(data.index.day_of_year, data['sst'], color=new_cmap(norm(year)))\n\n# colorbar setup\nsm = mpl.cm.ScalarMappable(cmap=new_cmap, norm=norm)\nsm.set_array([])\ncbar = fig.colorbar(sm, ax=ax.ravel().tolist(), orientation='vertical', fraction=0.046, pad=0.04)\nticks_years = [1981, 1991, 2001, 2011, 2024]  # Specify years for colorbar ticks\ncbar.set_ticks(np.linspace(min_year, max_year, num=len(ticks_years)))\ncbar.set_ticklabels(ticks_years)\n# adding shared ylabel and xlabel\nfig.text(0.02, 0.5, 'SST temperature (°C)', va='center', rotation='vertical')\nax[1].set_xlabel(\"day of year\")\nax[0].text(0.97, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='right', verticalalignment='top',)\nax[1].text(0.97, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='right', verticalalignment='top',);\n\n\n\n\n\nNow let’s do the same for the detrended SST:\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True)\nfig.subplots_adjust(hspace=0.1)\n\n# define the segment of the colormap to use\nstart, end = 0.3, 0.8\nbase_cmap = plt.cm.hot_r\nnew_colors = base_cmap(np.linspace(start, end, 256))\nnew_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"trunc({n},{a:.2f},{b:.2f})\".format(n=base_cmap.name, a=start, b=end), new_colors)\n# defining the years for plotting\nyears = [year for year, _ in gb_year_world] + [year for year, _ in gb_year_north]\nmin_year, max_year = min(years), max(years)\n# create a new normalization that matches the years to the new colormap\nnorm = mpl.colors.Normalize(vmin=min_year, vmax=max_year)\n# plotting the data with year-specific colors from the new colormap\nfor year, data in gb_year_world:\n    ax[0].plot(data.index.day_of_year, data['detrended'], color=new_cmap(norm(year)))\nfor year, data in gb_year_north:\n    ax[1].plot(data.index.day_of_year, data['detrended'], color=new_cmap(norm(year)))\n# colorbar setup\nsm = mpl.cm.ScalarMappable(cmap=new_cmap, norm=norm)\nsm.set_array([])\ncbar = fig.colorbar(sm, ax=ax.ravel().tolist(), orientation='vertical', fraction=0.046, pad=0.04)\nticks_years = [1981, 1991, 2001, 2011, 2024]  # Specify years for colorbar ticks\ncbar.set_ticks(np.linspace(min_year, max_year, num=len(ticks_years)))\ncbar.set_ticklabels(ticks_years)\n# adding shared ylabel and xlabel\nfig.text(0.02, 0.5, 'detrended SST temperature (°C)', va='center', rotation='vertical')\nax[0].set(yticks=[-0.4,0,0.4])\nax[1].set_xlabel(\"day of year\")\nax[0].text(0.97, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='right', verticalalignment='top',)\nax[1].text(0.97, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='right', verticalalignment='top',);\n\n\n\n\n\nIn order to determine what is the seasonal component, we need to calculate the average of the detrended data across all years. Again, groupby comes to rescue in a most elegant way:\n\navg_north = df_north.groupby('doy')['detrended'].mean()\navg_world = df_world.groupby('doy')['detrended'].mean()\n\nLet’s incorporate the averages in the same graphs we saw above:\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True)\nfig.subplots_adjust(hspace=0.1)\n\n# define the segment of the colormap to use\nstart, end = 0.3, 0.8\nbase_cmap = plt.cm.hot_r\nnew_colors = base_cmap(np.linspace(start, end, 256))\nnew_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"trunc({n},{a:.2f},{b:.2f})\".format(n=base_cmap.name, a=start, b=end), new_colors)\n# defining the years for plotting\nyears = [year for year, _ in gb_year_world] + [year for year, _ in gb_year_north]\nmin_year, max_year = min(years), max(years)\n# create a new normalization that matches the years to the new colormap\nnorm = mpl.colors.Normalize(vmin=min_year, vmax=max_year)\n# plotting the data with year-specific colors from the new colormap\nfor year, data in gb_year_world:\n    ax[0].plot(data.index.day_of_year, data['detrended'], color=new_cmap(norm(year)))\nfor year, data in gb_year_north:\n    ax[1].plot(data.index.day_of_year, data['detrended'], color=new_cmap(norm(year)))\n\nax[0].plot(avg_world, color='dodgerblue', lw=2, ls='--')\nax[1].plot(avg_north, color='dodgerblue', lw=2, ls='--', label=\"mean across years\")\nax[1].legend(loc=\"upper left\", frameon=False)\n\n# colorbar setup\nsm = mpl.cm.ScalarMappable(cmap=new_cmap, norm=norm)\nsm.set_array([])\ncbar = fig.colorbar(sm, ax=ax.ravel().tolist(), orientation='vertical', fraction=0.046, pad=0.04)\nticks_years = [1981, 1991, 2001, 2011, 2024]  # Specify years for colorbar ticks\ncbar.set_ticks(np.linspace(min_year, max_year, num=len(ticks_years)))\ncbar.set_ticklabels(ticks_years)\n# adding shared ylabel and xlabel\nfig.text(0.02, 0.5, 'detrended SST temperature (°C)', va='center', rotation='vertical')\nax[0].set(yticks=[-0.4,0,0.4])\nax[1].set_xlabel(\"day of year\")\nax[0].text(0.97, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='right', verticalalignment='top',)\nax[1].text(0.97, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='right', verticalalignment='top',);\n\n\n\n\n\n\n\nShow the code\navg_north\n\n\ndoy\n1     -1.258903\n2     -1.287413\n3     -1.312651\n4     -1.345564\n5     -1.378712\n         ...   \n362   -1.127575\n363   -1.160508\n364   -1.192747\n365   -1.220802\n366   -1.234997\nName: detrended, Length: 366, dtype: float64\n\n\n\n\nShow the code\ndf_north\n\n\n\n\n\n\n  \n    \n      \n      sst\n      trend\n      detrended\n      doy\n      year\n      seasonal\n      resid\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1981-09-01\n      23.78\n      21.237432\n      2.542568\n      244\n      1981\n      2.655598\n      -0.113030\n    \n    \n      1981-09-02\n      23.77\n      21.225380\n      2.544620\n      245\n      1981\n      2.650287\n      -0.105667\n    \n    \n      1981-09-03\n      23.80\n      21.213351\n      2.586649\n      246\n      1981\n      2.638455\n      -0.051806\n    \n    \n      1981-09-04\n      23.83\n      21.201237\n      2.628763\n      247\n      1981\n      2.635235\n      -0.006472\n    \n    \n      1981-09-05\n      23.87\n      21.188877\n      2.681123\n      248\n      1981\n      2.629680\n      0.051443\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2024-02-04\n      20.37\n      23.096310\n      -2.726310\n      35\n      2024\n      -2.018653\n      -0.707657\n    \n    \n      2024-02-05\n      20.41\n      23.085914\n      -2.675914\n      36\n      2024\n      -2.034478\n      -0.641436\n    \n    \n      2024-02-06\n      20.42\n      23.075405\n      -2.655405\n      37\n      2024\n      -2.052858\n      -0.602547\n    \n    \n      2024-02-07\n      20.41\n      23.064565\n      -2.654565\n      38\n      2024\n      -2.066810\n      -0.587756\n    \n    \n      2024-02-08\n      20.42\n      23.053661\n      -2.633661\n      39\n      2024\n      -2.076346\n      -0.557315\n    \n  \n\n15501 rows × 7 columns\n\n\n\nThe seasonal component is the average across all years, repeated over and over.\n\ndf_north['seasonal'] = df_north['doy'].map(avg_north)\ndf_world['seasonal'] = df_world['doy'].map(avg_world)\n\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True)\nfig.subplots_adjust(hspace=0.1)\nax[0].plot(df_world.loc['2000':'2004', 'seasonal'])\nax[1].plot(df_north.loc['2000':'2004', 'seasonal'])\n# adding shared ylabel and xlabel\nfig.text(0.02, 0.5, 'seasonal component (°C)', va='center', rotation='vertical')\nax[0].text(0.97, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='right', verticalalignment='top',)\nax[1].text(0.97, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='right', verticalalignment='top',);"
  },
  {
    "objectID": "seasonality/seasonal-decomposition-from-scratch.html#residual",
    "href": "seasonality/seasonal-decomposition-from-scratch.html#residual",
    "title": "32  seasonal decomposition from scratch",
    "section": "32.5 residual",
    "text": "32.5 residual\n\n\\text{residual} = \\text{signal} - \\text{trend} - \\text{seasonal}\n\n\ndf_world['resid'] = df_world['detrended'] - df_world['seasonal']\ndf_north['resid'] = df_north['detrended'] - df_north['seasonal']\n\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True)\nfig.subplots_adjust(hspace=0.1)\nax[0].plot(df_world['resid'])\nax[1].plot(df_north['resid'])\n# adding shared ylabel and xlabel\nfig.text(0.02, 0.5, 'residual (°C)', va='center', rotation='vertical')\nax[0].text(0.97, 0.97, r\"world average\", transform=ax[0].transAxes,\n           horizontalalignment='right', verticalalignment='top',)\nax[1].text(0.97, 0.97, r\"north atlantic\", transform=ax[1].transAxes,\n           horizontalalignment='right', verticalalignment='top',);\n\n\n\n\n\nHow do we know we have properly decomposed our signal into trend, seasonal and residual? The residual should be stationary. Let’s check using the ADF test:\n\n\nShow the code\nresult = adfuller(df_world['resid'])\nprint('World-average residual\\nADF test p-value: ', result[1])\n\nresult = adfuller(df_north['resid'])\nprint('North-Atlantic residual\\nADF test p-value: ', result[1])\n\n\nWorld-average residual\nADF test p-value:  1.5673285788450267e-25\nNorth-Atlantic residual\nADF test p-value:  5.2285789371886856e-18"
  },
  {
    "objectID": "seasonality/seasonal-decomposition-from-scratch.html#seasonal-decomposition",
    "href": "seasonality/seasonal-decomposition-from-scratch.html#seasonal-decomposition",
    "title": "32  seasonal decomposition from scratch",
    "section": "32.6 seasonal decomposition",
    "text": "32.6 seasonal decomposition\nIt is customary to plot each component in its own panel:\n\n\nShow the code\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\npos = (0.5, 0.9)\ncomponents =[\"sst\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(df_world, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\nax[0].set(title=\"world average\");\n\n\n\n\n\n\n\nShow the code\nfig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True)\npos = (0.5, 0.9)\ncomponents =[\"sst\", \"trend\", \"seasonal\", \"resid\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\nfor axx, component, color in zip(ax, components, colors):\n    data = getattr(df_north, component)\n    axx.plot(data, color=color)\n    axx.text(*pos, component, bbox=dict(facecolor='white', alpha=0.8),\n           transform=axx.transAxes, ha='center', va='top')\nax[0].set(title=\"north atlantic\");\n\n\n\n\n\n\n\nShow the code\nfig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].plot(df_world['sst'], color=\"tab:blue\", label=\"sst\")\nax[0].plot(df_world['trend'] + df_world['resid'], color=\"tab:orange\", label=\"trend+resid\")\nax[0].plot(df_world['trend'] + df_world['seasonal'], color=\"tab:red\", label=\"trend+seasonal\")\nax[0].plot(df_world['trend'], color=\"black\", label=\"trend\")\nax[0].set(ylabel=\"SST (°C)\",\n          title=\"world average\")\ndate_form = DateFormatter(\"%Y\")\nax[0].xaxis.set_major_formatter(date_form)\nax[0].xaxis.set_major_locator(mdates.YearLocator(10))\nax[0].legend(frameon=False)\n\nstart = \"2013-01-01\"\nend = \"2016-01-01\"\nzoom = slice(start, end)\nax[1].plot(df_world.loc[zoom, 'sst'], color=\"tab:blue\", label=\"sst\")\nax[1].plot(df_world.loc[zoom, 'trend'] + df_world.loc[zoom, 'resid'], color=\"tab:orange\", label=\"trend+resid\")\nax[1].plot(df_world.loc[zoom, 'trend'] + df_world.loc[zoom, 'seasonal'], color=\"tab:red\", label=\"trend+seasonal\")\nax[1].plot(df_world.loc[zoom, 'trend'], color=\"black\", label=\"trend\")\ndate_form = DateFormatter(\"%Y\")\nax[1].xaxis.set_major_formatter(date_form)\nax[1].xaxis.set_major_locator(mdates.YearLocator(1))\nax[1].set_title(\"components, 2013--2016\");"
  },
  {
    "objectID": "seasonality/theory.html#additive-model",
    "href": "seasonality/theory.html#additive-model",
    "title": "33  theory",
    "section": "33.1 additive model",
    "text": "33.1 additive model\nseasonal_decompose returns an object with four components:\n\nobserved: Y(t)\ntrend: T(t)\nseasonal: S(t)\nresid: e(t)\n\nThe default assumption is that the various components are summed together to produce the original observed time series: \nY(t) = T(t) + S(t) + e(t)\n\nThis is called the additive model of seasonal decomposition."
  },
  {
    "objectID": "seasonality/theory.html#multiplicative-model",
    "href": "seasonality/theory.html#multiplicative-model",
    "title": "33  theory",
    "section": "33.2 multiplicative model",
    "text": "33.2 multiplicative model\nseasonal_decompose returns an object with four components:\nNot all time series seem to behave well when decomposed assuming an additive model. See, for instance, the famous time series for passengers of a US airline between 1949 to 1960.\n\n\nShow the code\ndf_air = pd.read_csv(\"airline-passengers.csv\")\ndf_air['date'] = pd.to_datetime(df_air['Month'], format='%Y-%m')\ndf_air = df_air.set_index('date')\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(df_air['Passengers'])\nax.set(ylabel='passangers');\n\n\n\n\n\n\n\nplot\nresult = seasonal_decompose(df_air['Passengers'], period=12)\n\nfig, ax = plt.subplots(4,1, figsize=(8,5), sharex=True)\nax[0].plot(result.observed)\nax[0].text(0.5, 0.9, \"observed\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[0].transAxes, ha='center', va='top')\nax[1].plot(result.trend)\nax[1].text(0.5, 0.9, \"trend\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[1].transAxes, ha='center', va='top')\nax[2].plot(result.seasonal)\nax[2].text(0.5, 0.9, \"seasonal\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[2].transAxes, ha='center', va='top')\nax[3].plot(result.resid)\nax[3].text(0.5, 0.9, \"residual\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[3].transAxes, ha='center', va='top');\n\n\n\n\n\nThe residual is much larger at the beginning and at the end, and this is probably because the seasonal component increases with time. A good way to overcome this is to use the multiplicative model for seasonal decomposition:\n\nY(t) = T(t) \\times S(t) \\times e(t)\n\nThe components are now multiplied together to yield the observed time series. Let’s test it:\n\n\nShow the code\nresult = seasonal_decompose(df_air['Passengers'], period=12, model=\"multiplicative\")\n\n\n\n\nplot\nfig, ax = plt.subplots(4,1, figsize=(8,5), sharex=True)\nax[0].plot(result.observed)\nax[0].text(0.5, 0.9, \"observed\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[0].transAxes, ha='center', va='top')\nax[1].plot(result.trend)\nax[1].text(0.5, 0.9, \"trend\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[1].transAxes, ha='center', va='top')\nax[2].plot(result.seasonal)\nax[2].text(0.5, 0.9, \"seasonal\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[2].transAxes, ha='center', va='top')\nax[3].plot(result.resid)\nax[3].text(0.5, 0.9, \"residual\", bbox=dict(facecolor='white', alpha=0.8),\n           transform=ax[3].transAxes, ha='center', va='top');"
  },
  {
    "objectID": "seasonality/theory.html#stl",
    "href": "seasonality/theory.html#stl",
    "title": "33  theory",
    "section": "33.3 STL",
    "text": "33.3 STL\nSTL stands for “Seasonal and Trend decomposition using LOESS”. LOESS, in turn means “locally estimated scatterplot smoothing”, and it is a spiced up version of the Savitzky-Golay smoothing filter we saw earlier in this course. The method is not trivially understood as the naive seasonal decompostion we build together, and the full method is described by Cleveland et al. (1990) here. See in this video how the smoothing method works.\n\n\nThey the most striking difference with the plain seasonal decompose method is that the seasonal component is not constant in time, it can stlightly change and evolve over time.\n\n\nShow the code\ndata = df['co2']\nres = STL(data, period=365).fit()\nres.plot();\n\n\n\n\n\nA possible downside of STL is the fact that it assumes an additive model. If the seasonal oscillations in your data increase with the value of the series, then most probably a multiplicative model would be best for you. How to use STL then? One can take the logarithm of the series, since for a multiplicative model\n\nY(t) = T(t) \\times S(t) \\times e(t)\n\ntaking the logarithms yields\n\n\\log(Y) = \\log(T) + \\log(S) + \\log(e).\n\nWe can now use STL since our model looks like additive now. After using STL, one can exponentiate the results to reverse the logarithm operation.\n\n\n\n\nCleveland, Robert B, William S Cleveland, Jean E McRae, and Irma Terpenning. 1990. “STL: A Seasonal-Trend Decomposition.” J. Off. Stat 6 (1): 3–73."
  },
  {
    "objectID": "seasonality/widget.html",
    "href": "seasonality/widget.html",
    "title": "34  widgets",
    "section": "",
    "text": "import stuff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()  # datetime converter for a matplotlib\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nfrom datetime import datetime as dt\nimport time\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.colors as mcolors\nimport urllib.request\nimport json\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# %matplotlib widget\n\n\n\n\ndownload latest data, save as csv\n# helper function to check if a year is a leap year\ndef is_leap_year(year):\n    if year % 4 != 0:\n        return False\n    elif year % 100 != 0:\n        return True\n    elif year % 400 != 0:\n        return False\n    else:\n        return True\n    \ndef load_and_process_data(type, filename):\n    # from https://climatereanalyzer.org/clim/sst_daily/\n    # up-to-date URLs for world average and for north atlantic SST\n    if type == \"world\":\n        url = \"https://climatereanalyzer.org/clim/sst_daily/json/oisst2.1_world2_sst_day.json\"\n    if type == \"north\":\n        url = \"https://climatereanalyzer.org/clim/sst_daily/json/oisst2.1_natlan1_sst_day.json\"\n    # load JSON data from URL\n    with urllib.request.urlopen(url) as response:\n        data = json.load(response)\n    # Convert JSON data to DataFrame\n    data = pd.DataFrame(data)\n    # this dataframe is not good to work with, let's make a new one\n    # columns are year numbers, rows are temperatures for each day of the year\n    years = data['name'].astype(str).tolist()\n    values = data['data'].tolist()\n    df = pd.DataFrame(values).T\n    df.columns = years\n    # now let's make a continuous df, with all the data in one column\n    # initializing an empty list to hold dataframes before concatenating them\n    dfs = []\n    # iterating over each column in the original dataframe\n    for column in df.columns:\n        try:\n            # converting the column name to an integer to handle it as a year\n            year = int(column)\n            # determining the number of days in the year\n            days_in_year = 366 if is_leap_year(year) else 365\n            # creating a date range for the year\n            dates = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', periods=days_in_year)\n            # creating a temporary dataframe for the year's data\n            temp_df = pd.DataFrame({'sst': df[column][:days_in_year].values}, index=dates)\n            # adding the temporary dataframe to the list\n            dfs.append(temp_df)\n        except ValueError:\n            # skipping columns that do not represent a year (e.g., \"1982-2011 mean\", \"plus 2σ\", \"minus 2σ\")\n            continue\n    # concatenating all the temporary dataframes into one\n    df_sst_concat = pd.concat(dfs)\n    # resetting the index to have a datetime index\n    df_sst_concat.index = pd.to_datetime(df_sst_concat.index)\n    df_sst_concat.index.name = 'date'\n    df_sst_concat.dropna(inplace=True)\n    # save to file\n    df_sst_concat.to_csv(filename, index=True)\n\nload_and_process_data(\"world\", \"sst_world.csv\")\nload_and_process_data(\"north\", \"sst_north.csv\")\n\n\n\n\nload and plot data\ndf_north = pd.read_csv(\"sst_north.csv\", index_col='date', parse_dates=True)\ndf_world = pd.read_csv(\"sst_world.csv\", index_col='date', parse_dates=True)\n\n\n\n\nadd columns: day and year\ndf_world['doy'] = df_world.index.day_of_year\ndf_world['year'] = df_world.index.year\ndf_grouped = df_world.groupby('year')\n\n\n\n\nwidget 1\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=list(df_world.index), y=list(df_world['sst']), name='world mean'))\n\nfig.add_trace(\n    go.Scatter(x=list(df_north.index), y=list(df_north['sst']), name='north atlantic'))\n\n# Add range slider\nfig.update_layout(\n    title='sea surface temperature',\n    yaxis_title='temperature (°C)',\n    xaxis=dict(\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\"\n    ),\n)\n\n\n\n                                                \n\n\n\n\nchoose colors for next widget\n# Define colormap and color range\n# base_cmap = plt.cm.hot_r\n# start, end = 0.3, 0.8\n\n# base_cmap = plt.cm.coolwarm\n# start, end = 0.3, 0.7\n\nbase_cmap = plt.cm.turbo\nstart, end = 0.1, 0.75\n\n\n# Create truncated colormap\nnew_colors = base_cmap(np.linspace(start, end, 256))\nnew_cmap = mcolors.LinearSegmentedColormap.from_list(\"trunc({n},{a:.2f},{b:.2f})\".format(n=base_cmap.name, a=start, b=end), new_colors)\n\n# List of years\nyears = range(1981, 2025)\n\n# Create dictionary mapping years to hexadecimal colors\nyear_colors = {}\nfor i, year in enumerate(years):\n    # Calculate normalized value for the year\n    norm_value = (year - years[0]) / (years[-1] - years[0])\n    \n    # Get color from colormap\n    rgba_color = new_cmap(norm_value)\n    \n    # Convert RGBA to hex\n    hex_color = mcolors.rgb2hex(rgba_color[:3])  # Exclude alpha channel\n    \n    # Add to dictionary\n    year_colors[year] = hex_color\n\n# Print the dictionary\n# print(year_colors)\n\n# overwrite last 2 years with hotter colors\nyear_colors[2023] = plt.cm.colors.to_hex('hotpink')\nyear_colors[2024] = plt.cm.colors.to_hex('deeppink')\n\n\n\n\nwidget 2\n# Assuming df_world is your DataFrame containing the data\n\n# Convert the index to datetime if it's not already in datetime format\ndf_world.index = pd.to_datetime(df_world.index)\n\n# Create a Plotly figure\nfig = go.Figure()\n\n# Iterate over each unique year\nfor year in df_world.index.year.unique():\n    # Filter the data for the current year\n    data_year = df_world[df_world.index.year == year]\n    \n    # Add a trace for the current year\n    fig.add_trace(go.Scatter(\n        x=data_year.index.dayofyear,  # x-axis: day of year\n        y=data_year['sst'],            # y-axis: sea surface temperature\n        mode='lines',\n        name=str(year),                 # Name of the trace (year)\n        line=dict(\n            color=year_colors[year]\n            ),\n        hovertemplate='<b>Date</b>: %{text}<br>' +\n                       '<b>SST (°C)</b>: %{y}',  # Customize hover template\n        text=data_year.index.strftime('%Y-%m-%d'),  # Convert date to YYYY-MM-DD format\n        hoverlabel=dict(namelength=0)  # Set namelength to 0 to remove the tag\n    ))\n\n# Update layout\nfig.update_layout(\n    title='Sea Surface Temperature, World Average',\n    xaxis_title='Day of Year (DOY)',\n    yaxis_title='Sea Surface Temperature (°C)',\n    legend=dict(\n        orientation=\"h\",           # Horizontal legend\n        yanchor=\"top\",             # Anchor legend to the top\n        y=-0.2,                    # Adjust vertical position\n        xanchor=\"right\",           # Anchor legend to the right\n        x=1,                       # Adjust horizontal position\n    ),\n    plot_bgcolor='white',  # Set plot background color to white\n    xaxis=dict(showgrid=True,    # Show gridlines for the x-axis\n               gridcolor='lightgrey',  # Set grid color for x-axis\n               gridwidth=0.5),     # Set grid width for x-axis\n    yaxis=dict(showgrid=True,    # Show gridlines for the y-axis\n               gridcolor='lightgrey',  # Set grid color for y-axis\n               gridwidth=0.5),      # Set grid width for y-axis\n)\n\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "lags/cross-correlation.html",
    "href": "lags/cross-correlation.html",
    "title": "36  cross-correlation",
    "section": "",
    "text": "import numpy as np\n\n\nprint('dfvdfv')\n\ndfvdfv"
  },
  {
    "objectID": "lags/LDTW.html",
    "href": "lags/LDTW.html",
    "title": "38  LDTW",
    "section": "",
    "text": "according to this paper"
  },
  {
    "objectID": "frequency/fourier.html#basic-wave-concepts",
    "href": "frequency/fourier.html#basic-wave-concepts",
    "title": "40  Fourier transform",
    "section": "40.1 basic wave concepts",
    "text": "40.1 basic wave concepts\nThe function\n\nf(t) = B\\sin(2\\pi f t)\n\\tag{40.1}\nhas two basic characteristics, its amplitude B and frequency f.\n\nIn the figure above, the amplitude B=0.6 and we see that the distance between two peaks is called period, T=2 s. The frequency is defined as the inverse of the period:\n\nf = \\frac{1}{T}.\n\\tag{40.2}\nWhen time is in seconds, then the frequency is measured in Hertz (Hz). For the graph above, therefore, we see a wave whose frequency is f = 1/(2 \\text{ s}) = 0.5 Hz.\nIn the figure below, we see what happens when we vary the values of the frequency and amplitude.\n\nThe graph above introduces two new characteristics of a wave, its phase \\phi, and its offset B. A more general description of a sine wave is\n\nf(t) = B\\sin(2\\pi f t + \\phi) + B_0.\n\\tag{40.3}\nThe offset B_0 moves the wave up and down, while changing the value of \\phi makes the sine wave move left and right. When the phase \\phi=2\\pi, the sine wave will have shifted a full period, and the resulting wave is identical to the original:\n\nB\\sin(2\\pi f t) = B\\sin(2\\pi f t + 2\\pi).\n\\tag{40.4}\nAll the above can also be said about a cosine, whose general for can be given as\n\nA\\cos(2\\pi f t + \\phi) + A_0\n\\tag{40.5}\nOne final point before we jump into the deep waters is that the sine and cosine functions are related through a simple phase shift:\n\n\\cos\\left(2\\pi f t + \\frac{\\pi}{2}\\right) = \\sin\\left(2\\pi f t\\right)"
  },
  {
    "objectID": "frequency/fourier.html#fouriers-theorem",
    "href": "frequency/fourier.html#fouriers-theorem",
    "title": "40  Fourier transform",
    "section": "40.2 Fourier’s theorem",
    "text": "40.2 Fourier’s theorem\nFourier’s theorem states that\n\nAny periodic signal is composed of a superposition of pure sine waves, with suitably chosen amplitudes and phases, whose frequencies are harmonics of the fundamental frequency of the signal.\n\nSee the following animations to visualize the theorem in action.\n\nSource: https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Fourier_synthesis_square_wave_animated.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Sawtooth_Fourier_Animation.gif\n\nSource: https://commons.wikimedia.org/wiki/File:Continuous_Fourier_transform_of_rect_and_sinc_functions.gif"
  },
  {
    "objectID": "frequency/fourier.html#fourier-series",
    "href": "frequency/fourier.html#fourier-series",
    "title": "40  Fourier transform",
    "section": "40.3 Fourier series",
    "text": "40.3 Fourier series\n\na periodic function can be described as a sum of sines and cosines.\n\n\n\nNot any function, but certainly most functions we will deal with in this course. The function has to fullful the Dirichlet conditions\nThe classic examples are usually the square function and the sawtooth function:\n\n\n[Source: https://www.geogebra.org/m/tkajbzmg]\nhttps://www.geogebra.org/m/k4eq4fkr\n\n\n\nF[x(t)] = F(f) = \\int_{-\\infty}^{\\infty}x(t)e^{-2\\pi i f t}dt\n\n\nf(t) = \\int_{-\\infty}^{\\infty}F(f)e^{2\\pi i f t}df\n\nhttps://dibsmethodsmeetings.github.io/fourier-transforms/\nhttps://www.jezzamon.com/fourier/index.html"
  },
  {
    "objectID": "rates-of-change/motivation.html",
    "href": "rates-of-change/motivation.html",
    "title": "43  motivation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n%matplotlib widget\n\n\nfilename = \"../archive/data/kinneret_cleaned.csv\"\ndf = pd.read_csv(filename)\ndf['date'] = pd.to_datetime(df['date'], dayfirst=True)\ndf = df.set_index('date')\ndf\n\n\n\n\n\n  \n    \n      \n      level\n    \n    \n      date\n      \n    \n  \n  \n    \n      2023-09-12\n      -211.115\n    \n    \n      2023-09-11\n      -211.105\n    \n    \n      2023-09-10\n      -211.095\n    \n    \n      2023-09-09\n      -211.085\n    \n    \n      2023-09-08\n      -211.070\n    \n    \n      ...\n      ...\n    \n    \n      1966-11-01\n      -210.390\n    \n    \n      1966-10-15\n      -210.320\n    \n    \n      1966-10-01\n      -210.270\n    \n    \n      1966-09-15\n      -210.130\n    \n    \n      1966-09-01\n      -210.020\n    \n  \n\n10286 rows × 1 columns\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df['level'], color=\"tab:blue\")\nax.set(title=\"Kinneret Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n\n\n\nThe data seems ok, until we take a closer look. Data points are not evenly spaced in time.\n\nfig, ax = plt.subplots()\nax.plot(df.loc[\"1993\":\"1995\", 'level'], color=\"tab:blue\", marker=\"o\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_3777/934261896.py:2: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n  ax.plot(df.loc[\"1993\":\"1995\", 'level'], color=\"tab:blue\", marker=\"o\")\n\n\n\n\n\nWe can resample by day (a much higher rate than the original), and linearly interpolate:\n\ndf2 = df['level'].resample('D').interpolate('time').to_frame()\ndf2['level_sm'] = df2['level'].rolling('30D', center=True).mean()\ndf3 = df2['level'].resample('W').mean().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df2.loc[\"1993\":\"1995\", 'level_sm'],\n        color=\"tab:red\",\n        label=\"daily resapled\")\nax.plot(df3.loc[\"1993\":\"1995\", 'level'],\n        color=\"black\",\n        label=\"daily resapled\")\nax.plot(df2.loc[\"1993\":\"1995\", 'level'],\n        color=\"tab:orange\",\n        label=\"daily resapled\")\nax.plot(df.loc[\"1993\":\"1995\", 'level'],\n        color=\"tab:blue\",\n        marker=\"o\",\n        linestyle=\"None\",\n        label=\"original\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\nplt.gcf().autofmt_xdate()  # makes slanted dates\nax.legend(frameon=False)\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_3777/2583247388.py:11: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n  ax.plot(df.loc[\"1993\":\"1995\", 'level'],\n\n\n<matplotlib.legend.Legend at 0x7fa71e8b0bb0>\n\n\n\n\n\n\ndf2['naive'] = df2['level'].diff()\ndf2['gradient'] = np.gradient(df2['level'])\n\ndf3['naive'] = df3['level'].diff()\ndf3['gradient'] = np.gradient(df3['level'])\n\n\nfig, ax = plt.subplots()\nax.plot(df3.loc[\"1980\":\"2020\", 'naive'], color=\"tab:blue\")\nax.plot(df3.loc[\"1980\":\"2020\", 'gradient'], color=\"tab:red\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\n\n[Text(0.5, 1.0, 'Dead Sea Level'), Text(0, 0.5, 'level (m)')]\n\n\n\n\n\n\ndf3 = df2[\"level\"].rolling('365.24D', center=True).mean().to_frame()\n\n\nfig, ax = plt.subplots()\nax.plot(df3.loc[\"1980\":\"2020\", 'level'], color=\"tab:blue\")\nax.set(title=\"Dead Sea Level\",\n       ylabel=\"level (m)\")\n\n[Text(0.5, 1.0, 'Dead Sea Level'), Text(0, 0.5, 'level (m)')]"
  },
  {
    "objectID": "rates-of-change/finite-differences.html",
    "href": "rates-of-change/finite-differences.html",
    "title": "45  finite differences",
    "section": "",
    "text": "Definition of a derivative:\n\n\\underbrace{\\dot{f} = f'(t) = \\frac{df(t)}{dt}}_{\\text{same thing}} = \\lim_{\\Delta t \\rightarrow 0} \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}.\n\nNumerically, we can approximate the derivative f'(t) of a time series f(t) as\n\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{45.1}\n\n\nThe expression \\mathcal{O}(\\Delta t) means that the error associated with the approximation is proportional to \\Delta t. This is called “Big O notation”.\nThe expression above is called the two-point forward difference formula. Likewise, we can define the two-point backward difference formula:\n\n\\frac{df(t)}{dt} = \\frac{f(t) - f(t-\\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n\\tag{45.2}\nIf we sum together Equation 45.1 and Equation 45.2 we get:\n\n\n\\begin{aligned}\n2\\frac{df(t)}{dt} &= \\frac{f(t+\\Delta t) - \\cancel{f(t)}}{\\Delta t} + \\frac{\\cancel{f(t)} - f(t-\\Delta t)}{\\Delta t} \\\\\n&= \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{\\Delta t}.\n\\end{aligned}\n\\tag{45.3}\nDividing both sides by 2 gives the two-point central difference formula:\n\n\\frac{df(t)}{dt} = \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n\\tag{45.4}\nTwo things are worth mentioning about the approximation above:\n\nit is balanced, that is, there is no preference of the future over the past.\nits error is proportional to \\Delta t^2, it is a lot more precise than the unbalanced approximations :)\n\n\n\nTo understand why the error is proportional to \\Delta t^2, one can subtract the Taylor expansion of f(t-\\Delta t) from the Taylor expansion of f(t+\\Delta t). See this, pages 3 and 4.\n\nThe function np.gradient calculates the derivative using the central difference for points in the interior of the array, and uses the forward (backward) difference for the derivative at the beginning (end) of the array.\n\n\nThe “gradient” usually refers to a first derivative with respect to space, and it is denoted as \\nabla f(x)=\\frac{df(x)}{dx}. However, it doesn’t really matter if we call the independent variable x or t, the derivative operator is exactly the same.\nCheck out this nice example."
  },
  {
    "objectID": "rates-of-change/fourier-based-derivatives.html",
    "href": "rates-of-change/fourier-based-derivatives.html",
    "title": "46  Fourier-based derivatives",
    "section": "",
    "text": "This tutorial is based on Pelliccia (2019).\nnice trick: https://math.stackexchange.com/questions/430858/fourier-transform-of-derivative\n\n\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing Method.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/."
  },
  {
    "objectID": "assignments/assignment1.html#task",
    "href": "assignments/assignment1.html#task",
    "title": "50  assignment 1",
    "section": "50.1 task",
    "text": "50.1 task\nGo to the IMS website, and choose another weather station we have not worked with yet. Download 10-minute data for a full year, any year.\nMake 3 graphs:\n\nDaily maximum humidity. Bonus: add another line to the graph, the daily minimum humidity.\nThe number of rainy days for each month.\nFor each day of the year, show the number of hours when global solar radiation was above, on average, the threshold 10 W/m^2. Now add another line, for the threshold 500 W/m^2.\n\nMake 5 more graphs (total of 8 graphs) of whatever you find interesting. You have the liberty to explore various facets of your dataset that capture your interest. It’s essential, however, to maintain a focus on resampling. Each of your plots should effectively showcase and emphasize different aspects or techniques of resampling in your data analysis. To ensure diversity in your visualizations, avoid repetitive themes; for instance, if your first plot illustrates daily wind speed, then your second plot should not simply be a monthly resampling of wind speed. Aim for variety and innovation in each plot to fully explore the potential of resampling in data visualization.\nYou must download this Jupyter Notebook template. Create a zip file with your Jupyter notebook and with the .csv you used. Upload this zip file to the moodle task we created."
  },
  {
    "objectID": "assignments/assignment1.html#guidelines",
    "href": "assignments/assignment1.html#guidelines",
    "title": "50  assignment 1",
    "section": "50.2 guidelines",
    "text": "50.2 guidelines\n\nAlways name the axes and add units when relevant.\nAlways give a title to the plot.\nMake sure that all axis tick labels (the numbers/dates on the axes) are readable.\nInclude a legend if you have multiple lines, colors, or groups.\nUse appropriate scales for the axes (linear, logarithmic, etc.) depending on the data’s nature.\nEnsure that the plot is adequately sized for all elements to be clear and visible.\nChoose colors and markers that are distinguishable, especially for plots with multiple elements.\nIf applicable, include error bars to indicate the variability or uncertainty in the data.\nUse grid lines sparingly; they should not overshadow the data."
  },
  {
    "objectID": "assignments/assignment1.html#evaluation",
    "href": "assignments/assignment1.html#evaluation",
    "title": "50  assignment 1",
    "section": "50.3 evaluation",
    "text": "50.3 evaluation\nAll your assignments will be evaluated according to the following criteria:\n\nPresentation. How the graphs look, labels, general organization, markdown, clean code.\nDiscussion. This is where you explain what you did, what you found out, etc.\nDepth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration.\nReplicability: Your code runs flawlessly.\nCode commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself!\nBonus: for originality, creative problem solving, or notable analysis."
  },
  {
    "objectID": "assignments/assignment2.html#smoothing",
    "href": "assignments/assignment2.html#smoothing",
    "title": "51  assignment 2",
    "section": "51.1 Smoothing",
    "text": "51.1 Smoothing\nIn this assignment, you will delve into the application of different smoothing techniques on time series data. Utilizing meteorological data, your task is to create a series of plots that demonstrate the effects of various smoothing methods.\n\n51.1.1 1. Comparative Smoothing Methods Analysis\n\nGoal: Showcase three smoothing techniques – Rolling Average, Savitzky-Golay, and Resampling – on the same time series data.\nTask: Overlay these methods over the actual data in a single plot. Ensure each method uses the same window size for consistency. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n51.1.2 2. Rolling Average Window Size Impact\n\nGoal: Analyze the effect of varying window sizes on the Rolling Average method.\nTask: Produce a plot with three lines, each representing the Rolling Average with a different window size. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n51.1.3 3. Savitzky-Golay Polynomial Order Variation\n\nGoal: Investigate how changing the polynomial order affects the Savitzky-Golay smoothing method.\nTask: Create a plot with three lines, where each represents the Savitzky-Golay method with a different polynomial order. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n51.1.4 4. Kernel Shape Influence in Rolling Mean\n\nGoal: Explore the impact of different kernel shapes on the Rolling Mean.\nTask: Generate a plot displaying three lines, each using a different kernel shape in the Rolling Mean. We encorage to use unique kernel shapes that we did not showcase in class. See this list of kernels. Describe in a few lines the differences you see.\n\n\n# code goes here\n\n\n\n51.1.5 5. Moving Average with Confidence Interval\n\nGoal: Plot a Moving Average along with a 75% confidence interval.\nTask: Design a plot illustrating both the Moving Average and its 75% confidence interval.\n\n\n# code goes here"
  },
  {
    "objectID": "assignments/assignment3.html#background",
    "href": "assignments/assignment3.html#background",
    "title": "52  assignment 3",
    "section": "52.1 background",
    "text": "52.1 background\nA tree in Boston is being continuously measured, using an IoT box created by me (Erez).\n\nSensors connected to this box measure the following\n\nAir temperature (C)\nAir relative humidity (%)\nChange of tree circumference (\\mum) measured by a dendrometer\nBattery voltage (V)\nInternal temperature (°C) (inside the box)\n\nThe box collects data from sensors every 30 minutes and uploads them to the cloud once a day. The cloud server puts a time stamp on every datapoint, but to make sure things are right: the timestamp at the time of data collection is uploaded as a variable (in units of UNIX at GMT).\nYou can see the box’s dashboard here.\nDownload the file boston_raw.csv here.\nHere is a break down of the columns:\n\ncreated_at - Time stamp added by the cloud\nentry_id - Index added by the cloud\nfield1 - Change of tree circumference (\\mum)\nfield2 - Air temperature (°C)\nfield3 - Air relative humidity (%)\nfield4 - Battery voltage (V)\nfield5 - Internal temperature (c) (inside the box)\nfield6 - True timestamp (in units of UNIX at GMT)\nlatitude - Empty\nlongitude - Empty\nelevation - Empty\nstatus - Empty"
  },
  {
    "objectID": "assignments/assignment3.html#analysis",
    "href": "assignments/assignment3.html#analysis",
    "title": "52  assignment 3",
    "section": "52.2 analysis",
    "text": "52.2 analysis\nFollow the instructions below to process the data.\nMake sure your text answers are in markdown cells and are numbered. Text answers in Python cells might be ignored.\nUpload to Moodle the completed operational (no errors) Jupyter Notebook file (.ipynb), along with the boston_raw.csv file.\n\nLoad csv to a dataframe.\nRename the columns to short and convenient names that make sense for you.\nConvert the UNIX time stamp to a human-readable time stamp. Don’t forget to convert the time zone from GMT to Boston.\nSet the new converted timestamp as the index.\nSort the entire dataframe based on the chronological order of the index. Here is an example code: df.sort_index(inplace=True)\nPlot all columns and explore the data. Zoom in. Do you see gaps? Outliers? Jumps? Don’t process them yet. Make a list of 3 things you see.\nBonus: Identify any patterns related to outliers/gaps and explain their occurrence.\nCount how many nan values are in each column.\nNow take a look at the datetime index. As noted earlier, the readings on the device are every 30 minutes, but is that what we see in the data? Do we have a continuous datetime index where every index is 30 min apart? Are there gaps? Are the timestamps consistent? Is there a drift (e.g. sometimes the data comes in at 00:30 and sometimes 00:27 or 00:20, etc..). Explain.\nThink about a quick and dirty fix to this problem. Then read below:\n\nAn easy fix will be to resample the data at 30T and take the mean. That will ensure consistent timestamps and no gaps (in the index).\nWhy quick and “dirty”? What is dirty about it? is there a problem? Explain.\nAnyway, we will continue with this. There are other ways but they are a bit more complex.\n\nRe-count the number of nan values in each column. Explain any changes observed.\nNow that the index is consistent and without gaps, we want to all data (fix jumps, remove outliers and fill gaps).\n\nI’ll start with a hint regarding the dendrometer that requires some background knowledge you may not know. The dendrometer has a metal band wrapping the trunk of the tree and it needs to be reset when the sensor readings are approaching the limit of 60,000 \\mum. So you can see it in the data that someone reset the band some day in September. What needs to be done is to shift up all the data after the jump to be a continuation of the data before the jump. Plot the dendrometer data after the shift.\n\nOutliers: show results of at least 2 methods of outlier identification applied to the data. Compare them and choose the one that did the best work and explain. Outliers should be replaced with nan values.\nFilling gaps (missing values): show results of at least 3 methods of gap filling (at least one of them should be advanced for example SARIMAX or Randomforest). Compare them and choose the one that did the best work and explain. Differently from what we learned during class, this time we don’t have the real data to compare to. Bonus: you might find a way to download data from Boston from a meteorological station, and compare to our data.\nAt this point the data should be clean (no outliers or gaps) in all columns and ready for exploration.\nAdd a column of vapor pressure deficit (VPD) based on the air temperature and relative humidity. The formula can be found here.\nUsing the toolkit learned in the course (for example: smoothing, seasonal decompose, detrending, etc..) show the following:\n\nHow temperature affects dendrometer readings (hint, this happens in a large time scale of months). Showing it graphically is enough! No need to prove it statistically (for example don’t apply correlation).\nHow VPD affects tree dendrometer readings (hint, this happens in a small time scale of days, during the hotter months). Showing it graphically is enough! No need to prove it statistically (for example don’t apply correlation)."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#operating-systems",
    "href": "technical-stuff/technical-stuff.html#operating-systems",
    "title": "53  technical stuff",
    "section": "53.1 operating systems",
    "text": "53.1 operating systems\nI recommend working with UNIX-based operating systems (MacOS or Linux). Everything is easier.\nIf you use Windows, consider installing Linux on Windows with WSL."
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#software",
    "href": "technical-stuff/technical-stuff.html#software",
    "title": "53  technical stuff",
    "section": "53.2 software",
    "text": "53.2 software\nAnaconda’s Python distribution\nVSCode"
  },
  {
    "objectID": "technical-stuff/technical-stuff.html#python-packages",
    "href": "technical-stuff/technical-stuff.html#python-packages",
    "title": "53  technical stuff",
    "section": "53.3 python packages",
    "text": "53.3 python packages\nKats — a one-stop shop for time series analysis\nDeveloped by Meta\nstatsmodels statsmodels is a Python package that provides a complement to scipy for statistical computations including descriptive statistics and estimation and inference for statistical models.\nydata-profiling\nQuick Exploratory Data Analysis on time-series data. Read also this."
  },
  {
    "objectID": "technical-stuff/datasets.html#sunspots",
    "href": "technical-stuff/datasets.html#sunspots",
    "title": "54  datasets",
    "section": "54.1 Sunspots",
    "text": "54.1 Sunspots\nThe solar cycle produces varying amounts of sunspots throughout the years.\n\n\n\nSource: https://www.sidc.be/SILSO/monthlyssnplot\nDownload data from the Royal Observatory of Belgium."
  },
  {
    "objectID": "technical-stuff/datasets.html#covid-19-open-data",
    "href": "technical-stuff/datasets.html#covid-19-open-data",
    "title": "54  datasets",
    "section": "54.2 Covid-19 Open Data",
    "text": "54.2 Covid-19 Open Data\nDownload the data into your own tools and systems to analyze the virus’s spread or decline, investigate COVID-related deaths, study the effects of different vaccines, and more in 20,000-plus locations worldwide.\n\n\n\nSource: https://health.google.com/covid-19/open-data/explorer\nClick here to go to the download page. Choose desired region under section “Understanding the data”."
  },
  {
    "objectID": "technical-stuff/date-formatting.html",
    "href": "technical-stuff/date-formatting.html",
    "title": "55  date formatting",
    "section": "",
    "text": "Here you will find several examples of how to format dates in your plots. Not many explanations are provided.\nHow to use this page? Find first an example of a plot you like, only then go to the code and see how it’s done.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\n\n\nimport pandas as pd\n\nstart_date = '2018-01-01'\nend_date = '2018-04-30'\n\n# create date range with 1-hour intervals\ndates = pd.date_range(start_date, end_date, freq='1H')\n# create a random variable to plot\nvar = np.random.rand(len(dates)) - 0.51\nvar = var.cumsum()\nvar = var - var.min()\n# create dataframe, make \"date\" the index\ndf = pd.DataFrame({'date': dates, 'variable': var})\ndf.set_index(df['date'], inplace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2018-01-01 00:00:00\n      2018-01-01 00:00:00\n      28.317035\n    \n    \n      2018-01-01 01:00:00\n      2018-01-01 01:00:00\n      28.120523\n    \n    \n      2018-01-01 02:00:00\n      2018-01-01 02:00:00\n      28.596894\n    \n    \n      2018-01-01 03:00:00\n      2018-01-01 03:00:00\n      28.931941\n    \n    \n      2018-01-01 04:00:00\n      2018-01-01 04:00:00\n      28.561778\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2018-04-29 20:00:00\n      2018-04-29 20:00:00\n      1.914343\n    \n    \n      2018-04-29 21:00:00\n      2018-04-29 21:00:00\n      1.648757\n    \n    \n      2018-04-29 22:00:00\n      2018-04-29 22:00:00\n      1.992956\n    \n    \n      2018-04-29 23:00:00\n      2018-04-29 23:00:00\n      1.500860\n    \n    \n      2018-04-30 00:00:00\n      2018-04-30 00:00:00\n      1.650439\n    \n  \n\n2857 rows × 2 columns\n\n\n\ndefine a useful function to plot the graphs below\n\ndef explanation(ax, text, letter):\n    ax.text(0.99, 0.97, text,\n            transform=ax.transAxes,\n            horizontalalignment='right', verticalalignment='top',\n            fontweight=\"bold\")\n    ax.text(0.01, 0.01, letter,\n            transform=ax.transAxes,\n            horizontalalignment='left', verticalalignment='bottom',\n            fontweight=\"bold\")\n    ax.set(ylabel=\"variable (units)\")\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nax.plot(df['variable'])\nplt.gcf().autofmt_xdate()  # makes slated dates\nexplanation(ax, \"slanted dates\", \"\")\nfig.savefig(\"dates1.png\")\n\n\n\n\n\nfig, ax = plt.subplots(4, 1, figsize=(10, 16),\n                       gridspec_kw={'hspace': 0.3})\n\n### plot a ###\nax[0].plot(df['variable'])\ndate_form = DateFormatter(\"%b\")\nax[0].xaxis.set_major_locator(mdates.MonthLocator(interval=2))\nax[0].xaxis.set_major_formatter(date_form)\n\n### plot b ###\nax[1].plot(df['variable'])\ndate_form = DateFormatter(\"%B\")\nax[1].xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax[1].xaxis.set_major_formatter(date_form)\n\n### plot c ###\nax[2].plot(df['variable'])\nax[2].xaxis.set_major_locator(mdates.MonthLocator())\n# 16 is a slight approximation for the center, since months differ in number of days.\nax[2].xaxis.set_minor_locator(mdates.MonthLocator(bymonthday=16))\nax[2].xaxis.set_major_formatter(ticker.NullFormatter())\nax[2].xaxis.set_minor_formatter(DateFormatter('%B'))\nfor tick in ax[2].xaxis.get_minor_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n\n### plot d ###\nax[3].plot(df['variable'])\ndate_form = DateFormatter(\"%d %b\")\nax[3].xaxis.set_major_locator(mdates.DayLocator(interval=15))\nax[3].xaxis.set_major_formatter(date_form)\n\nexplanation(ax[0], \"month abbreviations, every 2 months\", \"a\")\nexplanation(ax[1], \"full month names\", \"b\")\nexplanation(ax[2], \"full month names centered between the 1st of the month\", \"c\")\nexplanation(ax[3], \"day + month abbr. --- every 15 days\", \"d\")\n\nfig.savefig(\"dates2.png\")\n\n\n\n\n\nfig, ax = plt.subplots(4, 1, figsize=(10, 16),\n                       gridspec_kw={'hspace': 0.3})\n\n### plot e ###\nax[0].plot(df['variable'])\ndate_form = DateFormatter(\"%d/%m\")\nax[0].xaxis.set_major_locator(mdates.DayLocator(bymonthday=[5, 20]))\nax[0].xaxis.set_major_formatter(date_form)\n\n### plot f ###\nax[1].plot(df['variable'])\nlocator = mdates.AutoDateLocator(minticks=11, maxticks=17)\nformatter = mdates.ConciseDateFormatter(locator)\nax[1].xaxis.set_major_locator(locator)\nax[1].xaxis.set_major_formatter(formatter)\n\n### plot g ###\nax[2].plot(df.loc['2018-01-01':'2018-03-01', 'variable'])\nlocator = mdates.AutoDateLocator(minticks=6, maxticks=14)\nformatter = mdates.ConciseDateFormatter(locator)\nax[2].xaxis.set_major_locator(locator)\nax[2].xaxis.set_major_formatter(formatter)\n\n### plot h ###\nax[3].plot(df.loc['2018-01-01':'2018-01-02', 'variable'])\nlocator = mdates.AutoDateLocator(minticks=6, maxticks=10)\nformatter = mdates.ConciseDateFormatter(locator)\nax[3].xaxis.set_major_locator(locator)\nax[3].xaxis.set_major_formatter(formatter)\n\nexplanation(ax[0], \"exactly on days 05 and 20 of each month\", \"e\")\nexplanation(ax[1], \"ConciseDateFormatter\", \"f\")\nexplanation(ax[2], \"ConciseDateFormatter\", \"g\")\nexplanation(ax[3], \"ConciseDateFormatter\", \"h\")\n\nfig.savefig(\"dates3.png\")\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4),\n                       gridspec_kw={'hspace': 0.3})\n\n# import constants for the days of the week\nfrom matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\nax.plot(df['variable'])\n# tick on sundays every third week\nloc = mdates.WeekdayLocator(byweekday=SU, interval=3)\nax.xaxis.set_major_locator(loc)\ndate_form = DateFormatter(\"%a, %b %d\")\nax.xaxis.set_major_formatter(date_form)\nfig.autofmt_xdate(bottom=0.2, rotation=30, ha='right')\nexplanation(ax, \"every 3 Sundays, rotate labels\", \"\")\n\n\n\n\n\n\n\nCode\nExplanation\n\n\n\n\n%Y\n4-digit year (e.g., 2022)\n\n\n%y\n2-digit year (e.g., 22)\n\n\n%m\n2-digit month (e.g., 12)\n\n\n%B\nFull month name (e.g., December)\n\n\n%b\nAbbreviated month name (e.g., Dec)\n\n\n%d\n2-digit day of the month (e.g., 09)\n\n\n%A\nFull weekday name (e.g., Tuesday)\n\n\n%a\nAbbreviated weekday name (e.g., Tue)\n\n\n%H\n24-hour clock hour (e.g., 23)\n\n\n%I\n12-hour clock hour (e.g., 11)\n\n\n%M\n2-digit minute (e.g., 59)\n\n\n%S\n2-digit second (e.g., 59)\n\n\n%p\n“AM” or “PM”\n\n\n%Z\nTime zone name\n\n\n%z\nTime zone offset from UTC (e.g., -0500)"
  },
  {
    "objectID": "technical-stuff/sources.html#books",
    "href": "technical-stuff/sources.html#books",
    "title": "56  sources",
    "section": "56.1 books",
    "text": "56.1 books\nfrom Data to Viz\nFundamentals of Data Visualization, by Claus O. Wilke\nPyNotes in Agriscience\nForecasting: Principles and Practice (3rd ed), by Rob J Hyndman and George Athanasopoulos\nPython for Finance Cookbook 2nd Edition - Code Repository\nPractical time series analysis,: prediction with statistics and machine learning, by Aileen Nielsen\nThe online edition of this book is available for Hebrew University staff and students.\nTime series analysis with Python cookbook : practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation, by Tarek A. Atwan\nThe online edition of this book is available for Hebrew University staff and students.\nHands-on Time Series Analysis with Python: From Basics to Bleeding Edge Techniques, by B V Vishwas, Ashish Patel\nThe online edition of this book is available for Hebrew University staff and students."
  },
  {
    "objectID": "technical-stuff/sources.html#videos",
    "href": "technical-stuff/sources.html#videos",
    "title": "56  sources",
    "section": "56.2 videos",
    "text": "56.2 videos\nTimes Series Analysis for Everyone, by Bruno Goncalves\nThis series is available for Hebrew University staff and students.\nTime Series Analysis with Pandas, by Joshua Malina This video is available for Hebrew University staff and students."
  },
  {
    "objectID": "technical-stuff/sources.html#references",
    "href": "technical-stuff/sources.html#references",
    "title": "56  sources",
    "section": "56.3 references",
    "text": "56.3 references\n\n\nBrockwell, Peter J., and Richard A. Davis. 2016. Introduction to Time Series and Forecasting.\n3rd ed. Springer.\n\n\nChatfield, C. 2016. The Analysis of Time Series: An Introduction,\nSixth Edition. Chapman & Hall/CRC Texts in Statistical Science.\nCRC Press.\n\n\nCleveland, Robert B, William S Cleveland, Jean E McRae, and Irma\nTerpenning. 1990. “STL: A Seasonal-Trend Decomposition.”\nJ. Off. Stat 6 (1): 3–73.\n\n\nMcDonald, Andy. 2022. “Creating Boxplots with the Seaborn Python\nLibrary.” Medium. Towards Data Science. https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57.\n\n\nPelliccia, Daniel. 2019. “Fourier Spectral Smoothing\nMethod.” 2019. https://nirpyresearch.com/fourier-spectral-smoothing-method/.\n\n\nShumway, Robert H., and David S. Stoffer. 2017. Time Series Analysis and Its Applications With R\nExamples. 4th ed. Springer. http://www.stat.ucla.edu/~frederic/415/S23/tsa4.pdf.\n\n\nTsay, R. S. 2010. Analysis of Financial Time Series. Wiley.\n\n\nWesten, René M. van, Michael Kliphuis, and Henk A. Dijkstra. 2024.\n“Physics-Based Early Warning Signal Shows That AMOC Is on Tipping\nCourse.” Science Advances 10 (6): eadk1189. https://doi.org/10.1126/sciadv.adk1189.\n\n\nZhang, Ou. 2020. “Outliers-Part 3:outliers in Regression.”\nouzhang.me. https://ouzhang.me/blog/outlier-series/outliers-part3/."
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#rectangular-kernel",
    "href": "behind-the-scenes/sliding-window-video.html#rectangular-kernel",
    "title": "sliding window video",
    "section": "Rectangular kernel",
    "text": "Rectangular kernel\n\n%matplotlib widget\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True,\n                       gridspec_kw={'height_ratios':[1,0.4], 'hspace':0.1})\n\nclass Lines:\n    \"\"\"\n    empty class, later will be populated with graph objects.\n    this is useful to draw and erase lines on demand.\n    \"\"\"\n    pass\nlines = Lines()\n\n# rename axes for convenience\nax0 = ax[0]\nax1 = ax[1]\n# sm = df['TD'].rolling(10, center=True).mean()\n# ga = df['TD'].rolling(10, center=True, win_type=\"gaussian\").mean(std=100.0)\n\n# set graph y limits\nylim = [3, 22]\n# choose here windown width in minutes\nwindow_width_min = 200.0\nwindow_width_min_integer = int(window_width_min)  # same but integer\nwindow_width_int = int(window_width_min // 10 + 1)  # window width in points\nN = len(df)  # df length\n# time range over which the kernel will slide\n# starts at \"start\", minus the width of the window,\n# minus half an hour, so that the window doesn't start sliding right away at the beginning of the video\n# ends an hour after the window has finished sliding\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\n# starting time\nt0 = t_swipe[0]\n# show sliding window on the top panel as a light blue shade\nlines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1)\n# this is our \"boxcart\" kernel (a rectangle)\nkernel_rect = np.ones(window_width_int)\n# calculate the moving average with \"kernel_rect\" as weights\n# this is the same as a convolution, which is just faster to compute\ndf.loc[:, 'con'] = np.convolve(df['TD'].values, kernel_rect, mode='same') / len(kernel_rect)\n# create a new column for the kernel, fill it with zeros\ndf['kernel_plus'] = 0.0\n# populate the kernel column with the window at the very beginning\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_rect\n# plot kernel on the bottom panel\nlines.kernel_line, = ax1.plot(df['kernel_plus'])\n# plot temperature on the top panel\nax0.plot(df.loc[start:end, 'TD'], color=\"black\")\n# make temperature look gray when inside the sliding window\nlines.gray_line, = ax0.plot(df.loc[df['kernel_plus']==1.0, 'TD'],\n                     color=[0.6]*3, lw=3)\n# calculate the middle of the sliding window\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# plot a pink line showing the result of the moving average\n# from the beginning to the middle of the sliding window\nlines.pink_line, = ax0.plot(df.loc[start:window_middle, 'con'], color=\"xkcd:hot pink\", lw=3)\n# emphasize the location of the middle on the window with a circle\nlines.pink_circle, = ax0.plot([window_middle], [df.loc[window_middle, 'con']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\n# some explanation\nax0.text(0.99, 0.97, f\"kernel: boxcar (rectangle)\\nwidth = {window_width_min:.0f} minutes\", transform=ax0.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\n# axis tweaking\nax0.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (°C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\nax1.set(ylim=[-0.2, 1.2],\n        xlim=[start, end],\n        ylabel=\"kernel\"\n       )\n# adjust dates on both panels as defined before\ncenter_dates_two_panels(ax0, ax1)\n\ndef update_swipe(k, lines):\n    \"\"\"\n    updates both panels, given the index k along which the window is sliding\n    \"\"\"\n    # left side of the sliding window\n    t0 = t_swipe[k]\n    # middle position\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    # erase previous blue shade on the top graph\n    lines.fill_bet.remove()\n    # fill again the blue shade in the updated window position\n    lines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                               y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n    # update pink curve\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'con'].values)\n    # update pink circle\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'con']])\n    # update the kernel in its current position\n    lines.kernel_rect = np.ones(window_width_int)\n    df.loc[:, 'kernel_plus'] = 0.0\n    df.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_rect\n    # update gray line\n    lines.gray_line.set_data(df.loc[df['kernel_plus']==1.0, 'TD'].index,\n                             df.loc[df['kernel_plus']==1.0, 'TD'].values)\n    # update kernel line\n    lines.kernel_line.set_data(df['kernel_plus'].index, df['kernel_plus'].values)\n\n# create a tqdm progress bar\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\n# loop over all sliding indices, update graph and then save it\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/boxcar{window_width_min_integer}/boxcar_{window_width_min_integer}min_{fignum:03}.png\", dpi=600)\n    # update the progress bar\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|█████████▉| 604/605 [05:27<00:00,  1.85iteration/s]\n\n\n\n\n\nCombine all saved images into one mp4 video.\n\n# Define the path to your PNG images\npngs_path = f\"pngs/boxcar{window_width_min_integer}\"\npngs_name = f\"boxcar_{window_width_min_integer}min_%03d.png\"\n\n# Define the output video file path\nvideo_output = f\"output{window_width_min_integer}.mp4\"\n\n# Use ffmpeg to create a video from PNG images\n# desired framerate. choose 24 if you don't know what to do\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.0 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58.  2.100 / 58.  2.100\n  libavcodec     60.  3.100 / 60.  3.100\n  libavformat    60.  3.100 / 60.  3.100\n  libavdevice    60.  1.100 / 60.  1.100\n  libavfilter     9.  3.100 /  9.  3.100\n  libswscale      7.  1.100 /  7.  1.100\n  libswresample   4. 10.100 /  4. 10.100\n  libpostproc    57.  1.100 / 57.  1.100\nInput #0, image2, from 'pngs/boxcar200/boxcar_200min_%03d.png':\n  Duration: 00:00:50.33, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7fa027f2e300] using SAR=1/1\n[libx264 @ 0x7fa027f2e300] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7fa027f2e300] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7fa027f2e300] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output200.mp4':\n  Metadata:\n    encoder         : Lavf60.3.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.3.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\nframe=  604 fps= 23 q=-1.0 Lsize=    1412kB time=00:00:50.08 bitrate= 231.0kbits/s speed=1.91x     \nvideo:1404kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.564556%\n[libx264 @ 0x7fa027f2e300] frame I:3     Avg QP: 9.98  size:135751\n[libx264 @ 0x7fa027f2e300] frame P:154   Avg QP:14.75  size:  2507\n[libx264 @ 0x7fa027f2e300] frame B:447   Avg QP:22.66  size:  1440\n[libx264 @ 0x7fa027f2e300] consecutive B-frames:  1.0%  0.7%  1.0% 97.4%\n[libx264 @ 0x7fa027f2e300] mb I  I16..4: 55.5% 38.8%  5.7%\n[libx264 @ 0x7fa027f2e300] mb P  I16..4:  0.4%  0.3%  0.0%  P16..4:  0.2%  0.1%  0.0%  0.0%  0.0%    skip:99.0%\n[libx264 @ 0x7fa027f2e300] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  1.0%  0.2%  0.0%  direct: 0.0%  skip:98.7%  L0:50.0% L1:49.3% BI: 0.7%\n[libx264 @ 0x7fa027f2e300] 8x8 transform intra:37.1% inter:49.0%\n[libx264 @ 0x7fa027f2e300] coded y,u,v intra: 3.6% 0.4% 0.6% inter: 0.1% 0.0% 0.0%\n[libx264 @ 0x7fa027f2e300] i16 v,h,dc,p: 90% 10%  0%  0%\n[libx264 @ 0x7fa027f2e300] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41%  3% 56%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7fa027f2e300] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 51% 14% 20%  3%  2%  3%  2%  3%  2%\n[libx264 @ 0x7fa027f2e300] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7fa027f2e300] ref P L0: 56.6%  3.8% 28.4% 11.3%\n[libx264 @ 0x7fa027f2e300] ref B L0: 85.6% 13.4%  1.0%\n[libx264 @ 0x7fa027f2e300] ref B L1: 95.9%  4.1%\n[libx264 @ 0x7fa027f2e300] kb/s:228.44\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/boxcar200/boxcar_200min_%03d.png -c:v libx264 -vf fps=12 output200.mp4', returncode=0)\n\n\nThe following code does exactly as you see above, but it is not well commented. You are an intelligent person, you’ll figure this out."
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#triangular-kernel",
    "href": "behind-the-scenes/sliding-window-video.html#triangular-kernel",
    "title": "sliding window video",
    "section": "Triangular kernel",
    "text": "Triangular kernel\n\n%matplotlib widget\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True,\n                       gridspec_kw={'height_ratios':[1,0.4], 'hspace':0.1})\n\nclass Lines:\n    pass\nlines = Lines()\n\nax0 = ax[0]\nax1 = ax[1]\nylim = [3, 22]\nwindow_width_min = 500.0\nwindow_width_int = int(window_width_min / 10) + 1\nN = len(df)\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\nt0 = t_swipe[200]\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# fill between blue shade, plot kernel\nlines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1)\nhalf_triang = np.arange(1, window_width_int/2+1, 1)\nkernel_triang = np.hstack([half_triang, half_triang[-2::-1]])\nkernel_triang = kernel_triang / kernel_triang.max()\ndf.loc[:, 'con'] = np.convolve(df['TD'].values, kernel_triang, mode='same') / len(kernel_triang) * 2\ndf['kernel_plus'] = 0.0\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_triang\nlines.kernel_line, = ax1.plot(df['kernel_plus'], color=\"tab:blue\")\nax0.plot(df.loc[start:end, 'TD'], color=\"black\")\nlines.gray_line, = ax0.plot(df.loc[df['kernel_plus']!=0.0, 'TD'],\n                     color=[0.6]*3, lw=3)\nlines.pink_line, = ax0.plot(df.loc[start:window_middle, 'con'], color=\"xkcd:hot pink\", lw=3)\nlines.pink_circle, = ax0.plot([window_middle], [df.loc[window_middle, 'con']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\nax0.text(0.99, 0.97, f\"kernel: triangle\\nwidth = {window_width_min:.0f} minutes\", transform=ax0.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\nax0.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (°C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\nax1.set(ylim=[-0.2, 1.2],\n        xlim=[start, end],\n        ylabel=\"kernel\"\n       )\ncenter_dates_two_panels(ax0, ax1)\n\ndef update_swipe(k, lines):\n    t0 = t_swipe[k]\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    lines.fill_bet.remove()\n    lines.fill_bet = ax0.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                               y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'con'].values)\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'con']])\n    lines.kernel_rect = np.ones(window_width_int)\n    df['kernel_plus'] = 0.0\n    df.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_triang\n    lines.gray_line.set_data(df.loc[df['kernel_plus']!=0.0,'TD'].index,\n                             df.loc[df['kernel_plus']!=0.0,'TD'].values)\n    lines.kernel_line.set_data(df['kernel_plus'].index, df['kernel_plus'].values)\n\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/triangle/triangle_{fignum:03}.png\", dpi=600)\n    # update the progress bar\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|█████████▉| 634/635 [05:35<00:00,  1.89iteration/s]\n\n\n\n\n\n\n# Define the path to your PNG images\npngs_path = \"pngs/triangle\"\npngs_name = \"triangle_%03d.png\"\n\n# Define the output video file path\nvideo_output = \"output_triangle.mp4\"\n\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.0 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58.  2.100 / 58.  2.100\n  libavcodec     60.  3.100 / 60.  3.100\n  libavformat    60.  3.100 / 60.  3.100\n  libavdevice    60.  1.100 / 60.  1.100\n  libavfilter     9.  3.100 /  9.  3.100\n  libswscale      7.  1.100 /  7.  1.100\n  libswresample   4. 10.100 /  4. 10.100\n  libpostproc    57.  1.100 / 57.  1.100\nInput #0, image2, from 'pngs/triangle/triangle_%03d.png':\n  Duration: 00:00:52.83, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7fa9b0807f80] using SAR=1/1\n[libx264 @ 0x7fa9b0807f80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7fa9b0807f80] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7fa9b0807f80] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output_triangle.mp4':\n  Metadata:\n    encoder         : Lavf60.3.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.3.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\nframe=  634 fps= 23 q=-1.0 Lsize=    1324kB time=00:00:52.58 bitrate= 206.2kbits/s speed=1.94x     \nvideo:1316kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.624018%\n[libx264 @ 0x7fa9b0807f80] frame I:3     Avg QP:10.55  size:133880\n[libx264 @ 0x7fa9b0807f80] frame P:162   Avg QP:12.92  size:  2541\n[libx264 @ 0x7fa9b0807f80] frame B:469   Avg QP:21.63  size:  1137\n[libx264 @ 0x7fa9b0807f80] consecutive B-frames:  0.6%  0.3%  5.7% 93.4%\n[libx264 @ 0x7fa9b0807f80] mb I  I16..4: 51.0% 43.4%  5.6%\n[libx264 @ 0x7fa9b0807f80] mb P  I16..4:  0.4%  0.2%  0.0%  P16..4:  0.2%  0.1%  0.0%  0.0%  0.0%    skip:99.0%\n[libx264 @ 0x7fa9b0807f80] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.0%  0.2%  0.0%  direct: 0.0%  skip:98.8%  L0:50.5% L1:48.9% BI: 0.6%\n[libx264 @ 0x7fa9b0807f80] 8x8 transform intra:39.7% inter:41.2%\n[libx264 @ 0x7fa9b0807f80] coded y,u,v intra: 2.9% 0.5% 0.6% inter: 0.0% 0.0% 0.0%\n[libx264 @ 0x7fa9b0807f80] i16 v,h,dc,p: 91%  9%  0%  0%\n[libx264 @ 0x7fa9b0807f80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41%  3% 55%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7fa9b0807f80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 44% 17% 20%  4%  3%  4%  2%  4%  2%\n[libx264 @ 0x7fa9b0807f80] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7fa9b0807f80] ref P L0: 53.5%  2.7% 25.8% 18.0%\n[libx264 @ 0x7fa9b0807f80] ref B L0: 85.3% 13.2%  1.5%\n[libx264 @ 0x7fa9b0807f80] ref B L1: 96.6%  3.4%\n[libx264 @ 0x7fa9b0807f80] kb/s:203.87\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/triangle/triangle_%03d.png -c:v libx264 -vf fps=12 output_triangle.mp4', returncode=0)"
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#gaussian-kernel",
    "href": "behind-the-scenes/sliding-window-video.html#gaussian-kernel",
    "title": "sliding window video",
    "section": "Gaussian kernel",
    "text": "Gaussian kernel\n\n%matplotlib widget\nfig, ax = plt.subplots(2, 1, figsize=(8,5), sharex=True,\n                       gridspec_kw={'height_ratios':[1,0.4], 'hspace':0.1})\n\nclass Lines:\n    pass\nlines = Lines()\n\nax0 = ax[0]\nax1 = ax[1]\nylim = [3, 22]\nwindow_width_min = 500.0\nwindow_width_int = int(window_width_min / 10) + 1\nN = len(df)\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\nt0 = t_swipe[0]\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# fill between blue shade, plot kernel\nhalf_triang = np.arange(1, window_width_int/2+1, 1)\nkernel_triang = np.hstack([half_triang, half_triang[-2::-1]])\nkernel_triang = kernel_triang / kernel_triang.max()\ndf['con'] = np.convolve(df['TD'].values, kernel_triang, mode='same') / len(kernel_triang) * 2\ndf['kernel_plus'] = 0.0\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = kernel_triang\n\n# array of minutes. multiply by 10 because data is every 10 minutes\n\nstd_in_minutes = 60\ng = sp.signal.gaussian(window_width_int, std_in_minutes/10)#, sym=True)\ndf.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = g\ngaussian_threshold = np.exp(-2**2)  # two sigmas\nlines.kernel_line, = ax1.plot(df['kernel_plus'], color=\"tab:blue\")\nwindow_above_threshold = df.loc[df['kernel_plus'] > gaussian_threshold, 'kernel_plus'].index\nlines.fill_bet = ax0.fill_between([window_above_threshold[0], window_above_threshold[-1]],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n\n# gaussian convolution from here: https://stackoverflow.com/questions/27205402/pandas-rolling-window-function-offsets-data\ndf.loc[:, 'con'] = np.convolve(df['TD'].values, g/g.sum(), mode='same')\nax0.plot(df.loc[start:end, 'TD'], color=\"black\")\nlines.gray_line, = ax0.plot(df.loc[window_above_threshold[0]:window_above_threshold[-1], 'TD'],\n                     color=[0.6]*3, lw=3)\nlines.pink_line, = ax0.plot(df.loc[start:window_middle, 'con'], color=\"xkcd:hot pink\", lw=3)\nlines.pink_circle, = ax0.plot([window_middle], [df.loc[window_middle, 'con']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\nax0.text(0.99, 0.97, f\"kernel: gaussian\\nwidth = {window_width_min:.0f} minutes\\nstd = {std_in_minutes:.0f} minutes\", transform=ax0.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\nax0.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (°C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\nax1.set(ylim=[-0.2, 1.2],\n        xlim=[start, end],\n        ylabel=\"kernel\"\n       )\ngauss = df['TD'].rolling(window=window_width_int, center=True, win_type=\"gaussian\").mean(std=6)#, sym=True)\ncenter_dates_two_panels(ax0, ax1)\n\ndef update_swipe(k, lines):\n    t0 = t_swipe[k]\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    lines.fill_bet.remove()\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'con'].values)\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'con']])\n    lines.kernel_rect = np.ones(window_width_int)\n    df['kernel_plus'] = 0.0\n    df.loc[t0: t0 + pd.Timedelta(minutes=window_width_min), 'kernel_plus'] = g\n    window_above_threshold = df.loc[df['kernel_plus'] > gaussian_threshold, 'kernel_plus'].index\n    lines.gray_line.set_data(df.loc[window_above_threshold[0]:window_above_threshold[-1], 'TD'].index,\n                             df.loc[window_above_threshold[0]:window_above_threshold[-1], 'TD'].values)\n    lines.kernel_line.set_data(df['kernel_plus'].index, df['kernel_plus'].values)\n    window_above_threshold = df.loc[df['kernel_plus'] > gaussian_threshold, 'kernel_plus'].index\n    lines.fill_bet = ax0.fill_between([window_above_threshold[0], window_above_threshold[-1]],\n                                       y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/gaussian/gaussian_{fignum:03}.png\", dpi=600)\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|█████████▉| 634/635 [05:47<00:00,  1.83iteration/s]\n\n\n\n\n\n\n# Define the path to your PNG images\npngs_path = \"pngs/gaussian\"\npngs_name = \"gaussian_%03d.png\"\n\n# Define the output video file path\nvideo_output = \"output_gaussian.mp4\"\n\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.0 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58.  2.100 / 58.  2.100\n  libavcodec     60.  3.100 / 60.  3.100\n  libavformat    60.  3.100 / 60.  3.100\n  libavdevice    60.  1.100 / 60.  1.100\n  libavfilter     9.  3.100 /  9.  3.100\n  libswscale      7.  1.100 /  7.  1.100\n  libswresample   4. 10.100 /  4. 10.100\n  libpostproc    57.  1.100 / 57.  1.100\nInput #0, image2, from 'pngs/gaussian/gaussian_%03d.png':\n  Duration: 00:00:52.83, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7ff6d8907580] using SAR=1/1\n[libx264 @ 0x7ff6d8907580] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7ff6d8907580] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7ff6d8907580] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output_gaussian.mp4':\n  Metadata:\n    encoder         : Lavf60.3.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.3.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\nframe=  634 fps= 21 q=-1.0 Lsize=    1386kB time=00:00:52.58 bitrate= 215.9kbits/s speed=1.77x     \nvideo:1378kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.602110%\n[libx264 @ 0x7ff6d8907580] frame I:3     Avg QP:10.13  size:140267\n[libx264 @ 0x7ff6d8907580] frame P:161   Avg QP:14.05  size:  2700\n[libx264 @ 0x7ff6d8907580] frame B:470   Avg QP:21.81  size:  1180\n[libx264 @ 0x7ff6d8907580] consecutive B-frames:  0.9%  0.6%  0.0% 98.4%\n[libx264 @ 0x7ff6d8907580] mb I  I16..4: 53.9% 40.2%  5.9%\n[libx264 @ 0x7ff6d8907580] mb P  I16..4:  0.4%  0.3%  0.0%  P16..4:  0.2%  0.1%  0.0%  0.0%  0.0%    skip:99.0%\n[libx264 @ 0x7ff6d8907580] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  1.0%  0.1%  0.0%  direct: 0.0%  skip:98.8%  L0:50.7% L1:48.4% BI: 0.9%\n[libx264 @ 0x7ff6d8907580] 8x8 transform intra:39.0% inter:41.4%\n[libx264 @ 0x7ff6d8907580] coded y,u,v intra: 3.0% 0.5% 0.6% inter: 0.0% 0.0% 0.0%\n[libx264 @ 0x7ff6d8907580] i16 v,h,dc,p: 91%  9%  0%  0%\n[libx264 @ 0x7ff6d8907580] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 40%  5% 55%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7ff6d8907580] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 45% 17% 20%  4%  3%  4%  2%  3%  2%\n[libx264 @ 0x7ff6d8907580] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7ff6d8907580] ref P L0: 60.0%  3.7% 25.1% 11.2%\n[libx264 @ 0x7ff6d8907580] ref B L0: 87.0% 11.7%  1.3%\n[libx264 @ 0x7ff6d8907580] ref B L1: 96.7%  3.3%\n[libx264 @ 0x7ff6d8907580] kb/s:213.50\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/gaussian/gaussian_%03d.png -c:v libx264 -vf fps=12 output_gaussian.mp4', returncode=0)"
  },
  {
    "objectID": "behind-the-scenes/sliding-window-video.html#comparison",
    "href": "behind-the-scenes/sliding-window-video.html#comparison",
    "title": "sliding window video",
    "section": "Comparison",
    "text": "Comparison\nLet’s plot in one graph the smoothed temperature for each kernel shape we calculated above (rectangular, triangular, gaussian), all of which with a 500-minute-wide window.\n\nwindow_width_min = 500.0\nwindow_width_int = int(window_width_min // 10 + 1)\n\n# rectangular, 500 min\nkernel_rect = np.ones(window_width_int)\nrect = np.convolve(df['TD'].values, kernel_rect, mode='same') / len(kernel_rect)\n\n# triangular\nhalf_triang = np.arange(1, window_width_int/2+1, 1)\nkernel_triang = np.hstack([half_triang, half_triang[-2::-1]])\nkernel_triang = kernel_triang / kernel_triang.max()\ntriang = np.convolve(df['TD'].values, kernel_triang, mode='same') / len(kernel_triang) * 2\n\n# gaussian\ngauss = df['TD'].rolling(window=window_width_int, center=True, win_type=\"gaussian\").mean(std=6)#, sym=True)\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.figure.subplots_adjust(top=0.93, bottom=0.10, left=0.1, right=0.95)\n\nax.plot(df.loc[start:end, 'TD'], color='black', label=\"measured\")\nax.plot(df.index, rect, color=\"tab:blue\", label=\"rectangular kernel\")\nax.plot(df.index, triang, color=\"tab:orange\", label=\"triangular kernel\")\nax.plot(df.index, gauss, color=\"tab:red\", label=\"gaussian kernel\")\nax.legend()\n\nax.set(ylim=[5, 17.5],\n       xlim=['2022-01-04 00:00:00', '2022-01-05 23:50:00'],\n       ylabel=\"Temperature (°C)\",\n       title=\"Yatir Forest, 2022\",\n       yticks=[5,10,15])\ncenter_dates(ax)\nfig.savefig(\"kernel_comparison.png\")\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.figure.subplots_adjust(top=0.93, bottom=0.15, left=0.1, right=0.95)\n\nN=500\nrec_window = np.zeros(800)\nrec_window[150:150+N] = signal.windows.boxcar(N)\ntri_window = np.zeros(800)\ntri_window[150:150+N] = signal.windows.triang(N)\ngau_window = np.zeros(800)\ngau_window[150:150+N] = signal.windows.gaussian(N, std=60)\nt = np.arange(-150, 650)\nax.plot(t, rec_window, color=\"tab:blue\")\nax.plot(t, tri_window, color=\"tab:orange\")\nax.plot(t, gau_window, color=\"tab:red\")\nax.text(0, 0.5, \"rect\", color=\"tab:blue\")\nax.text(373, 0.5, \"triang\", color=\"tab:orange\")\nax.text(150, 0.1, \"gauss\\n\"+r\"$\\sigma=60$ min\", color=\"tab:red\")\nax.set(ylim=[-0.1, 1.1],\n       xlim=[-150, 650],\n       ylabel=\"amplitude\",\n       xlabel=\"minutes\",\n       title=\"kernel shape comparison\",)\nfig.savefig(\"kernel_shapes.png\")"
  },
  {
    "objectID": "behind-the-scenes/savgol-video.html#savgol-filter",
    "href": "behind-the-scenes/savgol-video.html#savgol-filter",
    "title": "savgol video",
    "section": "Savgol filter",
    "text": "Savgol filter\n\n# Function to fit and get polynomial values\ndef fit_polynomial(x, y, degree):\n    coeffs = np.polyfit(x, y, degree)\n    poly = np.poly1d(coeffs)\n    return poly(x), coeffs\n\n# Function to fit and get polynomial values\ndef poly_coeffs(x, y, degree):\n    coeffs = np.polyfit(x, y, degree)\n    return coeffs\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(df.loc[start:end, 'TD'], color='black')\n\nsg = savgol_filter(df['TD'], 13, 2)\n\ni = 500\nax.plot(df.index[:i], sg[:i], color='xkcd:hot pink')\n\nwindow_pts = 31\np_order = 3\n\nwindow_x = np.arange(i - window_pts // 2, i + window_pts // 2)\nwindow_y = df['TD'][i - window_pts // 2:i + window_pts // 2]\n\n# Fit and plot polynomial inside the window\nfitted_y, coeffs = fit_polynomial(window_x, window_y, p_order)\n\nwhole_x = np.arange(len(df))\nwhole_y = df['TD'].values\npoly = np.poly1d(coeffs)\nwhole_poly = poly(whole_x)\n\nax.plot(df.index, whole_poly, color='xkcd:sun yellow', lw=2)\n# ax.plot(df.index[window_x], fitted_y, color='0.8', lw=3)\nax.plot(df.index[window_x], fitted_y, color='xkcd:mustard', lw=2)\n\n\n\nax.set(ylim=[5, 17.5],\n       xlim=[start, end],\n       ylabel=\"Temperature (°C)\",\n       title=\"Yatir Forest, 2022\",\n       yticks=[5,10,15])\ncenter_dates(ax)\n# fig.savefig(\"sliding_YF_temperature_2022.png\")\n\n\n\n\n\n\n\n\n\np_order = 3\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,5))\n\nclass Lines:\n    \"\"\"\n    empty class, later will be populated with graph objects.\n    this is useful to draw and erase lines on demand.\n    \"\"\"\n    pass\nlines = Lines()\n\n# set graph y limits\nylim = [3, 22]\n# choose here windown width in minutes\nwindow_width_min = 500.0\nwindow_width_min_integer = int(window_width_min)  # same but integer\nwindow_width_int = int(window_width_min // 10 + 1)  # window width in points\nN = len(df)  # df length\nt_swipe = pd.date_range(start=pd.to_datetime(start) - pd.Timedelta(minutes=window_width_min) - pd.Timedelta(minutes=30),\n                        end=pd.to_datetime(end) + pd.Timedelta(minutes=60),\n                        freq=\"10min\")\n# starting time\nt0 = t_swipe[0]\nind0 = df.index.get_loc(t0) + window_width_int//2 + 1\n# show sliding window on the top panel as a light blue shade\nlines.fill_bet = ax.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                           y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1)\n\nsg = savgol_filter(df['TD'], window_width_int, p_order)\ndf.loc[:, 'sg'] = sg \n# plot temperature\nax.plot(df.loc[start:end, 'TD'], color=\"black\")\n\n# define x,y data inside window to execute polyfit on\nwindow_x = np.arange(ind0 - window_width_int // 2, ind0 + window_width_int // 2)\nwindow_y = df['TD'][ind0 - window_width_int // 2:ind0 + window_width_int // 2].values\n# fit and plot polynomial inside the window\nfitted_y, coeffs = fit_polynomial(window_x, window_y, p_order)\n# get x,y data for the whole array\nwhole_x = np.arange(len(df))\nwhole_y = df['TD'].values\npoly = np.poly1d(coeffs)\nwhole_poly = poly(whole_x)\n\n# calculate the middle of the sliding window\nwindow_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n# plot a pink line showing the result of the moving average\n# from the beginning to the middle of the sliding window\nlines.pink_line, = ax.plot(df.loc[start:window_middle, 'sg'], color=\"xkcd:hot pink\", lw=3)\n\nlines.poly_all, = ax.plot(df.index, whole_poly, color='xkcd:sun yellow', lw=2)\nlines.poly_window, = ax.plot(df.index[window_x], fitted_y, color='xkcd:mustard', lw=2)\n\n# emphasize the location of the middle on the window with a circle\nlines.pink_circle, = ax.plot([window_middle], [df.loc[window_middle, 'sg']],\n         marker='o', markerfacecolor=\"None\", markeredgecolor=\"xkcd:dark pink\", markeredgewidth=2,\n         markersize=8)\n# some explanation\nax.text(0.99, 0.97, f\"savitzky-golay\\nwidth = {window_width_int:.0f} pts\\npoly order = {p_order}\", transform=ax.transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontsize=14)\n# axis tweaking\nax.set(ylim=ylim,\n        xlim=[start, end],\n        ylabel=\"Temperature (°C)\",\n        yticks=[5,10,15,20],\n        title=\"Yatir Forest, 2022\")\n# adjust dates on both panels as defined before\ncenter_dates(ax)\n\ndef update_swipe(k, lines):\n    \"\"\"\n    updates both panels, given the index k along which the window is sliding\n    \"\"\"\n    # left side of the sliding window\n    t0 = t_swipe[k]\n    # middle position\n    window_middle = t0 + pd.Timedelta(minutes=window_width_min/2)\n    ind0 = df.index.get_loc(window_middle)\n    # erase previous blue shade on the top graph\n    lines.fill_bet.remove()\n    # fill again the blue shade in the updated window position\n    lines.fill_bet = ax.fill_between([t0, t0 + pd.Timedelta(minutes=window_width_min)],\n                                               y1=ylim[0], y2=ylim[1], alpha=0.1, zorder=-1, color=\"tab:blue\")\n    # update pink curve\n    lines.pink_line.set_data(df[start:window_middle].index,\n                             df.loc[start:window_middle, 'sg'].values)\n    # update pink circle\n    lines.pink_circle.set_data([window_middle], [df.loc[window_middle, 'sg']])\n    # define x,y data inside window to execute polyfit on\n    \n    window_x = np.arange(ind0 - window_width_int // 2, ind0 + window_width_int // 2)\n    window_y = df['TD'][ind0 - window_width_int // 2:ind0 + window_width_int // 2]\n    # fit and plot polynomial inside the window\n    fitted_y, coeffs = fit_polynomial(window_x, window_y, p_order)\n    poly = np.poly1d(coeffs)\n    whole_poly = poly(whole_x)\n    lines.poly_all.set_data(df.index, whole_poly)\n    lines.poly_window.set_data(df.index[window_x], fitted_y)\n\nfig.savefig(f\"pngs/savgol{window_width_int}/savgol_zero.png\", dpi=600)\n\n# create a tqdm progress bar\nprogress_bar = tqdm(total=len(t_swipe), unit=\"iteration\")\n# loop over all sliding indices, update graph and then save it\nfor fignum, i in enumerate(np.arange(0, len(t_swipe)-1, 1)):\n    update_swipe(i, lines)\n    fig.savefig(f\"pngs/savgol{window_width_int}/savgol_{window_width_int}_{fignum:03}.png\", dpi=600)\n    # update the progress bar\n    progress_bar.update(1)\n# close the progress bar\nprogress_bar.close()\n\n100%|█████████▉| 634/635 [13:07<00:01,  1.24s/iteration]\n\n\n\n\n\nCombine all saved images into one mp4 video.\n\n# Define the path to your PNG images\npngs_path = f\"pngs/savgol51\"\npngs_name = f\"savgol_51_%03d.png\"\n\n# Define the output video file path\nvideo_output = f\"output_savgol51.mp4\"\n\n# Use ffmpeg to create a video from PNG images\n# desired framerate. choose 24 if you don't know what to do\nfr = 12\n# run command\nffmpeg_cmd = f\"ffmpeg -framerate {fr} -i {pngs_path}/{pngs_name} -c:v libx264 -vf fps={fr} {video_output}\"\nsubprocess.run(ffmpeg_cmd, shell=True)\n\nffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/6.1.1_2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n  libavutil      58. 29.100 / 58. 29.100\n  libavcodec     60. 31.102 / 60. 31.102\n  libavformat    60. 16.100 / 60. 16.100\n  libavdevice    60.  3.100 / 60.  3.100\n  libavfilter     9. 12.100 /  9. 12.100\n  libswscale      7.  5.100 /  7.  5.100\n  libswresample   4. 12.100 /  4. 12.100\n  libpostproc    57.  3.100 / 57.  3.100\nInput #0, image2, from 'pngs/savgol51/savgol_51_%03d.png':\n  Duration: 00:00:52.83, start: 0.000000, bitrate: N/A\n  Stream #0:0: Video: png, rgba(pc, gbr/unknown/unknown), 4800x3000 [SAR 23622:23622 DAR 8:5], 12 fps, 12 tbr, 12 tbn\nStream mapping:\n  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\nPress [q] to stop, [?] for help\n[libx264 @ 0x7f9cb7906dc0] using SAR=1/1\n[libx264 @ 0x7f9cb7906dc0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x7f9cb7906dc0] profile High 4:4:4 Predictive, level 6.0, 4:4:4, 8-bit\n[libx264 @ 0x7f9cb7906dc0] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'output_savgol51.mp4':\n  Metadata:\n    encoder         : Lavf60.16.100\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 4800x3000 [SAR 1:1 DAR 8:5], q=2-31, 12 fps, 12288 tbn\n    Metadata:\n      encoder         : Lavc60.31.102 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n[out#0/mp4 @ 0x7f9cb7806000] video:3513kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.225481%\nframe=  634 fps=3.2 q=-1.0 Lsize=    3521kB time=00:00:52.58 bitrate= 548.5kbits/s speed=0.27x    \n[libx264 @ 0x7f9cb7906dc0] frame I:3     Avg QP:13.70  size:146882\n[libx264 @ 0x7f9cb7906dc0] frame P:232   Avg QP:19.02  size:  7856\n[libx264 @ 0x7f9cb7906dc0] frame B:399   Avg QP:24.04  size:  3341\n[libx264 @ 0x7f9cb7906dc0] consecutive B-frames: 11.4% 10.7% 10.4% 67.5%\n[libx264 @ 0x7f9cb7906dc0] mb I  I16..4: 32.8% 61.0%  6.2%\n[libx264 @ 0x7f9cb7906dc0] mb P  I16..4:  0.5%  0.7%  0.3%  P16..4:  0.4%  0.2%  0.1%  0.0%  0.0%    skip:97.7%\n[libx264 @ 0x7f9cb7906dc0] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  1.9%  0.3%  0.0%  direct: 0.0%  skip:97.6%  L0:50.4% L1:47.8% BI: 1.7%\n[libx264 @ 0x7f9cb7906dc0] 8x8 transform intra:51.3% inter:35.8%\n[libx264 @ 0x7f9cb7906dc0] coded y,u,v intra: 7.2% 6.5% 4.4% inter: 0.1% 0.1% 0.0%\n[libx264 @ 0x7f9cb7906dc0] i16 v,h,dc,p: 89% 10%  1%  0%\n[libx264 @ 0x7f9cb7906dc0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31%  3% 65%  0%  0%  0%  0%  0%  0%\n[libx264 @ 0x7f9cb7906dc0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 54%  7% 23%  3%  1%  4%  1%  5%  1%\n[libx264 @ 0x7f9cb7906dc0] Weighted P-Frames: Y:0.0% UV:0.0%\n[libx264 @ 0x7f9cb7906dc0] ref P L0: 58.7%  5.7% 25.2% 10.4%\n[libx264 @ 0x7f9cb7906dc0] ref B L0: 85.9% 11.8%  2.4%\n[libx264 @ 0x7f9cb7906dc0] ref B L1: 96.9%  3.1%\n[libx264 @ 0x7f9cb7906dc0] kb/s:544.58\n\n\nCompletedProcess(args='ffmpeg -framerate 12 -i pngs/savgol51/savgol_51_%03d.png -c:v libx264 -vf fps=12 output_savgol51.mp4', returncode=0)"
  },
  {
    "objectID": "behind-the-scenes/API-IMS.html",
    "href": "behind-the-scenes/API-IMS.html",
    "title": "API to download data from IMS",
    "section": "",
    "text": "# TOKEN = \"f058958a-d8bd-47cc-95d7-7ecf98610e47\"\n# STATION_NUM = 28  # 28 = \"SHANI\"\n# DATA = 10  # 10 = TDmax (max temperature)\n# start = \"2022/01/01\"\n# end = \"2022/02/01\"\n# url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}\"\n# url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}/data/?from={start}&to={end}\"\n# # url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}/data/{DATA}/data/11/?from={start}&to={end}\"\n# headers = {'Authorization': 'ApiToken f058958a-d8bd-47cc-95d7-7ecf98610e47'}\n# response = requests.request(\"GET\", url, headers=headers)\n# data= json.loads(response.text.encode('utf8'))\n\n# # Save the JSON data to a file\n# with open('shani_2022_january.json', 'w') as json_file:\n#     json.dump(data, json_file)\n\n# data\n\n\n# # https://ims.gov.il/he/ObservationDataAPI\n# # https://ims.gov.il/sites/default/files/2021-09/API%20explanation.pdf\n# # https://ims.gov.il/sites/default/files/2022-04/Python%20API%20example.pdf\n# TOKEN = \"f058958a-d8bd-47cc-95d7-7ecf98610e47\"\n# STATION_NUM = 23  # 23 = \"JERUSALEM CENTRE\"\n# DATA = 9  # 9 = TDmax (max temperature)\n# start = \"2022/01/01\"\n# end = \"2022/02/01\"\n# url = f\"https://api.ims.gov.il/v1/envista/stations/{STATION_NUM}/data/{DATA}/?from={start}&to={end}\"\n# headers = {'Authorization': 'ApiToken f058958a-d8bd-47cc-95d7-7ecf98610e47'}\n# response = requests.request(\"GET\", url, headers=headers)\n# data= json.loads(response.text.encode('utf8'))\n\n# print(url)\n\n\n# url = \"https://api.ims.gov.il/v1/envista/stations/28/data/10/data/11/?from=2022/01/01&to=2022/01/03\"\n# response = requests.request(\"GET\", url, headers=headers)\n# data = json.loads(response.text.encode('utf8'))\n\n\n# # RH = 8\n# # TDmax = 10, max temperature\n# # TDmin = 11, min temperature\n# url = \"https://api.ims.gov.il/v1/envista/stations/28/data/10/?from=2022/01/01&to=2022/01/31\"\n# response = requests.request(\"GET\", url, headers=headers)\n# data = json.loads(response.text.encode('utf8'))\n\n# df = pd.json_normalize(data['data'],record_path=['channels'], meta=['datetime'])\n# df['date'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)  # ignore time zone information\n# df = df.set_index('date')\n\n# df\n# data['data']"
  },
  {
    "objectID": "behind-the-scenes/consecutive_sequence.html",
    "href": "behind-the-scenes/consecutive_sequence.html",
    "title": "remove consecutive values",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport warnings\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nsns.set(style=\"ticks\", font_scale=1.5)  # white graphs, with large and legible letters\n\n# %matplotlib widget\n\ncreate data, put some defective windows here and there…\n\nsteps = np.random.randint(low=-2, high=2, size=500)\ndata = steps.cumsum()\ndate_range = pd.date_range(start='2023-01-01', periods=len(data), freq='1D')\ndf = pd.DataFrame({'series': data}, index=date_range)\n\n# make sequence of consecutive values\ndf.loc['2023-06-05':'2023-07-20', 'series'] = 2\ndf.loc['2023-10-05':'2023-10-25', 'series'] = -150\n\nplot\n\ndef concise(ax):\n    locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n    formatter = mdates.ConciseDateFormatter(locator)\n    ax.xaxis.set_major_locator(locator)\n    ax.xaxis.set_major_formatter(formatter)\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(df['series'], color=\"tab:blue\")\nconcise(ax)\nax.legend(frameon=False)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n<matplotlib.legend.Legend at 0x7fe480a66230>\n\n\n\n\n\nnice function, keep that for future reference\n\n# function to copy paste:\ndef conseq_series(series, N):\n    \"\"\"\n    part A:\n    1. assume a string of 5 equal values. that's what we want to identify\n    2. diff produces a string of only 4 consecutive zeros\n    3. no problem, because when applying cumsum, the 4 zeros turn into a plateau of 5, that's what we want\n    so far, so good\n    part B:\n    1. groupby value_grp splits data into groups according to cumsum.\n    2. because cumsum is monotonically increasing, necessarily all groups will be composed of neighboring rows, no funny business\n    3. what are those groups made of? of rows of column 'series'. this specific column is not too important, because:\n    4. count 'counts' the number of elements inside each group.\n    5. the real magic here is that 'transform' assigns each row of the original group with the count result.\n    6. finally, we can ask the question: which rows belong to a string of identical values greater-equal than some threshold.\n    zehu, you now have a mask (True-False) with the same shape as the original series.\n\n    \"\"\"\n    # part A:\n    sumsum_series = (\n                   (series.diff() != 0)         # diff zero becomes false, otherwise true\n                      .astype('int')           # true -> 1  , false -> 0\n                      .cumsum()                # cumulative sum, monotonically increasing\n                  )\n    # part B:\n    mask_outliers = (\n                    series.groupby(sumsum_series)           # take original series and group it by values of cumsum\n                                .transform('count')        # now count how many are in each group, assign result to each existing row. that's what transform does\n                                .ge(N)                    # if row count >= than user-defined n_consecutives, assign True, otherwise False\n                    )\n    \n    # apply mask:\n    result = pd.Series(np.where(mask_outliers,\n                                np.nan,  # use this if mask_outliers is True\n                                series), # otherwise\n                            index=series.index)\n    return result\n\nplot results. it works :)\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(df['series'], color=\"tab:blue\", label='original')\nax.plot(conseq_series(df['series'], 10)-5, c='tab:red', label='clean\\n(-5 for visual)')\nconcise(ax)\nax.legend(frameon=False);"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#define-functions",
    "href": "behind-the-scenes/outliers_graphs.html#define-functions",
    "title": "outliers graphs",
    "section": "define functions",
    "text": "define functions\n\ndef random_walk_with_outliers(origin, n_steps, perc_outliers=0.0, outlier_mult=10, seed=42):\n    '''\n    Function for generating a random time series based on random walk.\n    It adds a specified percentage of outliers by multiplying the random walk step by a scalar.\n    \n    Parameters\n    ------------\n    origin : int\n        The starting point of the series\n    n_steps : int\n        Lenght of the series\n    perc_outliers : float\n        Percentage of outliers to introduce to the series [0.0-1.0]\n    outlier_mult : float\n        Scalar by which to multiply the RW increment to create an outlier\n    seed : int\n        Random seed\n\n    Returns\n    -----------\n    rw : np.ndarray\n        The generated random walk series with outliers\n    indices : np.ndarray\n        The indices of the introduced outliers \n    '''\n    assert (perc_outliers >= 0.0) & (perc_outliers <= 1.0)\n    \n    #set seed for reproducibility\n    np.random.seed(seed)\n    \n    # possible steps\n    steps = [-1, 1]\n\n    # simulate steps\n    steps = np.random.choice(a=steps, size=n_steps-1)\n    rw = np.append(origin, steps).cumsum(0)\n    \n    # add outliers\n    n_outliers = int(np.round(perc_outliers * n_steps, 0))\n    indices = np.random.randint(0, len(rw), n_outliers)\n    rw[indices] = rw[indices] + steps[indices + 1] * outlier_mult\n    \n    return rw, indices\n\ndef concise(ax):\n    locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n    formatter = mdates.ConciseDateFormatter(locator)\n    ax.xaxis.set_major_locator(locator)\n    ax.xaxis.set_major_formatter(formatter)"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#load-and-process-data",
    "href": "behind-the-scenes/outliers_graphs.html#load-and-process-data",
    "title": "outliers graphs",
    "section": "load and process data",
    "text": "load and process data\n\nstart = '2023-01-10 00:00:00'\nn_steps = 1000\nrw39, outlier_ind39 = random_walk_with_outliers(origin=0,\n                                            n_steps=n_steps,\n                                            perc_outliers=0.0031,\n                                            outlier_mult=50,\n                                            seed=39)\ndate_range = pd.date_range(start, periods=n_steps, freq='1min')\ndf = pd.DataFrame({'date': date_range, 'signal': rw39}).set_index('date')\nstart = df.index[0]\nend = df.index[-1]\n\n\nrw40, outlier_ind40 = random_walk_with_outliers(origin=0,\n                                            n_steps=n_steps,\n                                            perc_outliers=0.0031,\n                                            outlier_mult=50,\n                                            seed=40)\ndf['signal40'] = rw40\n\ndf.loc['2023-01-10 04:40:00', 'signal40'] = 43.0\n\n\n# rw41, outlier_ind41 = random_walk_with_outliers(origin=0,\n#                                             n_steps=n_steps,\n#                                             perc_outliers=0.0031,\n#                                             outlier_mult=50,\n#                                             seed=41)\n# df['signal41'] = rw41"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#stationary-signal",
    "href": "behind-the-scenes/outliers_graphs.html#stationary-signal",
    "title": "outliers graphs",
    "section": "stationary signal",
    "text": "stationary signal\n\nfig, ax = plt.subplots(figsize=(8,4))\n# plot signal\nax.plot(df['signal'], color=\"tab:blue\")\n# make graph look nice\nax.set(ylabel='values',\n       xlim=[start,end],\n       title=\"a stationary signal\",\n       ylim=[-45, 80])\nconcise(ax)\nfig.savefig(\"signal_39_stationary.png\", bbox_inches='tight')"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#visual-inspection",
    "href": "behind-the-scenes/outliers_graphs.html#visual-inspection",
    "title": "outliers graphs",
    "section": "visual inspection",
    "text": "visual inspection\n\nfig, ax = plt.subplots(figsize=(8,4))\n# plot signal\nax.plot(df['signal'], color=\"tab:blue\")\n# plot horizontal lines\nax.plot([start, end], [40]*2, color=\"black\", alpha=0.4)\nax.text(end, 40, \" 40\", va=\"center\")\nax.plot([start, end], [-30]*2, color=\"black\", alpha=0.4)\nax.text(end, -30, \" -30\", va=\"center\")\n# find and plot outliers\noutliers_index = df.index[(df['signal'] > 40) | (df['signal'] < -30)]\nax.plot(df.loc[outliers_index, 'signal'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax.set(ylabel='values',\n       xlim=[start,end],\n       title=\"visual inspection\",\n       ylim=[-45, 80])\nconcise(ax)\nfig.savefig(\"outliers_visual_inspection.png\", bbox_inches='tight')"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#mean---3-std",
    "href": "behind-the-scenes/outliers_graphs.html#mean---3-std",
    "title": "outliers graphs",
    "section": "mean +- 3 std",
    "text": "mean +- 3 std\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\navg = df['signal'].mean()\nstd = df['signal'].std()\n\n# plot signal\nax0.plot(df['signal'], color=\"tab:blue\")\n\nsns.kdeplot(data=df, y='signal', shade=True, ax=ax1)\n\npdf_xlim = ax1.get_xlim()\n# plot horizontal lines\n# mean\nax0.plot([start, end], [avg]*2, color=\"black\", zorder=-10, alpha=0.7)\nax1.plot(pdf_xlim, [avg]*2, color=\"black\", alpha=0.7)\nax1.text(1.1*pdf_xlim[1], avg, \"mean\", va=\"center\")\n# mean + std\nax0.plot([start, end], [avg+std]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [avg+std]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], avg+std, r\"mean$+$std\", va=\"center\", alpha=0.4)\n# mean - std\nax0.plot([start, end], [avg-std]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [avg-std]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], avg-std, r\"mean$-$std\", va=\"center\", alpha=0.4)\n\nn_sigma = 3\n# mean + 3std\nax0.plot([start, end], [avg+n_sigma*std]*2, color=\"tab:red\")\nax1.plot(pdf_xlim, [avg+n_sigma*std]*2, color=\"tab:red\")\nax1.text(1.1*pdf_xlim[1], avg+n_sigma*std, r\"mean$+3\\cdot$std\", va=\"center\", color=\"tab:red\")\n# mean - 3std\nax0.plot([start, end], [avg-n_sigma*std]*2, color=\"tab:red\")\nax1.plot(pdf_xlim, [avg-n_sigma*std]*2, color=\"tab:red\")\nax1.text(1.1*pdf_xlim[1], avg-n_sigma*std, r\"mean$-3\\cdot$std\", va=\"center\", color=\"tab:red\")\n\n# find and plot outliers\noutliers_index = df.index[(df['signal'] > avg + n_sigma*std) | \n                          (df['signal'] < avg - n_sigma*std)\n                         ]\nax0.plot(df.loc[outliers_index, 'signal'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-45, 80],\n       title=r\"threshold: mean $\\pm3$ std\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-45, 80],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\nfig.savefig(\"outliers_3sigma.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/653833997.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#iqr",
    "href": "behind-the-scenes/outliers_graphs.html#iqr",
    "title": "outliers graphs",
    "section": "IQR",
    "text": "IQR\n\n# get kdeplot data\nfig, ax = plt.subplots(figsize=(8,4))\nmy_kde = sns.kdeplot(df['signal'], bw_adjust=0.5)\nline = my_kde.lines[0]\nkde_vals, kde_pdf = line.get_data()\nkde_cdf = np.cumsum(kde_pdf) / np.sum(kde_pdf)\n\ndef find_nearest(array, value):\n    return (np.abs(array - value)).argmin()\n\n# Find the boundaries where the KDE is 25% and 75% of the area\nQ1_index = find_nearest(kde_cdf, 0.25)\nQ1_boundary = kde_vals[Q1_index]\nQ3_index = find_nearest(kde_cdf, 0.75)\nQ3_boundary = kde_vals[Q3_index]\nIQR = Q3_boundary - Q1_boundary\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,4))\n\nsns.kdeplot(df['signal'], ax=ax, shade=True, bw_adjust=0.5)\n\nax.fill_between(x=kde_vals[Q1_index:Q3_index],\n                y1=kde_pdf[Q1_index:Q3_index],\n                color=\"tab:pink\"\n                )\n\nh = 0.02\nax.annotate(\"\",\n            xy=(Q1_boundary, h), xycoords='data',\n            xytext=(Q3_boundary, h), textcoords='data',\n            size=20,\n            arrowprops=dict(arrowstyle=\"<->\",\n                            connectionstyle=\"arc3,rad=0.0\",\n                            shrinkA=0, shrinkB=0,\n                            linewidth=2.5\n                            ),\n            )\nax.annotate(\"\",\n            xy=(Q1_boundary, h), xycoords='data',\n            xytext=(Q3_boundary, h), textcoords='data',\n            size=20,\n            arrowprops=dict(arrowstyle=\"<->\",\n                            connectionstyle=\"arc3,rad=0.0\",\n                            shrinkA=0, shrinkB=0,\n                            linewidth=2.5\n                            ),\n            )\n\nax.annotate(\"Q1\\nquantile 0.25\",\n            xy=(Q1_boundary, 0.025), xycoords='data',\n            xytext=(Q1_boundary-IQR, 0.040), textcoords='data',\n            size=20,\n            ha=\"right\",\n            va=\"top\",\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=5\",\n                            shrinkA=0, shrinkB=0,\n                            linewidth=2.5,\n                            color=\"black\"\n                            ),\n            )\n\nax.annotate(\"Q3\\nquantile 0.75\",\n            xy=(Q3_boundary, 0.025), xycoords='data',\n            xytext=(Q3_boundary+IQR, 0.040), textcoords='data',\n            size=20,\n            ha=\"left\",\n            va=\"top\",\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=5\",\n                            shrinkA=0, shrinkB=0,\n                            linewidth=2.5,\n                            color=\"black\"\n                            ),\n            )\n\nax.annotate(r\"Q3 + 1.5$\\cdot$IQR\",\n            xy=(Q3_boundary+1.5*IQR, 0.00), xycoords='data',\n            xytext=(Q3_boundary+1.5*IQR, 0.015), textcoords='data',\n            size=20,\n            ha=\"left\",\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=5\",\n                            shrinkA=0, shrinkB=0,\n                            linewidth=2.5,\n                            color=\"black\"\n                            ),\n            )\n\nax.annotate(r\"Q1 - 1.5$\\cdot$IQR\",\n            xy=(Q1_boundary-1.5*IQR, 0.00), xycoords='data',\n            xytext=(Q1_boundary-1.5*IQR, 0.015), textcoords='data',\n            size=20,\n            ha=\"right\",\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=5\",\n                            shrinkA=0, shrinkB=0,\n                            linewidth=2.5,\n                            color=\"black\"\n                            ),\n            )\n\nax.text(Q1_boundary+IQR/2, 0.018, \"IQR\",\n        ha=\"center\", va=\"top\", color=\"white\")\nax.text(Q1_boundary+IQR/2, 0.013, r\"50% of\"+\"\\nthe area\",\n        ha=\"center\", va=\"top\", color=\"black\")\n\nax.set(xlim=[Q1_boundary-5*IQR, Q3_boundary+5*IQR],\n       ylabel=\"pdf\",)\nfig.savefig(\"IQR_pdf.png\", bbox_inches='tight')\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\nmedian = df['signal'].quantile(0.50)\nQ1 = df['signal'].quantile(0.25)\nQ3 = df['signal'].quantile(0.75)\nIQR = Q3 - Q1\n\n# plot signal\nax0.plot(df['signal'], color=\"tab:blue\")\n\nmy_kde = sns.kdeplot(data=df, y='signal', shade=True, ax=ax1, bw_adjust=0.5)\npdf_xlim =ax1.get_xlim()\n\nkde_q1_idx = find_nearest(kde_vals, Q1)\nkde_q3_idx = find_nearest(kde_vals, Q3)\nax1.fill_betweenx(y=kde_vals[kde_q1_idx:kde_q3_idx],\n                  x1=kde_pdf[kde_q1_idx:kde_q3_idx],\n                  color=\"tab:pink\")\n# plot horizontal lines\n# Q3 + 1.5 IQR\nax0.plot([start, end], [Q3+1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [Q3+1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], Q3+1.5*IQR, \"Q3+1.5*IQR\", va=\"center\", alpha=0.4)\n# Q1 - 1.5 IQR\nax0.plot([start, end], [Q1-1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [Q1-1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], Q1-1.5*IQR, \"Q1-1.5*IQR\", va=\"center\", alpha=0.4)\n\n# find and plot outliers\noutliers_index = df.index[(df['signal'] > Q3+1.5*IQR) | \n                          (df['signal'] < Q1-1.5*IQR)\n                         ]\nax0.plot(df.loc[outliers_index, 'signal'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-45, 80],\n       title=r\"threshold: Q$\\pm1.5$ IQR\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-45, 80],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\nfig.savefig(\"outliers_1.5IQR.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/3077230861.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#non-stationary-signal",
    "href": "behind-the-scenes/outliers_graphs.html#non-stationary-signal",
    "title": "outliers graphs",
    "section": "non stationary signal",
    "text": "non stationary signal\n\nfig, ax = plt.subplots(figsize=(8,4))\n# plot signal\nax.plot(df['signal40'], color=\"tab:blue\")\n# make graph look nice\nax.set(ylabel='values',\n       xlim=[start,end],\n       title=\"a non-stationary signal\",\n       ylim=[-45, 80])\nconcise(ax)\nfig.savefig(\"signal_40_non_stationary.png\", bbox_inches='tight')"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#running---3-std",
    "href": "behind-the-scenes/outliers_graphs.html#running---3-std",
    "title": "outliers graphs",
    "section": "running +- 3 std",
    "text": "running +- 3 std\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\navg = df['signal40'].mean()\nstd = df['signal40'].std()\n\n# plot signal\nax0.plot(df['signal40'], color=\"tab:blue\")\n\nsns.kdeplot(data=df, y='signal40', shade=True, ax=ax1)\n\n\n# plot horizontal lines\n# mean\nax0.plot([start, end], [avg]*2, color=\"black\", zorder=-10, alpha=0.7)\nax1.plot(pdf_xlim, [avg]*2, color=\"black\", alpha=0.7)\nax1.text(1.1*pdf_xlim[1], avg, \"mean\", va=\"center\")\n# mean + std\nax0.plot([start, end], [avg+std]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [avg+std]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], avg+std, \"mean+std\", va=\"center\", alpha=0.4)\n# mean - std\nax0.plot([start, end], [avg-std]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [avg-std]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], avg-std, \"mean-std\", va=\"center\", alpha=0.4)\n\nn_sigma = 3\n# mean + 3std\nax0.plot([start, end], [avg+n_sigma*std]*2, color=\"tab:red\")\nax1.plot(pdf_xlim, [avg+n_sigma*std]*2, color=\"tab:red\")\nax1.text(1.1*pdf_xlim[1], avg+n_sigma*std, r\"mean $+3\\times$std\", va=\"center\", color=\"tab:red\")\n# mean - 3std\nax0.plot([start, end], [avg-n_sigma*std]*2, color=\"tab:red\")\nax1.plot(pdf_xlim, [avg-n_sigma*std]*2, color=\"tab:red\")\nax1.text(1.1*pdf_xlim[1], avg-n_sigma*std, r\"mean $-3\\times$std\", va=\"center\", color=\"tab:red\")\n\n# find and plot outliers\noutliers_index = df.index[(df['signal40'] > avg + n_sigma*std) | \n                          (df['signal40'] < avg - n_sigma*std)\n                         ]\nax0.plot(df.loc[outliers_index, 'signal40'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-45, 120],\n       title=r\"threshold: mean $\\pm3$ std\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-45, 120],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\nfig.savefig(\"outliers_3sigma_seed40.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/372016966.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])\n\n\n\n\n\n\ndf['signal40_rol_mean'] = df['signal40'].rolling('60min', center=True).mean()\ndf['signal40_rol_std'] = df['signal40'].rolling('60min', center=True).std()\n\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\navg = df['signal40'].mean()\nstd = df['signal40'].std()\n\n# plot signal\nax0.plot(df['signal40'], color=\"tab:blue\")\n\nsns.kdeplot(data=df, y='signal40', shade=True, ax=ax1)\n\n# mean\nax0.plot(df['signal40_rol_mean'], color=\"black\", alpha=0.7, label=\"mean\")\n# mean +- 3 std\nn_sigma = 3\nax0.plot(df['signal40_rol_mean']+ n_sigma*df['signal40_rol_std'], color=\"tab:red\")\nplot_threshold, = ax0.plot(df['signal40_rol_mean']- n_sigma*df['signal40_rol_std'],\n                           color=\"tab:red\", label=\"threshold\")\n\n# find and plot outliers\noutliers_index = df.index[(df['signal40'] > df['signal40_rol_mean']+ n_sigma*df['signal40_rol_std']) | \n                          (df['signal40'] < df['signal40_rol_mean']- n_sigma*df['signal40_rol_std'])\n                         ]\nax0.plot(df.loc[outliers_index, 'signal40'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-45, 120],\n       title=r\"1-hour rolling window: mean $\\pm3$ std\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-45, 120],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\nax0.legend(frameon=False, loc=\"upper left\")\n\nfig.savefig(\"outliers_rolling_3std.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/142534422.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#running-q---iqr",
    "href": "behind-the-scenes/outliers_graphs.html#running-q---iqr",
    "title": "outliers graphs",
    "section": "running: Q +- IQR",
    "text": "running: Q +- IQR\n\nfig, ax = plt.subplots(figsize=(8,4))\n\nmy_kde = sns.kdeplot(df['signal40'], bw_adjust=0.5)\nline = my_kde.lines[0]\nkde_vals, kde_pdf = line.get_data()\nkde_cdf = np.cumsum(kde_pdf) / np.sum(kde_pdf)\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\nmedian = df['signal40'].quantile(0.50)\nQ1 = df['signal40'].quantile(0.25)\nQ3 = df['signal40'].quantile(0.75)\nIQR = Q3 - Q1\n\n# plot signal\nax0.plot(df['signal40'], color=\"tab:blue\")\n\nmy_kde = sns.kdeplot(data=df, y='signal40', shade=True, ax=ax1, bw_adjust=0.5)\npdf_xlim =ax1.get_xlim()\n\nkde_q1_idx = find_nearest(kde_vals, Q1)\nkde_q3_idx = find_nearest(kde_vals, Q3)\nax1.fill_betweenx(y=kde_vals[kde_q1_idx:kde_q3_idx],\n                  x1=kde_pdf[kde_q1_idx:kde_q3_idx],\n                  color=\"tab:pink\")\n# plot horizontal lines\n# Q3 + 1.5 IQR\nax0.plot([start, end], [Q3+1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [Q3+1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], Q3+1.5*IQR, \"Q3+1.5*IQR\", va=\"center\", alpha=0.4)\n# Q1 - 1.5 IQR\nax0.plot([start, end], [Q1-1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.plot(pdf_xlim, [Q1-1.5*IQR]*2, color=\"black\", alpha=0.4)\nax1.text(1.1*pdf_xlim[1], Q1-1.5*IQR, \"Q1-1.5*IQR\", va=\"center\", alpha=0.4)\n\n# find and plot outliers\noutliers_index = df.index[(df['signal40'] > Q3+1.5*IQR) | \n                          (df['signal40'] < Q1-1.5*IQR)\n                         ]\nax0.plot(df.loc[outliers_index, 'signal40'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-50, 140],\n       title=r\"threshold: Q$\\pm1.5$ IQR\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-50, 140],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\nfig.savefig(\"outliers_1.5IQR_seed40.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/678430211.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])\n\n\n\n\n\n\ndef Q1(window):\n    return window.quantile(0.25)\ndef Q3(window):\n    return window.quantile(0.75)\n\n\ndf['signal40_rol_Q1'] = df['signal40'].rolling('60min', center=True).apply(Q1)\ndf['signal40_rol_Q3'] = df['signal40'].rolling('60min', center=True).apply(Q3)\ndf['signal40_rol_IQR'] = df['signal40_rol_Q3'] - df['signal40_rol_Q1']\n\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\navg = df['signal40'].mean()\nstd = df['signal40'].std()\n\n# plot signal\nax0.plot(df['signal40'], color=\"tab:blue\")\n\nsns.kdeplot(data=df, y='signal40', shade=True, ax=ax1)\n\n# median\n# ax0.plot(df['signal40_rol_median'], color=\"black\", alpha=0.7, label=\"median\")\n# Q1 - 1.5 IQR\nthreshold_bottom = df['signal40_rol_Q1'] - 1.5*df['signal40_rol_IQR']\nax0.plot(threshold_bottom, color=\"tab:red\")\n# Q3 + 1.5 IQR\nthreshold_top = df['signal40_rol_Q3'] + 1.5*df['signal40_rol_IQR']\nax0.plot(threshold_top, color=\"tab:red\")\n\n# find and plot outliers\noutliers_index = df.index[(df['signal40'] > threshold_top) | \n                          (df['signal40'] < threshold_bottom)\n                         ]\nax0.plot(df.loc[outliers_index, 'signal40'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-45, 120],\n       title=r\"1-hour rolling threshold: Q $\\pm1.5$ IQR\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-45, 120],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\n\nax0.legend(frameon=False, loc=\"upper left\")\n\n\nfig.savefig(\"outliers_rolling_IQR.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/3108172465.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#hampel-running-mad",
    "href": "behind-the-scenes/outliers_graphs.html#hampel-running-mad",
    "title": "outliers graphs",
    "section": "Hampel, running MAD",
    "text": "Hampel, running MAD\n\n# def MAD(window):\n#     return np.median(                                # 3. compute median\n#                      np.abs(                         # 2. take absolute values\n#                          window - np.median(window)  # 1. calculate residuals\n#                          )\n#                     )\n\nk = 1.4826 # scale factor for Gaussian distribution\ndef MAD(window):\n    return (window - np.median(window)).abs().median()\n\n\ndf['signal40_rol_mad'] = k * df['signal40'].rolling('60min', center=True).apply(MAD)\ndf['signal40_rol_median'] = df['signal40'].rolling('60min', center=True).median()\n\n\nfig, ax = plt.subplots(figsize=(8,4))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.2], height_ratios=[1])\ngs.update(left=0.10, right=0.90, top=0.95, bottom=0.13,\n          hspace=0.02, wspace=0.02)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[0, 1])\n\n# plot signal\nax0.plot(df['signal40'], color=\"tab:blue\")\n\nsns.kdeplot(data=df, y='signal40', shade=True, ax=ax1)\n\n# median\n# ax0.plot(df['signal40_rol_median'], color=\"black\", alpha=0.7, label=\"median\")\n# Q1 - 1.5 IQR\nthreshold_bottom = df['signal40_rol_median'] - 3 * df['signal40_rol_mad']\nax0.plot(threshold_bottom, color=\"tab:red\")\n# Q3 + 1.5 IQR\nthreshold_top = df['signal40_rol_median'] + 3 * df['signal40_rol_mad']\nax0.plot(threshold_top, color=\"tab:red\")\n\n# find and plot outliers\noutliers_index = df.index[(df['signal40'] > threshold_top) | \n                          (df['signal40'] < threshold_bottom)\n                         ]\nax0.plot(df.loc[outliers_index, 'signal40'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n# make graph look nice\nax0.set(ylabel='values',\n       xlim=[start,end],\n       ylim=[-45, 120],\n       title=r\"1-hour rolling threshold: median $\\pm3$ MAD\",\n       )\nconcise(ax0)\nax1.set(xlabel='pdf',\n        ylabel='',\n        ylim=[-45, 120],\n        yticks=[],\n        xticks=[0, 0.03],\n        xticklabels=['0', '0.03']\n        )\n\nax0.legend(frameon=False, loc=\"upper left\")\n\n\nfig.savefig(\"outliers_rolling_MAD.png\", bbox_inches='tight')\n\n/var/folders/c3/7hp0d36n6vv8jc9hm2440__00000gn/T/ipykernel_73326/3446982039.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax0 = plt.subplot(gs[0, 0])\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#stationary-mad",
    "href": "behind-the-scenes/outliers_graphs.html#stationary-mad",
    "title": "outliers graphs",
    "section": "stationary MAD",
    "text": "stationary MAD\n\nfig, ax = plt.subplots(figsize=(8,4))\n# plot signal\nax.plot(df['signal'], color=\"tab:blue\")\n\nk = 1.4826 # scale factor for Gaussian distribution\nmad = median_abs_deviation(df['signal'])\nmedian = df['signal'].median()\n\nxlim = ax.get_xlim()\n\n# median +- 3*k*mad\nax.plot([start, end], [median]*2, color=\"black\")\n# ax.text(1.1, median, \"median\", va=\"center\", transform=ax.transAxes,)\n\nthreshold_top = median+3*k*mad\nthreshold_bottom = median-3*k*mad\n\nax.plot([start, end], [threshold_bottom]*2, color=\"black\", alpha=0.4)\nax.plot([start, end], [threshold_top]*2, color=\"black\", alpha=0.4)\n\nax.annotate(\"median\", xy=(1.02, median), xycoords=('axes fraction', 'data'),\n            va='center')\nax.annotate(r\"median$+3k\\cdot$MAD\", xy=(1.02, threshold_top), xycoords=('axes fraction', 'data'),\n            va='center')\nax.annotate(r\"median$-3k\\cdot$MAD\", xy=(1.02, threshold_bottom), xycoords=('axes fraction', 'data'),\n            va='center')\n\n# find and plot outliers\noutliers_index = df.index[(df['signal'] > threshold_top) | \n                          (df['signal'] < threshold_bottom)\n                         ]\nax.plot(df.loc[outliers_index, 'signal'], ls='None',\n        marker='o', markerfacecolor='yellow', markersize=5,\n        markeredgecolor=\"black\")\n\n# make graph look nice\nax.set(ylabel='values',\n       xlim=[start,end],\n       title=\"a stationary signal\",\n       ylim=[-45, 80])\nconcise(ax)\nfig.savefig(\"outliers_MAD_stationary.png\", bbox_inches='tight')\n\n\n\n\n\noutliers_index\n\nDatetimeIndex(['2023-01-10 04:09:00', '2023-01-10 14:05:00',\n               '2023-01-10 15:14:00'],\n              dtype='datetime64[ns]', name='date', freq=None)"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#save-data-as-csv-for-later-usage",
    "href": "behind-the-scenes/outliers_graphs.html#save-data-as-csv-for-later-usage",
    "title": "outliers graphs",
    "section": "save data as csv for later usage",
    "text": "save data as csv for later usage\n\nstart = '2023-01-01 00:00:00'\nend = '2023-12-31 23:55:00'\n# date_range = pd.date_range(start, end, freq='5min')\ndate_range = pd.date_range(start, end, freq='1D')\nn_steps = len(date_range)\nrw39, outlier_ind39 = random_walk_with_outliers(origin=0,\n                                            n_steps=n_steps,\n                                            perc_outliers=0.0031,\n                                            outlier_mult=50,\n                                            seed=39)\n# date_range = pd.date_range(start, periods=n_steps, freq='1min')\ndf = pd.DataFrame({'date': date_range, 'A': rw39}).set_index('date')\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,4))\n\nax.plot(df['A'])\n\n\n\n\n\ndf.loc['2023-03-02':'2023-03-10', 'A'] = -20.0\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,4))\ndf['Adiff'] = df['A'].diff()\nax.plot(df['A'])\nax.plot(df['Adiff'])\n\n\n\n\n\n(df.loc['2023-03-03':'2023-03-06', 'Adiff'] == np.zeros(4)).all()\n\nTrue\n\n\n\ndf.loc['2023-03-03':'2023-03-06', 'Adiff']\n\ndate\n2023-03-03    0.0\n2023-03-04    0.0\n2023-03-05    0.0\n2023-03-06    0.0\nName: Adiff, dtype: float64\n\n\n\nnp.zeros(4)\n\narray([0., 0., 0., 0.])\n\n\n\nn_consecutive = 2\n\ndef n_zeros(series, N):\n    \"\"\"\n    True if all series equals np.zeros(N)\n    False otherwise\n    \"\"\"\n    return (series == np.zeros(N)).all()\n\n\ndf['mask1'] = df['Adiff'].rolling(n_consecutive).apply(n_zeros, args=(n_consecutive,))\ndf['mask'] = 0.0\nfor i in range(len(df)):\n    if df['mask1'][i] == 1.0:\n        df['mask'][i-n_consecutive:i+1] = 1.0\n\n/var/folders/kv/9cqw3y_s6c75xmgqm9n0t5d40000gn/T/ipykernel_6180/2921310703.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['mask'][i-n_consecutive:i+1] = 1.0\n\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(df['mask1'])\nax.plot(df['mask'])\n\n\n\n\n\ndf['mask'] = 0.0\nfor j in range(len(df)-n_consecutive):\n    if (df['Adiff'][j:j+n_consecutive] == np.zeros(n_consecutive)).all():\n        df['mask'][j:j+n_consecutive] = 1.0\n\n/var/folders/kv/9cqw3y_s6c75xmgqm9n0t5d40000gn/T/ipykernel_6180/2364603426.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['mask'][j:j+n_consecutive] = 1.0\n\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,4))\n# ax.plot(df['mask1'])\nax.plot(df['mask'])\nax.plot(df['Adiff'])\nax.plot(df['A'])"
  },
  {
    "objectID": "behind-the-scenes/outliers_graphs.html#generate-datasets",
    "href": "behind-the-scenes/outliers_graphs.html#generate-datasets",
    "title": "outliers graphs",
    "section": "generate datasets",
    "text": "generate datasets\n\ndef random_walk_with_outliers2(origin, steps_dist, outlier_dist, n_steps, perc_outliers=0.0, outlier_mult=10, seed=42):\n    '''\n    Function for generating a random time series based on random walk.\n    It adds a specified percentage of outliers by multiplying the random walk step by a scalar.\n    \n    Parameters\n    ------------\n    origin : int\n        The starting point of the series\n    steps_dist : list of int or float\n        step distribution\n    outlier_dist : list of int or float\n        outlier distribution\n    n_steps : int\n        Lenght of the series\n    perc_outliers : float\n        Percentage of outliers to introduce to the series [0.0-1.0]\n    outlier_mult : float\n        Scalar by which to multiply the RW increment to create an outlier\n    seed : int\n        Random seed\n\n    Returns\n    -----------\n    rw : np.ndarray\n        The generated random walk series with outliers\n    indices : np.ndarray\n        The indices of the introduced outliers \n    '''\n    assert (perc_outliers >= 0.0) & (perc_outliers <= 1.0)\n    \n    #set seed for reproducibility\n    np.random.seed(seed)\n    \n    # possible steps\n    # steps = [-1, 1]\n\n    # simulate steps\n    steps = np.random.choice(a=steps_dist, size=n_steps-1)\n    rw = np.append(origin, steps).cumsum(0)\n    \n    # add outliers\n    n_outliers = int(np.round(perc_outliers * n_steps, 0))\n    indices = np.random.randint(0, len(rw), n_outliers)\n    outlier_jumps = np.random.choice(a=outlier_dist, size=n_outliers)\n    # rw[indices] = rw[indices] + steps[indices + 1] * outlier_mult\n    rw[indices] = rw[indices] + outlier_jumps\n    \n    return rw, indices\n\n\nrw39test, _ = random_walk_with_outliers2(origin=0,\n                                       steps_dist=np.random.normal(size=1000),\n                                       outlier_dist=10*np.random.normal(loc=5.0, size=100),\n                                       n_steps=n_steps,\n                                       perc_outliers=0.0002,\n                                       outlier_mult=50,\n                                       seed=206)\n\n\ndf['B'] = rw39test\n\n\nstart = '2023-01-01 00:00:00'\nend = '2023-12-31 23:00:00'\n# date_range = pd.date_range(start, end, freq='5min')\ndate_range = pd.date_range(start, end, freq='1D')\nn_steps = len(date_range)\n\n\n# date_range = pd.date_range(start, periods=n_steps, freq='1min')\ndf = pd.DataFrame({'date': date_range, 'A': rw39test}).set_index('date')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n    \n    \n      date\n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n    \n    \n      2023-01-02\n      -0.032027\n    \n    \n      2023-01-03\n      -0.586351\n    \n    \n      2023-01-04\n      -1.575972\n    \n    \n      2023-01-05\n      -2.726800\n    \n    \n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n    \n    \n      2023-12-28\n      6.415175\n    \n    \n      2023-12-29\n      7.603140\n    \n    \n      2023-12-30\n      8.668182\n    \n    \n      2023-12-31\n      8.472768\n    \n  \n\n365 rows × 1 columns\n\n\n\n\ndf['unix_time'] = df.index.timestamp()\n\nAttributeError: 'DatetimeIndex' object has no attribute 'timestamp'\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n    \n    \n      date\n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n    \n    \n      2023-01-02\n      -0.032027\n    \n    \n      2023-01-03\n      -0.586351\n    \n    \n      2023-01-04\n      -1.575972\n    \n    \n      2023-01-05\n      -2.726800\n    \n    \n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n    \n    \n      2023-12-28\n      6.415175\n    \n    \n      2023-12-29\n      7.603140\n    \n    \n      2023-12-30\n      8.668182\n    \n    \n      2023-12-31\n      8.472768\n    \n  \n\n365 rows × 1 columns\n\n\n\n\nrw02, outlier_ind02 = random_walk_with_outliers(origin=0,\n                                            n_steps=n_steps,\n                                            perc_outliers=0.0001,\n                                            outlier_mult=500,\n                                            seed=2)\n\n\ndf.loc['2023-01-02 23:00:00':'2023-01-03 03:00:00', 'E'] = np.nan\n\n\ndf.iloc[np.random.randint(0, high=len(df), size=600)]['E'] = np.nan\n\n/var/folders/kv/9cqw3y_s6c75xmgqm9n0t5d40000gn/T/ipykernel_9222/2255542716.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.iloc[np.random.randint(0, high=len(df), size=600)]['E'] = np.nan\n\n\n\n%matplotlib widget\nfig, ax = plt.subplots(figsize=(8,4))\n# ax.plot(df['B'])\nax.plot(rw39test)\n\n\n\n\n\ndf['date'] = df.index.strftime('%d%m%Y')\ndf['time'] = df.index.strftime('%H:%M:%S')\n\n\ndf['unix time (s)'] = df.index.view('int64') / 1e9\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      date\n      unix time (s)\n    \n    \n      date\n      \n      \n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n      2023-01-01\n      1.672531e+09\n    \n    \n      2023-01-02\n      -0.032027\n      2023-01-02\n      1.672618e+09\n    \n    \n      2023-01-03\n      -0.586351\n      2023-01-03\n      1.672704e+09\n    \n    \n      2023-01-04\n      -1.575972\n      2023-01-04\n      1.672790e+09\n    \n    \n      2023-01-05\n      -2.726800\n      2023-01-05\n      1.672877e+09\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n      2023-12-27\n      1.703635e+09\n    \n    \n      2023-12-28\n      6.415175\n      2023-12-28\n      1.703722e+09\n    \n    \n      2023-12-29\n      7.603140\n      2023-12-29\n      1.703808e+09\n    \n    \n      2023-12-30\n      8.668182\n      2023-12-30\n      1.703894e+09\n    \n    \n      2023-12-31\n      8.472768\n      2023-12-31\n      1.703981e+09\n    \n  \n\n365 rows × 3 columns\n\n\n\n\ndf.drop(columns=['date'], inplace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      unix time (s)\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2023-01-01\n      0.000000\n      1.672531e+09\n    \n    \n      2023-01-02\n      -0.032027\n      1.672618e+09\n    \n    \n      2023-01-03\n      -0.586351\n      1.672704e+09\n    \n    \n      2023-01-04\n      -1.575972\n      1.672790e+09\n    \n    \n      2023-01-05\n      -2.726800\n      1.672877e+09\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2023-12-27\n      6.651301\n      1.703635e+09\n    \n    \n      2023-12-28\n      6.415175\n      1.703722e+09\n    \n    \n      2023-12-29\n      7.603140\n      1.703808e+09\n    \n    \n      2023-12-30\n      8.668182\n      1.703894e+09\n    \n    \n      2023-12-31\n      8.472768\n      1.703981e+09\n    \n  \n\n365 rows × 2 columns\n\n\n\n\ndf.loc['2023-10-11 03:00:00', 'B'] = '-'\n\n\ndf.to_csv('cleaning3.csv', index=False, sep=' ')\n\n\ndf['A']\n\ndate\n2023-01-01 00:00:00     0.000000\n2023-01-01 01:00:00    -2.027536\n2023-01-01 02:00:00    -2.690617\n2023-01-01 03:00:00    -1.985990\n2023-01-01 04:00:00    -2.290898\n                         ...    \n2023-12-31 19:00:00   -74.514645\n2023-12-31 20:00:00   -74.738058\n2023-12-31 21:00:00   -75.848425\n2023-12-31 22:00:00   -77.272183\n2023-12-31 23:00:00   -76.557400\nName: A, Length: 8760, dtype: float64\n\n\n\nimport datetime\nimport time\n\ndt = datetime.datetime.now()\ntimestamp = time.mktime(dt.timetuple())\nprint(timestamp)\n\n1705330442.0\n\n\n\ntimestamp\n\n1705330442.0\n\n\n\ndt\n\ndatetime.datetime(2024, 1, 15, 16, 54, 2, 420872)\n\n\n\ndt.timetuple()\n\ntime.struct_time(tm_year=2024, tm_mon=1, tm_mday=15, tm_hour=16, tm_min=54, tm_sec=2, tm_wday=0, tm_yday=15, tm_isdst=-1)\n\n\n\npd.to_datetime(1705330442.0, unit='s')\n\nTimestamp('2024-01-15 14:54:02')\n\n\n\n(df.index.values).apply(lambda x: x.timetuple())\n\nAttributeError: 'numpy.ndarray' object has no attribute 'apply'\n\n\n\ndf['date'] = df.index.strftime('%Y-%m-%d')\n\n\ndf['date'].apply(lambda x: x.timetuple())\n\nAttributeError: 'str' object has no attribute 'timetuple'\n\n\n\ndf.index.to_pydatetime()\n\narray([datetime.datetime(2023, 1, 1, 0, 0),\n       datetime.datetime(2023, 1, 2, 0, 0),\n       datetime.datetime(2023, 1, 3, 0, 0),\n       datetime.datetime(2023, 1, 4, 0, 0),\n       datetime.datetime(2023, 1, 5, 0, 0),\n       datetime.datetime(2023, 1, 6, 0, 0),\n       datetime.datetime(2023, 1, 7, 0, 0),\n       datetime.datetime(2023, 1, 8, 0, 0),\n       datetime.datetime(2023, 1, 9, 0, 0),\n       datetime.datetime(2023, 1, 10, 0, 0),\n       datetime.datetime(2023, 1, 11, 0, 0),\n       datetime.datetime(2023, 1, 12, 0, 0),\n       datetime.datetime(2023, 1, 13, 0, 0),\n       datetime.datetime(2023, 1, 14, 0, 0),\n       datetime.datetime(2023, 1, 15, 0, 0),\n       datetime.datetime(2023, 1, 16, 0, 0),\n       datetime.datetime(2023, 1, 17, 0, 0),\n       datetime.datetime(2023, 1, 18, 0, 0),\n       datetime.datetime(2023, 1, 19, 0, 0),\n       datetime.datetime(2023, 1, 20, 0, 0),\n       datetime.datetime(2023, 1, 21, 0, 0),\n       datetime.datetime(2023, 1, 22, 0, 0),\n       datetime.datetime(2023, 1, 23, 0, 0),\n       datetime.datetime(2023, 1, 24, 0, 0),\n       datetime.datetime(2023, 1, 25, 0, 0),\n       datetime.datetime(2023, 1, 26, 0, 0),\n       datetime.datetime(2023, 1, 27, 0, 0),\n       datetime.datetime(2023, 1, 28, 0, 0),\n       datetime.datetime(2023, 1, 29, 0, 0),\n       datetime.datetime(2023, 1, 30, 0, 0),\n       datetime.datetime(2023, 1, 31, 0, 0),\n       datetime.datetime(2023, 2, 1, 0, 0),\n       datetime.datetime(2023, 2, 2, 0, 0),\n       datetime.datetime(2023, 2, 3, 0, 0),\n       datetime.datetime(2023, 2, 4, 0, 0),\n       datetime.datetime(2023, 2, 5, 0, 0),\n       datetime.datetime(2023, 2, 6, 0, 0),\n       datetime.datetime(2023, 2, 7, 0, 0),\n       datetime.datetime(2023, 2, 8, 0, 0),\n       datetime.datetime(2023, 2, 9, 0, 0),\n       datetime.datetime(2023, 2, 10, 0, 0),\n       datetime.datetime(2023, 2, 11, 0, 0),\n       datetime.datetime(2023, 2, 12, 0, 0),\n       datetime.datetime(2023, 2, 13, 0, 0),\n       datetime.datetime(2023, 2, 14, 0, 0),\n       datetime.datetime(2023, 2, 15, 0, 0),\n       datetime.datetime(2023, 2, 16, 0, 0),\n       datetime.datetime(2023, 2, 17, 0, 0),\n       datetime.datetime(2023, 2, 18, 0, 0),\n       datetime.datetime(2023, 2, 19, 0, 0),\n       datetime.datetime(2023, 2, 20, 0, 0),\n       datetime.datetime(2023, 2, 21, 0, 0),\n       datetime.datetime(2023, 2, 22, 0, 0),\n       datetime.datetime(2023, 2, 23, 0, 0),\n       datetime.datetime(2023, 2, 24, 0, 0),\n       datetime.datetime(2023, 2, 25, 0, 0),\n       datetime.datetime(2023, 2, 26, 0, 0),\n       datetime.datetime(2023, 2, 27, 0, 0),\n       datetime.datetime(2023, 2, 28, 0, 0),\n       datetime.datetime(2023, 3, 1, 0, 0),\n       datetime.datetime(2023, 3, 2, 0, 0),\n       datetime.datetime(2023, 3, 3, 0, 0),\n       datetime.datetime(2023, 3, 4, 0, 0),\n       datetime.datetime(2023, 3, 5, 0, 0),\n       datetime.datetime(2023, 3, 6, 0, 0),\n       datetime.datetime(2023, 3, 7, 0, 0),\n       datetime.datetime(2023, 3, 8, 0, 0),\n       datetime.datetime(2023, 3, 9, 0, 0),\n       datetime.datetime(2023, 3, 10, 0, 0),\n       datetime.datetime(2023, 3, 11, 0, 0),\n       datetime.datetime(2023, 3, 12, 0, 0),\n       datetime.datetime(2023, 3, 13, 0, 0),\n       datetime.datetime(2023, 3, 14, 0, 0),\n       datetime.datetime(2023, 3, 15, 0, 0),\n       datetime.datetime(2023, 3, 16, 0, 0),\n       datetime.datetime(2023, 3, 17, 0, 0),\n       datetime.datetime(2023, 3, 18, 0, 0),\n       datetime.datetime(2023, 3, 19, 0, 0),\n       datetime.datetime(2023, 3, 20, 0, 0),\n       datetime.datetime(2023, 3, 21, 0, 0),\n       datetime.datetime(2023, 3, 22, 0, 0),\n       datetime.datetime(2023, 3, 23, 0, 0),\n       datetime.datetime(2023, 3, 24, 0, 0),\n       datetime.datetime(2023, 3, 25, 0, 0),\n       datetime.datetime(2023, 3, 26, 0, 0),\n       datetime.datetime(2023, 3, 27, 0, 0),\n       datetime.datetime(2023, 3, 28, 0, 0),\n       datetime.datetime(2023, 3, 29, 0, 0),\n       datetime.datetime(2023, 3, 30, 0, 0),\n       datetime.datetime(2023, 3, 31, 0, 0),\n       datetime.datetime(2023, 4, 1, 0, 0),\n       datetime.datetime(2023, 4, 2, 0, 0),\n       datetime.datetime(2023, 4, 3, 0, 0),\n       datetime.datetime(2023, 4, 4, 0, 0),\n       datetime.datetime(2023, 4, 5, 0, 0),\n       datetime.datetime(2023, 4, 6, 0, 0),\n       datetime.datetime(2023, 4, 7, 0, 0),\n       datetime.datetime(2023, 4, 8, 0, 0),\n       datetime.datetime(2023, 4, 9, 0, 0),\n       datetime.datetime(2023, 4, 10, 0, 0),\n       datetime.datetime(2023, 4, 11, 0, 0),\n       datetime.datetime(2023, 4, 12, 0, 0),\n       datetime.datetime(2023, 4, 13, 0, 0),\n       datetime.datetime(2023, 4, 14, 0, 0),\n       datetime.datetime(2023, 4, 15, 0, 0),\n       datetime.datetime(2023, 4, 16, 0, 0),\n       datetime.datetime(2023, 4, 17, 0, 0),\n       datetime.datetime(2023, 4, 18, 0, 0),\n       datetime.datetime(2023, 4, 19, 0, 0),\n       datetime.datetime(2023, 4, 20, 0, 0),\n       datetime.datetime(2023, 4, 21, 0, 0),\n       datetime.datetime(2023, 4, 22, 0, 0),\n       datetime.datetime(2023, 4, 23, 0, 0),\n       datetime.datetime(2023, 4, 24, 0, 0),\n       datetime.datetime(2023, 4, 25, 0, 0),\n       datetime.datetime(2023, 4, 26, 0, 0),\n       datetime.datetime(2023, 4, 27, 0, 0),\n       datetime.datetime(2023, 4, 28, 0, 0),\n       datetime.datetime(2023, 4, 29, 0, 0),\n       datetime.datetime(2023, 4, 30, 0, 0),\n       datetime.datetime(2023, 5, 1, 0, 0),\n       datetime.datetime(2023, 5, 2, 0, 0),\n       datetime.datetime(2023, 5, 3, 0, 0),\n       datetime.datetime(2023, 5, 4, 0, 0),\n       datetime.datetime(2023, 5, 5, 0, 0),\n       datetime.datetime(2023, 5, 6, 0, 0),\n       datetime.datetime(2023, 5, 7, 0, 0),\n       datetime.datetime(2023, 5, 8, 0, 0),\n       datetime.datetime(2023, 5, 9, 0, 0),\n       datetime.datetime(2023, 5, 10, 0, 0),\n       datetime.datetime(2023, 5, 11, 0, 0),\n       datetime.datetime(2023, 5, 12, 0, 0),\n       datetime.datetime(2023, 5, 13, 0, 0),\n       datetime.datetime(2023, 5, 14, 0, 0),\n       datetime.datetime(2023, 5, 15, 0, 0),\n       datetime.datetime(2023, 5, 16, 0, 0),\n       datetime.datetime(2023, 5, 17, 0, 0),\n       datetime.datetime(2023, 5, 18, 0, 0),\n       datetime.datetime(2023, 5, 19, 0, 0),\n       datetime.datetime(2023, 5, 20, 0, 0),\n       datetime.datetime(2023, 5, 21, 0, 0),\n       datetime.datetime(2023, 5, 22, 0, 0),\n       datetime.datetime(2023, 5, 23, 0, 0),\n       datetime.datetime(2023, 5, 24, 0, 0),\n       datetime.datetime(2023, 5, 25, 0, 0),\n       datetime.datetime(2023, 5, 26, 0, 0),\n       datetime.datetime(2023, 5, 27, 0, 0),\n       datetime.datetime(2023, 5, 28, 0, 0),\n       datetime.datetime(2023, 5, 29, 0, 0),\n       datetime.datetime(2023, 5, 30, 0, 0),\n       datetime.datetime(2023, 5, 31, 0, 0),\n       datetime.datetime(2023, 6, 1, 0, 0),\n       datetime.datetime(2023, 6, 2, 0, 0),\n       datetime.datetime(2023, 6, 3, 0, 0),\n       datetime.datetime(2023, 6, 4, 0, 0),\n       datetime.datetime(2023, 6, 5, 0, 0),\n       datetime.datetime(2023, 6, 6, 0, 0),\n       datetime.datetime(2023, 6, 7, 0, 0),\n       datetime.datetime(2023, 6, 8, 0, 0),\n       datetime.datetime(2023, 6, 9, 0, 0),\n       datetime.datetime(2023, 6, 10, 0, 0),\n       datetime.datetime(2023, 6, 11, 0, 0),\n       datetime.datetime(2023, 6, 12, 0, 0),\n       datetime.datetime(2023, 6, 13, 0, 0),\n       datetime.datetime(2023, 6, 14, 0, 0),\n       datetime.datetime(2023, 6, 15, 0, 0),\n       datetime.datetime(2023, 6, 16, 0, 0),\n       datetime.datetime(2023, 6, 17, 0, 0),\n       datetime.datetime(2023, 6, 18, 0, 0),\n       datetime.datetime(2023, 6, 19, 0, 0),\n       datetime.datetime(2023, 6, 20, 0, 0),\n       datetime.datetime(2023, 6, 21, 0, 0),\n       datetime.datetime(2023, 6, 22, 0, 0),\n       datetime.datetime(2023, 6, 23, 0, 0),\n       datetime.datetime(2023, 6, 24, 0, 0),\n       datetime.datetime(2023, 6, 25, 0, 0),\n       datetime.datetime(2023, 6, 26, 0, 0),\n       datetime.datetime(2023, 6, 27, 0, 0),\n       datetime.datetime(2023, 6, 28, 0, 0),\n       datetime.datetime(2023, 6, 29, 0, 0),\n       datetime.datetime(2023, 6, 30, 0, 0),\n       datetime.datetime(2023, 7, 1, 0, 0),\n       datetime.datetime(2023, 7, 2, 0, 0),\n       datetime.datetime(2023, 7, 3, 0, 0),\n       datetime.datetime(2023, 7, 4, 0, 0),\n       datetime.datetime(2023, 7, 5, 0, 0),\n       datetime.datetime(2023, 7, 6, 0, 0),\n       datetime.datetime(2023, 7, 7, 0, 0),\n       datetime.datetime(2023, 7, 8, 0, 0),\n       datetime.datetime(2023, 7, 9, 0, 0),\n       datetime.datetime(2023, 7, 10, 0, 0),\n       datetime.datetime(2023, 7, 11, 0, 0),\n       datetime.datetime(2023, 7, 12, 0, 0),\n       datetime.datetime(2023, 7, 13, 0, 0),\n       datetime.datetime(2023, 7, 14, 0, 0),\n       datetime.datetime(2023, 7, 15, 0, 0),\n       datetime.datetime(2023, 7, 16, 0, 0),\n       datetime.datetime(2023, 7, 17, 0, 0),\n       datetime.datetime(2023, 7, 18, 0, 0),\n       datetime.datetime(2023, 7, 19, 0, 0),\n       datetime.datetime(2023, 7, 20, 0, 0),\n       datetime.datetime(2023, 7, 21, 0, 0),\n       datetime.datetime(2023, 7, 22, 0, 0),\n       datetime.datetime(2023, 7, 23, 0, 0),\n       datetime.datetime(2023, 7, 24, 0, 0),\n       datetime.datetime(2023, 7, 25, 0, 0),\n       datetime.datetime(2023, 7, 26, 0, 0),\n       datetime.datetime(2023, 7, 27, 0, 0),\n       datetime.datetime(2023, 7, 28, 0, 0),\n       datetime.datetime(2023, 7, 29, 0, 0),\n       datetime.datetime(2023, 7, 30, 0, 0),\n       datetime.datetime(2023, 7, 31, 0, 0),\n       datetime.datetime(2023, 8, 1, 0, 0),\n       datetime.datetime(2023, 8, 2, 0, 0),\n       datetime.datetime(2023, 8, 3, 0, 0),\n       datetime.datetime(2023, 8, 4, 0, 0),\n       datetime.datetime(2023, 8, 5, 0, 0),\n       datetime.datetime(2023, 8, 6, 0, 0),\n       datetime.datetime(2023, 8, 7, 0, 0),\n       datetime.datetime(2023, 8, 8, 0, 0),\n       datetime.datetime(2023, 8, 9, 0, 0),\n       datetime.datetime(2023, 8, 10, 0, 0),\n       datetime.datetime(2023, 8, 11, 0, 0),\n       datetime.datetime(2023, 8, 12, 0, 0),\n       datetime.datetime(2023, 8, 13, 0, 0),\n       datetime.datetime(2023, 8, 14, 0, 0),\n       datetime.datetime(2023, 8, 15, 0, 0),\n       datetime.datetime(2023, 8, 16, 0, 0),\n       datetime.datetime(2023, 8, 17, 0, 0),\n       datetime.datetime(2023, 8, 18, 0, 0),\n       datetime.datetime(2023, 8, 19, 0, 0),\n       datetime.datetime(2023, 8, 20, 0, 0),\n       datetime.datetime(2023, 8, 21, 0, 0),\n       datetime.datetime(2023, 8, 22, 0, 0),\n       datetime.datetime(2023, 8, 23, 0, 0),\n       datetime.datetime(2023, 8, 24, 0, 0),\n       datetime.datetime(2023, 8, 25, 0, 0),\n       datetime.datetime(2023, 8, 26, 0, 0),\n       datetime.datetime(2023, 8, 27, 0, 0),\n       datetime.datetime(2023, 8, 28, 0, 0),\n       datetime.datetime(2023, 8, 29, 0, 0),\n       datetime.datetime(2023, 8, 30, 0, 0),\n       datetime.datetime(2023, 8, 31, 0, 0),\n       datetime.datetime(2023, 9, 1, 0, 0),\n       datetime.datetime(2023, 9, 2, 0, 0),\n       datetime.datetime(2023, 9, 3, 0, 0),\n       datetime.datetime(2023, 9, 4, 0, 0),\n       datetime.datetime(2023, 9, 5, 0, 0),\n       datetime.datetime(2023, 9, 6, 0, 0),\n       datetime.datetime(2023, 9, 7, 0, 0),\n       datetime.datetime(2023, 9, 8, 0, 0),\n       datetime.datetime(2023, 9, 9, 0, 0),\n       datetime.datetime(2023, 9, 10, 0, 0),\n       datetime.datetime(2023, 9, 11, 0, 0),\n       datetime.datetime(2023, 9, 12, 0, 0),\n       datetime.datetime(2023, 9, 13, 0, 0),\n       datetime.datetime(2023, 9, 14, 0, 0),\n       datetime.datetime(2023, 9, 15, 0, 0),\n       datetime.datetime(2023, 9, 16, 0, 0),\n       datetime.datetime(2023, 9, 17, 0, 0),\n       datetime.datetime(2023, 9, 18, 0, 0),\n       datetime.datetime(2023, 9, 19, 0, 0),\n       datetime.datetime(2023, 9, 20, 0, 0),\n       datetime.datetime(2023, 9, 21, 0, 0),\n       datetime.datetime(2023, 9, 22, 0, 0),\n       datetime.datetime(2023, 9, 23, 0, 0),\n       datetime.datetime(2023, 9, 24, 0, 0),\n       datetime.datetime(2023, 9, 25, 0, 0),\n       datetime.datetime(2023, 9, 26, 0, 0),\n       datetime.datetime(2023, 9, 27, 0, 0),\n       datetime.datetime(2023, 9, 28, 0, 0),\n       datetime.datetime(2023, 9, 29, 0, 0),\n       datetime.datetime(2023, 9, 30, 0, 0),\n       datetime.datetime(2023, 10, 1, 0, 0),\n       datetime.datetime(2023, 10, 2, 0, 0),\n       datetime.datetime(2023, 10, 3, 0, 0),\n       datetime.datetime(2023, 10, 4, 0, 0),\n       datetime.datetime(2023, 10, 5, 0, 0),\n       datetime.datetime(2023, 10, 6, 0, 0),\n       datetime.datetime(2023, 10, 7, 0, 0),\n       datetime.datetime(2023, 10, 8, 0, 0),\n       datetime.datetime(2023, 10, 9, 0, 0),\n       datetime.datetime(2023, 10, 10, 0, 0),\n       datetime.datetime(2023, 10, 11, 0, 0),\n       datetime.datetime(2023, 10, 12, 0, 0),\n       datetime.datetime(2023, 10, 13, 0, 0),\n       datetime.datetime(2023, 10, 14, 0, 0),\n       datetime.datetime(2023, 10, 15, 0, 0),\n       datetime.datetime(2023, 10, 16, 0, 0),\n       datetime.datetime(2023, 10, 17, 0, 0),\n       datetime.datetime(2023, 10, 18, 0, 0),\n       datetime.datetime(2023, 10, 19, 0, 0),\n       datetime.datetime(2023, 10, 20, 0, 0),\n       datetime.datetime(2023, 10, 21, 0, 0),\n       datetime.datetime(2023, 10, 22, 0, 0),\n       datetime.datetime(2023, 10, 23, 0, 0),\n       datetime.datetime(2023, 10, 24, 0, 0),\n       datetime.datetime(2023, 10, 25, 0, 0),\n       datetime.datetime(2023, 10, 26, 0, 0),\n       datetime.datetime(2023, 10, 27, 0, 0),\n       datetime.datetime(2023, 10, 28, 0, 0),\n       datetime.datetime(2023, 10, 29, 0, 0),\n       datetime.datetime(2023, 10, 30, 0, 0),\n       datetime.datetime(2023, 10, 31, 0, 0),\n       datetime.datetime(2023, 11, 1, 0, 0),\n       datetime.datetime(2023, 11, 2, 0, 0),\n       datetime.datetime(2023, 11, 3, 0, 0),\n       datetime.datetime(2023, 11, 4, 0, 0),\n       datetime.datetime(2023, 11, 5, 0, 0),\n       datetime.datetime(2023, 11, 6, 0, 0),\n       datetime.datetime(2023, 11, 7, 0, 0),\n       datetime.datetime(2023, 11, 8, 0, 0),\n       datetime.datetime(2023, 11, 9, 0, 0),\n       datetime.datetime(2023, 11, 10, 0, 0),\n       datetime.datetime(2023, 11, 11, 0, 0),\n       datetime.datetime(2023, 11, 12, 0, 0),\n       datetime.datetime(2023, 11, 13, 0, 0),\n       datetime.datetime(2023, 11, 14, 0, 0),\n       datetime.datetime(2023, 11, 15, 0, 0),\n       datetime.datetime(2023, 11, 16, 0, 0),\n       datetime.datetime(2023, 11, 17, 0, 0),\n       datetime.datetime(2023, 11, 18, 0, 0),\n       datetime.datetime(2023, 11, 19, 0, 0),\n       datetime.datetime(2023, 11, 20, 0, 0),\n       datetime.datetime(2023, 11, 21, 0, 0),\n       datetime.datetime(2023, 11, 22, 0, 0),\n       datetime.datetime(2023, 11, 23, 0, 0),\n       datetime.datetime(2023, 11, 24, 0, 0),\n       datetime.datetime(2023, 11, 25, 0, 0),\n       datetime.datetime(2023, 11, 26, 0, 0),\n       datetime.datetime(2023, 11, 27, 0, 0),\n       datetime.datetime(2023, 11, 28, 0, 0),\n       datetime.datetime(2023, 11, 29, 0, 0),\n       datetime.datetime(2023, 11, 30, 0, 0),\n       datetime.datetime(2023, 12, 1, 0, 0),\n       datetime.datetime(2023, 12, 2, 0, 0),\n       datetime.datetime(2023, 12, 3, 0, 0),\n       datetime.datetime(2023, 12, 4, 0, 0),\n       datetime.datetime(2023, 12, 5, 0, 0),\n       datetime.datetime(2023, 12, 6, 0, 0),\n       datetime.datetime(2023, 12, 7, 0, 0),\n       datetime.datetime(2023, 12, 8, 0, 0),\n       datetime.datetime(2023, 12, 9, 0, 0),\n       datetime.datetime(2023, 12, 10, 0, 0),\n       datetime.datetime(2023, 12, 11, 0, 0),\n       datetime.datetime(2023, 12, 12, 0, 0),\n       datetime.datetime(2023, 12, 13, 0, 0),\n       datetime.datetime(2023, 12, 14, 0, 0),\n       datetime.datetime(2023, 12, 15, 0, 0),\n       datetime.datetime(2023, 12, 16, 0, 0),\n       datetime.datetime(2023, 12, 17, 0, 0),\n       datetime.datetime(2023, 12, 18, 0, 0),\n       datetime.datetime(2023, 12, 19, 0, 0),\n       datetime.datetime(2023, 12, 20, 0, 0),\n       datetime.datetime(2023, 12, 21, 0, 0),\n       datetime.datetime(2023, 12, 22, 0, 0),\n       datetime.datetime(2023, 12, 23, 0, 0),\n       datetime.datetime(2023, 12, 24, 0, 0),\n       datetime.datetime(2023, 12, 25, 0, 0),\n       datetime.datetime(2023, 12, 26, 0, 0),\n       datetime.datetime(2023, 12, 27, 0, 0),\n       datetime.datetime(2023, 12, 28, 0, 0),\n       datetime.datetime(2023, 12, 29, 0, 0),\n       datetime.datetime(2023, 12, 30, 0, 0),\n       datetime.datetime(2023, 12, 31, 0, 0)], dtype=object)\n\n\n\nunix = df.index.view('int64') / 1e9\n\n\nunix[0]\n\n1672531200.0\n\n\n\npd.to_datetime(unix, unit='s')\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10',\n               ...\n               '2023-12-22', '2023-12-23', '2023-12-24', '2023-12-25',\n               '2023-12-26', '2023-12-27', '2023-12-28', '2023-12-29',\n               '2023-12-30', '2023-12-31'],\n              dtype='datetime64[ns]', length=365, freq=None)"
  },
  {
    "objectID": "behind-the-scenes/make_your_own_website.html#random-tips",
    "href": "behind-the-scenes/make_your_own_website.html#random-tips",
    "title": "make your own website",
    "section": "random tips",
    "text": "random tips\n\nextensions\nUse icons on your website with iconify and fontawesome. You can actually use a wide variety of extensions, check them out.\n\n\nquarto.yml\nYou can configure whatever you need in the _quarto.yml file. I’ll paste here my html formatting for reference.\nformat:\nhtml:\n  theme:\n  # see all available themes https://bootswatch.com\n  - flatly            # chose whatever theme you find suitable\n  - custom.scss       # customize how your website looks\n  fontsize: 1.2em     # self explanatory\n  # choose a nice highlight style for the code\n  highlight-style: breezedark # monokai # breezedark # espresso\n  include-in-header:\n    - includes.html          # you might need to use css configurations in all your pages\n  code-line-numbers: true    # turn on line numbering\n  code-tools:\n    # if you defined repo-url, this will link your website to it.\n    # repo-url: https://github.com/github_username/repository_name/\n    source: repo  # https://quarto.org/docs/output-formats/html-code.html#code-tools\n  callout-icon: false\n  fig-align: center         # center images as default\n  # the default MathJax rendering option yields ugly results, use katex\n  html-math-method: katex\n\n\nconfigure your notebook with a suitable header\nYou could start your jupyter notebook with a markdown cell with this header\n# this jupyter notebook title\nbut in case you need a lot of control over the details, consider using:\n---\ntitle: \"this jupyter notebook title\"\nexecute:\n  # echo: false  # chose this if you don't want to see the code at all, just the output\n  freeze: auto  # re-render only when source changes, VERY useful\nformat:\n  html:\n    code-fold: true                 # hide code blocks, show them upon click\n    code-summary: \"Show the code\"   # rename button to show code block\n---\n\n\nobvious statement\nThis very website is a “Quarto website” project hosted on github. Click on “ Code” on the top of the page to go to the github repository. Then copy whatever you want, it’s all open for everyone to see."
  }
]